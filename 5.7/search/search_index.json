{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Percona Server for MySQL 5.7 Documentation","text":"<p>This documentation is for the latest release: Percona Server for MySQL 5.7.43-47 (Release Notes).</p> <p>Percona Server for MySQL is a freely available, fully compatible, enhanced, and open source drop-in replacement for any MySQL database. It provides superior and optimized performance, greater scalability, and availability, enhanced backups, increased visibility and instrumentation.</p> <p>Percona Server for MySQL is trusted by thousands of enterprises to provide better performance and concurrency for their most demanding workloads.</p>"},{"location":"index.html#for-monitoring-and-management","title":"For Monitoring and Management","text":"<p>Percona Monitoring and Management (PMM )monitors and provides actionable performance data for MySQL variants, including Percona Server for MySQL, Percona XtraDB Cluster, Oracle MySQL Community Edition, Oracle MySQL Enterprise Edition, and MariaDB. PMM captures metrics and data for the InnoDB, XtraDB, and MyRocks storage engines, and has specialized dashboards for specific engine details.</p> <p>Install PMM and connect your server instances to it.</p>"},{"location":"changed_in_57.html","title":"Changed in Percona Server 5.7","text":"<p>Percona Server for MySQL 5.7 is based on MySQL 5.7 and incorporates many of the improvements found in Percona Server for MySQL 5.6.</p>"},{"location":"changed_in_57.html#features-removed-from-percona-server-for-mysql-57-that-were-available-in-percona-server-for-mysql-56","title":"Features removed from Percona Server for MySQL 5.7 that were available in Percona Server for MySQL 5.6","text":"<p>Note</p> <p>Percona Server 5.7 won\u2019t be able to start if any of variables from these features are set in the server\u2019s configuration file.</p> <p>Some features that were present in Percona Server for MySQL 5.6 have been removed in Percona Server for MySQL 5.7. These are:</p> <ul> <li> <p>Handlersocket</p> <ul> <li>This feature might be included in a future release if HandlerSocket starts supporting 5.7.</li> </ul> </li> <li> <p>Support for Fake Changes</p> <ul> <li>Instead of replica prefetching using the fake changes, a 5.7 intra-schema parallel replication replica should be used.</li> </ul> </li> <li> <p><code>SHOW ENGINE INNODB STATUS</code> no longer prints the count of active     Read-Only transactions.</p> </li> <li> <p>InnoDB redo log archiving     has been removed due to lack of user uptake of the feature.</p> </li> </ul>"},{"location":"changed_in_57.html#changes-in-percona-server-56-features","title":"Changes in Percona Server 5.6 features","text":"<ul> <li> <p>The minor Percona Server for MySQL version number (\u201cy\u201d in \u201c5.a.b-x.y\u201d)     has been dropped to simplify Percona Server for MySQL versioning.</p> </li> <li> <p>Performance Schema memory instrumentation support has been added to the</p> <p>Audit Log Plugin, Metrics for scalability measurement, and PAM Authentication Plugin, and to the core server to track memory used by User Statistics, Per-query variable statement, XtraDB changed page tracking, and Thread Pool features.</p> </li> <li> <p>Audit Log Plugin now produces diagnostics in a format consistent with     the rest of the server.</p> </li> <li> <p>The performance_schema.metadata_locks table now displays <code>backup</code> and <code>binlog</code> lock information too. The <code>object_type</code> column has two new valid values: <code>backup</code>, and <code>binlog</code>.</p> </li> <li> <p>INFORMATION_SCHEMA.XTRADB_RSEG table schema has been changed to support     new possible InnoDB page sizes. The <code>zip_size</code> column has been removed     and replaced by new columns <code>physical_page_size</code>, <code>logical_page_size</code>,     and <code>is_compressed</code>.</p> </li> <li> <p>XTRADB_READ_VIEW table no longer contains the     <code>READ_VIEW_UNDO_NUMBER</code> column, which was associated with unused code     and always contained zero.</p> </li> <li> <p>Interaction between <code>--hidden-</code> option modifier     and session_track_system_variables     has been implemented as follows: any variables with <code>--hidden-</code>     modifier become hidden from the latter variable too. Thus, they should     not be present there. Even if you never     set <code>session_track_system_variables</code>, care must be taken if a variable     contained in its default value (i.e. autocommit) is hidden.</p> </li> <li> <p>Nested <code>SET STATEMENT ... FOR SET STATEMENT ... FOR ...</code> statements have a different effect in the innermost clause when the nested clauses set the same variables: in 5.6, the innermost assignment is effective, but, in 5.7, the outermost assignment is effective.</p> </li> <li> <p>Utility user is treated as a <code>SUPER</code> user for the purposes     of offline mode:     utility user connections are not dropped if server switches to offline     mode and new utility user connections can be established to such     server.</p> </li> <li> <p>The server will abort startup with an error message if conflicting     enforce_storage_engine and disabled_storage_engines option values are     specified, that is, if the enforced storage engine is in the list of     disabled storage engines.</p> </li> </ul>"},{"location":"changed_in_57.html#features-available-in-percona-server-for-mysql-56-that-have-been-replaced-with-mysql-57-features","title":"Features available in Percona Server for MySQL 5.6 that have been replaced with MySQL 5.7 features","text":"<p>Note</p> <p>Percona Server 5.7 won\u2019t be able to start if any of variables from these features are set in the server\u2019s configuration file.</p> <p>Some Percona Server for MySQL 5.6 features have been replaced by similar or equivalent MySQL 5.7 features, so we now keep the MySQL 5.7 implementations in Percona Server for MySQL 5.7. These are:</p> <ul> <li> <p>Lock-Free SHOW SLAVE STATUS NONBLOCKING     has been replaced by a     regular <code>SHOW SLAVE STATUS</code> implementation     . Oracle implementation forbids calling it from a stored function.</p> </li> <li> <p>Behavior corresponding to slow_query_log_timestamp_precision set</p> <p>to <code>microsecond</code> is now the default, the variable itself and the behavior corresponding to the variable\u2019s <code>second</code> value is removed.</p> </li> <li> <p>Behavior corresponding to slow_query_log_timestamp_always set to <code>TRUE</code></p> <p>is now the default, the variable itself and the behavior corresponding to the variable\u2019s <code>FALSE</code> value is removed.</p> </li> <li> <p>Statement timeout feature     has been replaced by     Oracle Server-side SELECT statement timeouts     implementation. Differences: the Oracle variable is     named max_statement_time     instead of max_statement_time; variable have_statement_timeout variable     has been removed; the timeouts only apply     for read-only SELECT     .</p> </li> <li> <p>Atomic write support on fusionIO devices     with NVMFS has been replaced by Oracle implementation. It is no longer     required to enable innodb_use_atomic_writes variable, and this variable     has been removed. The atomic write support will be enabled, and the     doublewrite buffer disabled, on supporting devices automatically. The     Oracle implementation does not silently adjust innodb_flush_method     to <code>O_DIRECT</code> if it has a different value. The user must set it     to <code>O_DIRECT</code> explicitly, or atomic writes will not be enabled.</p> </li> <li> <p>Online GTID migration patch     has been replaced by an upstream     variable gtid_mode     made dynamic.</p> </li> <li> <p>The Error Code Compatibility has been replaced by the multiple start-error-number directive in <code>sql/share/errmsg-utf8.txt</code> support.</p> </li> <li> <p>Ignoring missing tables in mysqldump     with <code>--ignore-create-error</code> option has been replaced by the more     general upstream     option \u2013ignore-error     option.</p> </li> <li> <p>innodb_log_block_size     has been replaced     by innodb_log_write_ahead_size     variable. To avoid read on write when the storage block size is not     equal to 512 bytes, the latter should be set to the same value the     former was. If innodb_log_block_size was set to non-default values, new     log files must be created during the upgrade. This can be done by     cleanly shutting down the service and removing the variable     from <code>my.cnf</code> configuration and removing the old logs and starting the     service again before doing the upgrade.</p> </li> <li> <p>Extended secure-file-priv server option     , which was used to disable <code>LOAD DATA INFILE</code>, <code>SELECT INTO OUTFILE</code>     statements, and <code>LOAD_FILE()</code> function completely, has been replaced by     upstream introducing <code>NULL</code> as a possible value to this variable. To     migrate, any value-less settings must be replaced by <code>NULL</code>.</p> </li> <li> <p>innodb_sched_priority_cleaner     variable has been removed, as the effect of setting it to 39 (     corresponding to nice value of -20), is now enabled by default.</p> </li> <li> <p>innodb_adaptive_hash_index_partitions     has been replaced     by innodb_adaptive_hash_index_parts     .</p> </li> <li> <p>In the default server setup (with InnoDB being the only one XA-capable     storage engine), <code>--tc-heuristic-recover=COMMIT</code> is silently converted     to <code>ROLLBACK</code>. If TokuDB or another XA-supporting 3<sup>rd</sup> party storage     engine is installed, <code>--tc-heuristic-recover=ROLLBACK</code> option is     unavailable. The default value of <code>tc-heuristic-recover</code> option in     Percona Server for MySQL 5.6 but not in MySQL 5.6 was <code>NONE</code> as a     result of fix for upstream     bug #70860. Since Oracle     fixed the same bug in 5.7, the default value is <code>OFF</code> now.</p> </li> <li> <p>innodb_log_checksum_algorithm     feature has been replaced     by innodb_log_checksums     option. In particular, to get the effect of setting the     innodb_log_checksum_algorithm to <code>crc32</code>, innodb_log_checksums should     be set to <code>ON</code>, which is a default setting for this variable.</p> </li> <li> <p>innodb_buffer_pool_populate     server option     and numa_interleave <code>mysql_safe.sh</code>     option have been replaced     by innodb_numa_interleave     server option. Note that flush_caches option still remains.</p> </li> <li> <p>Ability to change database for mysqlbinlog     implementation has been replaced from MariaDB one with     MySQL rewrite-db     one. The feature is mostly identical with two differences: 1) multiple     rewrite rules must be given as separate options, and the ability to     list them in a single rule, separated by commas, is lost. That is,     any <code>--rewrite-db='a-&gt;b,c-&gt;d'</code> occurrences must be replaced     with <code>--rewrite-db='a-&gt;b' --rewrite-db='c-&gt;d'</code>. 2) Whitespace around     database names is not ignored.</p> </li> <li> <p>INFORMATION_SCHEMA.PROCESSLIST.TID column has been replaced     by PERFORMANCE_SCHEMA.THREADS.THREAD_OS_ID column     . If running under thread pool, <code>THREAD_OS_ID</code> column will always     be <code>NULL</code>, whereas in the 5.6 implementation <code>TID</code> column showed     either <code>NULL</code> or the assigned worker thread id at the moment.</p> </li> <li> <p>innodb_foreground_preflush server     variable has been removed as the upstream implemented a similar feature     without a controlling option.</p> </li> <li> <p>Log All Client Commands (syslog)     feature has been replaced by     Oracle mysql Logging     implementation.</p> </li> <li> <p>Support     for Multiple user level locks per connection     has been replaced by Oracle implementation, which is based on the same     contributed patch by Kostja Osipov.</p> </li> <li> <p>super-read-only option     has been replaced by     Oracle super_read_only     variable implementation.</p> </li> <li> <p>Mutex names in <code>SHOW ENGINE INNODB MUTEX</code> have been replaced by Oracle     mutex name implementation.</p> </li> <li>Percona Server for MySQL now uses packaging similar to the upstream     MySQL version. Most important change is that for Debian/Ubuntu     upgrades you now need to run <code>mysql_upgrade</code> manually.</li> </ul>"},{"location":"changed_in_57.html#list-of-status-variables-that-are-no-longer-available-in-percona-server-for-mysql-57","title":"List of status variables that are no longer available in Percona Server for MySQL 5.7","text":"<p>Following status variables available in Percona Server for MySQL 5.6 are no longer present in Percona Server for MySQL 5.7:</p> Status Variables Replaced by Com_purge_archived InnoDB redo log archiving has been removed due to lack of user uptake of the feature. Com_purge_archived_before_date InnoDB redo log archiving has been removed due to lack of user uptake of the feature. read_views_memory transaction descriptors replaced by the upstream implementation descriptors_memory transaction descriptors replaced by the upstream implementation innodb_mem_total This variable was always zero in 5.6 with the default innodb_use_sys_malloc setting innodb_deadlocks Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (lock_deadlocks) Innodb_ibuf_merges Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (ibuf_merges) Innodb_ibuf_merged_deletes Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (ibuf_merges_delete) Innodb_ibuf_merged_delete_marks Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (ibuf_merges_delete_mark) Innodb_ibuf_discarded_deletes Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (ibuf_merges_discard_delete) Innodb_ibuf_discarded_delete_marks Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (ibuf_merges_discard_delete_mark) Innodb_ibuf_discarded_inserts Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (ibuf_merges_discard_insert) Innodb_ibuf_merged_inserts Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (ibuf_merges_insert) Innodb_ibuf_size Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (ibuf_size) Innodb_s_lock_os_waits Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (innodb_rwlock_s_os_waits) Innodb_s_lock_spin_rounds Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (innodb_rwlock_s_spin_rounds) Innodb_s_lock_spin_waits Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (innodb_rwlock_s_spin_waits) Innodb_x_lock_os_waits Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (innodb_rwlock_x_os_waits) Innodb_x_lock_spin_rounds Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (innodb_rwlock_x_spin_rounds) Innodb_x_lock_spin_waits Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (innodb_rwlock_x_spin_waits) Innodb_current_row_locks Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (lock_row_lock_current_waits) Innodb_history_list_length Information now available in INFORMATION_SCHEMA.INNODB_METRICS table (trx_rseg_history_len) Innodb_mutex_os_waits SHOW ENGINE INNODB MUTEX presents the same information, but per-mutex instead of whole system aggregation Innodb_mutex_spin_rounds SHOW ENGINE INNODB MUTEX presents the same information, but per-mutex instead of whole system aggregation Innodb_mutex_spin_waits SHOW ENGINE INNODB MUTEX presents the same information, but per-mutex instead of whole system aggregation"},{"location":"changed_in_57.html#list-of-system-variables-that-are-no-longer-available-in-percona-server-for-mysql-57","title":"List of system variables that are no longer available in Percona Server for MySQL 5.7","text":"<p>Following system variables available in Percona Server for MySQL 5.6 are no longer present in Percona Server for MySQL 5.7:</p> <p>Waring</p> <p>Percona Server for MySQL 5.7 won\u2019t be able to start if some of these variables are set in the server\u2019s configuration file.</p> System Variables Feature Comment gtid_deployment_step Replaced by an upstream variable gtid_mode made dynamic. innodb_fake_changes Instead of replica prefetching using the fake changes, a 5.7 intra-schema parallel replication replica should be used. innodb_locking_fake_changes Instead of replica prefetching using the fake changes, a 5.7 intra-schema parallel replication replica should be used. innodb_log_archive InnoDB redo log archiving has been removed due to lack of user uptake of the feature. innodb_log_arch_dir InnoDB redo log archiving has been removed due to lack of user uptake of the feature. innodb_log_arch_expire_sec InnoDB redo log archiving has been removed due to lack of user uptake of the feature. innodb_log_block_size Replaced by upstream innodb_log_write_ahead_size variable. To avoid read on write when the storage block size is not equal to 512 bytes, the latter should be set to the same value the former was. If innodb_log_block_size was set to non-default values, new log files must be created during the upgrade. This can be done by cleanly shutting down the service and removing the variable from my.cnf configuration and removing the old logs and starting the service again before doing the upgrade. max_statement_time Replaced by upstream max_execution_time variable in Server-side SELECT statement timeouts implementation. have_statement_timeout Variable has been removed due to upstream feature implementation innodb_use_atomic_writes Variable has been removed due to upstream feature implementation innodb_adaptive_hash_index_partitions Replaced by upstream variable innodb_adaptive_hash_index_parts"},{"location":"changed_in_57.html#features-ported-from-percona-server-for-mysql-56-to-percona-server-for-mysql-57","title":"Features ported from Percona Server for MySQL 5.6 to Percona Server for MySQL 5.7","text":"<p>Following features were ported from Percona Server for MySQL 5.6 to Percona Server for MySQL 5.7:</p> Feature Ported Version Improved Buffer Pool Scalability Percona Server for MySQL 5.7.10-1 Improved InnoDB I/O Scalability Percona Server for MySQL 5.7.10-1 Query Cache Enhancements Percona Server for MySQL 5.7.10-1 Improved NUMA support Percona Server for MySQL 5.7.10-1 Thread Pool Percona Server for MySQL 5.7.10-1 XtraDB Performance Improvements for I/O-Bound Highly-Concurrent Workloads Percona Server for MySQL 5.7.10-1 Suppress Warning Messages Percona Server for MySQL 5.7.10-1 Improved MEMORY Storage Engine Percona Server for MySQL 5.7.10-1 Restricting the number of binlog files Percona Server for MySQL 5.7.10-1 Extended SELECT INTO OUTFILE/DUMPFILE Percona Server for MySQL 5.7.10-1 Per-query variable statement Percona Server for MySQL 5.7.10-1 Extended mysqlbinlog Percona Server for MySQL 5.7.10-1 Slow Query Log Rotation and Expiration Percona Server for MySQL 5.7.10-1 CSV engine mode for standard-compliant quote and comma parsing Percona Server for MySQL 5.7.10-1 Support for PROXY protocol Percona Server for MySQL 5.7.10-1 Per-session server-id Percona Server for MySQL 5.7.10-1 Too Many Connections Warning Percona Server for MySQL 5.7.10-1 Handle Corrupted Tables Percona Server for MySQL 5.7.10-1 Percona Toolkit UDFs Percona Server for MySQL 5.7.10-1 Kill Idle Transactions Percona Server for MySQL 5.7.10-1 Enforcing Storage Engine Percona Server for MySQL 5.7.10-1 Utility user Percona Server for MySQL 5.7.10-1 Expanded Program Option Modifiers Percona Server for MySQL 5.7.10-1 XtraDB changed page tracking Percona Server for MySQL 5.7.10-1 PAM Authentication Plugin Percona Server for MySQL 5.7.10-1 Expanded Fast Index Creation Percona Server for MySQL 5.7.10-1 Backup Locks Percona Server for MySQL 5.7.10-1 Audit Log Plugin Percona Server for MySQL 5.7.10-1 Start transaction with consistent snapshot Percona Server for MySQL 5.7.10-1 Extended SHOW GRANTS Percona Server for MySQL 5.7.10-1 User Statistics Percona Server for MySQL 5.7.10-1 Slow Query Log Percona Server for MySQL 5.7.10-1 Extended Show Engine InnoDB Status Percona Server for MySQL 5.7.10-1 Show Storage Engines Percona Server for MySQL 5.7.10-1 Process List Percona Server for MySQL 5.7.10-1 Misc. INFORMATION_SCHEMA Tables Percona Server for MySQL 5.7.10-1 Thread Based Profiling Percona Server for MySQL 5.7.10-1 Metrics for scalability measurement Percona Server for MySQL 5.7.10-1 Response Time Distribution Percona Server for MySQL 5.7.10-1"},{"location":"compile.html","title":"Compiling Percona Server for MySQL from Source","text":"<p>After either fetching the source repository or extracting a source tarball (from Percona or one you generated yourself), you must configure and build Percona Server. Do the following:</p> <ol> <li>Run <code>cmake</code> to configure the build. Specify the build options like you would for a MySQL build. You may require other options on your sever. </li> </ol> <p>This example configures Percona Server for     MySQL with similar options to what Percona uses to produce the     binaries:</p> <pre><code>$ cmake . -DCMAKE_BUILD_TYPE=RelWithDebInfo -DBUILD_CONFIG=mysql_release -DFEATURE_SET=community -DWITH_EMBEDDED_SERVER=OFF\n</code></pre> <p>Second, compile using <code>make</code>:</p> <pre><code>$ make\n</code></pre> <p>Third, install the compiled file:</p> <p><pre><code>$ make install\n</code></pre> Percona Server 5.7 is installed on your system.</p>"},{"location":"copyright-and-licensing-information.html","title":"Copyright and licensing information","text":""},{"location":"copyright-and-licensing-information.html#documentation-licensing","title":"Documentation licensing","text":"<p>Percona Server for MySQL documentation is (C)2009-2023 Percona LLC and/or its affiliates and is distributed under the Creative Commons Attribution 4.0 International License.</p>"},{"location":"copyright-and-licensing-information.html#software-license","title":"Software license","text":"<p>Percona Server is built upon MySQL from Oracle. Along with making our own modifications, we merge in changes from other sources such as community contributions and changes from MariaDB.</p> <p>The original SHOW USER/TABLE/INDEX statistics code came from Google.</p> <p>Percona does not require copyright assignment.</p> <p>See the COPYING files accompanying the software distribution.</p>"},{"location":"development.html","title":"Development of Percona Server for MySQL","text":"<p>Percona Server for MySQL is an open source project to produce a distribution of the MySQL Server with improved performance, scalability and diagnostics.</p>"},{"location":"development.html#submitting-changes","title":"Submitting Changes","text":"<p>We keep trunk in a constant state of stability to allow for a release at any time and to minimize wasted time by developers due to broken code.</p>"},{"location":"development.html#overview","title":"Overview","text":"<p>At Percona we use Git for source control, GitHub for code hosting, and Jira for release management.</p> <p>We change our software to implement new features and/or to fix bugs. Refactoring could be classed either as a new feature or a bug depending on the scope of work.</p> <p>New features and bugs are targeted to specific releases. A release is part of a series. For example, 5.7 is a series in Percona XtraBackup and 5.7.15, 5.7.16 and 5.7.17 are releases in this series.</p> <p>Code is proposed for merging in the form of pull requests on GitHub.</p> <p>For Percona Server for MySQL, we have Git branches on which development occurs:  5.7, and 8.0. As Percona Server for MySQL is not a traditional project, instead of being a set of patches against an existing product, these branches are not related. In other words, we do not merge from one release branch to another. To have your changes in several branches, you must propose branches to each release branch.</p>"},{"location":"development.html#making-a-change-to-a-project","title":"Making a Change to a Project","text":"<p>In this case, we are going to use <code>percona-xtrabackup</code> as an example. The workflow is similar for Percona Server for MySQL, but the patch will need to be modified in all release branches of Percona Server for MySQL.</p> <ul> <li> <p><code>git branch https://github.com/percona/percona-xtrabackup/featureX</code> (where \u2018featureX\u2019 is a sensible name for the task at hand)</p> </li> <li> <p>(developer makes changes in featureX, testing locally)</p> </li> <li> <p>The Developer pushes to <code>https://github.com/percona/username/percona-xtrabackup/featureX</code></p> </li> <li> <p>The developer can submit a pull request to https://github.com/percona/percona-xtrabackup,</p> </li> <li> <p>Code undergoes a review</p> </li> <li> <p>Once code is accepted, it can be merged</p> </li> </ul> <p>If the change also applies to a stable release (e.g. 2.4) then changes should be made on a branch of 2.4 and merged to a branch of trunk. In this case there should be two branches run through the param build and two merge proposals (one for the stable release and one with the changes merged to trunk). This prevents somebody else having to guess how to merge your changes.</p>"},{"location":"development.html#percona-server-for-mysql","title":"Percona Server for MySQL","text":"<p>Percona Server for MySQL has the same process, but we have different  branches and merge requests.</p>"},{"location":"faq.html","title":"Frequently Asked Questions","text":""},{"location":"faq.html#q-will-percona-server-for-mysql-with-xtradb-invalidate-our-mysql-support","title":"Q: Will Percona Server for MySQL with XtraDB invalidate our MySQL support?","text":"<p>A: We don\u2019t know the details of your support contract. You should check with your Oracle representative.</p>"},{"location":"faq.html#q-will-we-have-to-gpl-our-whole-application-if-we-use-percona-server-for-mysql-with-xtradb","title":"Q: Will we have to GPL our whole application if we use Percona Server for MySQL with XtraDB?","text":"<p>A: This is a common misconception about the GPL. We suggest reading the Free Software Foundation \u2018s excellent reference material on the GPL Version 2, which is the license that applies to MySQL and therefore to Percona Server for MySQL with XtraDB. That document contains links to many other documents which should answer your questions. Percona is unable to give legal advice about the GPL.</p>"},{"location":"faq.html#q-do-i-need-to-install-percona-client-libraries","title":"Q: Do I need to install Percona client libraries?","text":"<p>A: No, you don\u2019t need to change anything on the clients. Percona Server for MySQL is 100% compatible with all existing client libraries and connectors.</p>"},{"location":"faq.html#q-when-using-the-percona-xtrabackup-to-setup-a-replication-replica-on-debian-based-systems-im-getting-error-1045-28000-access-denied-for-user-debian-sys-maintlocalhost-using-password-yes","title":"Q: When using the Percona XtraBackup to setup a replication replica on Debian based systems I\u2019m getting: \u201cERROR 1045 (28000): Access denied for user \u2018debian-sys-maint\u2019@\u2019localhost\u2019 (using password: YES)\u201d","text":"<p>A: In case you\u2019re using init script on Debian based system to start  <code>mysqld</code>, be sure that the password for <code>debian-sys-maint</code> user has  been updated, and it\u2019s the same as that user\u2019s password from the server  that the backup has been taken from. Password can be seen and updated  in <code>/etc/mysql/debian.cnf</code>. For more information on how to set up a  replication replica using Percona XtraBackup see How to set up a  replica in 6 simple steps.</p>"},{"location":"feature_comparison.html","title":"Percona Server for MySQL Feature Comparison","text":"<p>Percona Server for MySQL is an enhanced drop-in replacement for MySQL. With Percona Server for MySQL,</p> <ul> <li> <p>Your queries will run faster and more consistently.</p> </li> <li> <p>You will consolidate servers on powerful hardware.</p> </li> <li> <p>You will delay sharding, or avoid it entirely.</p> </li> <li> <p>You will save money on hosting fees and power.</p> </li> <li> <p>You will spend less time tuning and administering.</p> </li> <li> <p>You will achieve higher uptime.</p> </li> <li> <p>You will troubleshoot without guesswork.</p> </li> </ul> <p>We provide these benefits by significantly enhancing Percona Server for MySQL as compared to the standard MySQL database server:</p> Features Percona Server 5.7.27 MySQL 5.7.27 Open source Yes Yes ACID Compliance Yes Yes Multi-Version Concurrency Control Yes Yes Row-Level Locking Yes Yes Automatic Crash Recovery Yes Yes Table Partitioning Yes Yes Views Yes Yes Subqueries Yes Yes Triggers Yes Yes Stored Procedures Yes Yes Foreign Keys Yes Yes GTID Replication Yes Yes Group Replication Yes Yes MyRocks Storage Engine Yes No TokuDB Storage Engine Yes No Extra Features for Developers Percona Server 5.7.27 MySQL 5.7.27 NoSQL Socket-Level Interface Yes Yes X API Support Yes Yes InnoDB Full-Text Search Improvements Yes No Extra Hash/Digest Functions Yes No Instrumentation and Troubleshooting Features Percona Server 5.7.27 MySQL 5.7.27 INFORMATION_SCHEMA Tables 71 61 Global Performance and Status Counters 432 357 Per-Table Performance Counters Yes No Per-Index Performance Counters Yes No Per-User Performance Counters Yes No Per-Client Performance Counters Yes No Per-Thread Performance Counters Yes No Global Query Response Time Statistics Yes No Enhanced SHOW ENGINE INNODB STATUS Yes No Undo Segment Information Yes No Temporary Tables Information Yes No Extended Slow Query Logging Yes No User Statistics Yes No Performance and Scalability Features Percona Server 5.7.27 MySQL 5.7.27 Improved scalability by splitting mutexes Yes No Improved MEMORY Storage Engine Yes No Improved Flushing Yes No Parallel Doublewrite Buffer Yes No Configurable Page Sizes Yes Yes Configurable Fast Index Creation Yes No Per-column Compression for VARCHAR/BLOB and JSON Yes No Compressed columns with Dictionaries Yes No Security Features Percona Server 5.7.27 MySQL 5.7.27 PAM Authentication Plugin Yes Enterprise-Only Audit Logging Plugin Yes Enterprise-only Encryption Features Percona Server 5.7.27 MySQL 5.7.27 Encrypt InnodDB data Yes Yes Encrypt InnoDB tablespaces Experimental No Encrypt InnoDB logs Experimental No Encrypt Binary logs Experimental No Encrypt temporary files Experimental No Key Rotation Experimental No Scrubbing Experimental No Enforce Encryption Experimental No Storing Keyring in a file Yes Yes Storing Keyring in a Hashicorp Vault Yes No Operational Improvements Percona Server 5.7.27 MySQL 5.7.27 Changed Page Tracking Yes Yes Threadpool Yes Enterprise-only Backup Locks Yes No Extended SHOW GRANTS Yes No Improved Handling of Corrupted Tables Yes No Ability to kill Idle Transactions Yes No Improvements to START TRANSACTION WITH CONSISTENT SNAPSHOT Yes No Features for Running Database as a Service (DBaaS) Percona Server 5.7.27 MySQL 5.7.27 Special Utility User Yes No Enforce a Specific Storage Engine Yes No Expanded Program Option Modifiers Yes No"},{"location":"git-source-tree.html","title":"Installing Percona Server for MySQL from the Git Source Tree","text":"<p>Percona uses the GitHub revision control system  for development. To build the latest Percona Server for MySQL from the source tree you must have <code>git</code> installed on your system.</p> <p>You can now fetch the latest Percona Server for MySQL 5.7 sources.</p> <pre><code>$ git clone https://github.com/percona/percona-server.git\n$ cd percona-server\n$ git checkout 5.7\n$ git submodule init\n$ git submodule update\n</code></pre> <p>If you have made changes to Percona Server for MySQL 5.7 and want to  distribute that work, you can generate a new source tarball, which is  process we follow for each release:</p> <pre><code>$ cmake .\n$ make dist\n</code></pre> <p>Follow the instructions in [Compiling Percona Server for MySQL] (installation/compile.md) to compile the server.</p>"},{"location":"glossary.html","title":"Glossary","text":""},{"location":"glossary.html#acid","title":"ACID","text":"<p>Set of properties that guarantee database transactions are processed  reliably. Stands for Atomicity, Consistency,  Isolation,  Durability.</p>"},{"location":"glossary.html#atomicity","title":"Atomicity","text":"<p>Atomicity means that database operations are applied following a \u201call or nothing\u201d rule. A transaction is either fully applied or not at all.</p>"},{"location":"glossary.html#consistency","title":"Consistency","text":"<p>Consistency means that each transaction that modifies the database takes it from one consistent state to another.</p>"},{"location":"glossary.html#durability","title":"Durability","text":"<p>Once a transaction is committed, it will remain so.</p>"},{"location":"glossary.html#foreign-key","title":"Foreign Key","text":"<p>A referential constraint between two tables. Example: A purchase order  in the purchase_orders table must have been made by a customer that exists in the customers table.</p>"},{"location":"glossary.html#isolation","title":"Isolation","text":"<p>The Isolation requirement means that no transaction can interfere with another.</p>"},{"location":"glossary.html#innodb","title":"InnoDB","text":"<p>A Storage Engine for MySQL and derivatives (Percona Server, MariaDB) originally written by Innobase Oy, since acquired by Oracle. It provides ACID compliant storage engine with foreign key support. As of MySQL version 5.5, InnoDB became the default storage engine on all platforms.</p>"},{"location":"glossary.html#jenkins","title":"Jenkins","text":"<p>Jenkins is a continuous integration system that we use to help ensure the continued quality of the software we produce. It helps us achieve the aims of:</p> <ul> <li> <p>no failed tests in trunk on any platform,</p> </li> <li> <p>aid developers in ensuring merge requests build and test on all platforms,</p> </li> <li> <p>no known performance regressions (without a damn good explanation). </p> </li> </ul>"},{"location":"glossary.html#lsn","title":"LSN","text":"<p>Log Serial Number. A term used in relation to the InnoDB or XtraDB storage engines.</p>"},{"location":"glossary.html#mariadb","title":"MariaDB","text":"<p>A fork of MySQL that is maintained primarily by Monty Program AB. It aims to add features, fix bugs while maintaining 100% backwards compatibility with MySQL.</p>"},{"location":"glossary.html#mycnf","title":"my.cnf","text":"<p>The file name of the default MySQL configuration file.</p>"},{"location":"glossary.html#myisam","title":"MyISAM","text":"<p>A MySQL storage engine that was the default until MySQL 5.5.</p>"},{"location":"glossary.html#mysql","title":"MySQL","text":"<p>An open source database that has spawned several distributions and forks. MySQL AB was the primary maintainer and distributor until bought by Sun Microsystems, which was then acquired by Oracle. As Oracle owns the MySQL trademark, the term MySQL is often used for the Oracle distribution of MySQL as distinct from the drop-in replacements such as MariaDB and Percona Server.</p>"},{"location":"glossary.html#numa","title":"NUMA","text":"<p>Non-Uniform Memory Access (NUMA) is a computer memory design used in multiprocessing, where the memory access time depends on the memory location relative to a processor. Under NUMA, a processor can access its own local memory faster than non-local memory, that is, memory local to another processor or memory shared between processors. The whole system may still operate as one unit, and all memory is basically accessible from everywhere, but at a potentially higher latency and lower performance.</p>"},{"location":"glossary.html#percona-server-for-mysql","title":"Percona Server for MySQL","text":"<p>Percona\u2019s branch of MySQL with performance and management improvements.</p>"},{"location":"glossary.html#storage-engine","title":"Storage Engine","text":"<p>A Storage Engine is a piece of software that implements the details of data storage and retrieval for a database system. This term is primarily used within the MySQL ecosystem due to it being the first widely used relational database to have an abstraction layer around storage. It is analogous to a Virtual File System layer in an Operating System. A VFS layer allows an operating system to read and write multiple file systems (e.g. FAT, NTFS, XFS, ext3) and a Storage Engine layer allows a database server to access tables stored in different engines (e.g. MyISAM, InnoDB).</p>"},{"location":"glossary.html#xtradb","title":"XtraDB","text":"<p>Percona\u2019s improved version of InnoDB providing performance, features and reliability above what is shipped by Oracle in InnoDB.</p>"},{"location":"index_info_schema_tables.html","title":"Index of INFORMATION_SCHEMA Tables","text":"<p>This is a list of the <code>INFORMATION_SCHEMA TABLES</code> that exist in Percona Server for MySQL with XtraDB. The entry for each table points to the page in the documentation where it\u2019s described.</p> <ul> <li> <p>INFORMATION_SCHEMA.CLIENT_STATISTICS</p> </li> <li> <p>INFORMATION_SCHEMA.GLOBAL_TEMPORARY_TABLES</p> </li> <li> <p>INFORMATION_SCHEMA.INDEX_STATISTICS</p> </li> <li> <p>INFORMATION_SCHEMA.INNODB_CHANGED_PAGES</p> </li> <li> <p>INFORMATION_SCHEMA.QUERY_RESPONSE_TIME</p> </li> <li> <p>INFORMATION_SCHEMA.TABLE_STATISTICS</p> </li> <li> <p>INFORMATION_SCHEMA.TEMPORARY_TABLES</p> </li> <li> <p>THREAD_STATISTICS</p> </li> <li> <p>INFORMATION_SCHEMA.USER_STATISTICS</p> </li> <li> <p>XTRADB_INTERNAL_HASH_TABLES</p> </li> <li> <p>XTRADB_READ_VIEW</p> </li> <li> <p>INFORMATION_SCHEMA.XTRADB_RSEG</p> </li> <li> <p>INFORMATION_SCHEMA.XTRADB_ZIP_DICT</p> </li> <li> <p>INFORMATION_SCHEMA.XTRADB_ZIP_DICT_COLS</p> </li> </ul>"},{"location":"installation.html","title":"Installing Percona Server for MySQL 5.7","text":"<p>Before installing, you might want to read the Percona Server for MySQL 5.7 Release notes.</p>"},{"location":"installation.html#installing-percona-server-for-mysql-from-repositories","title":"Installing Percona Server for MySQL from Repositories","text":"<p>Percona provides repositories for yum (<code>RPM</code> packages for Red Hat, CentOS and Amazon Linux AMI) and apt (<code>.deb</code> packages for Ubuntu and Debian) for software such as Percona Server for MySQL, Percona XtraBackup, and Percona Toolkit. This makes it easy to install and update your software and its dependencies through your operating system\u2019s package manager. This is the recommended way of installing where possible.</p> <p>Following guides describe the installation process for using the official Percona repositories for <code>.deb</code> and <code>.rpm</code> packages.</p> <ul> <li> <p>Installing Percona Server for MySQL on Debian and Ubuntu</p> </li> <li> <p>Installing Percona Server for MySQL on Red Hat Enterprise Linux and CentOS</p> </li> </ul>"},{"location":"installation.html#building-percona-server-for-mysql-debianubuntu-packages","title":"Building Percona Server for MySQL Debian/Ubuntu packages","text":"<p>If you wish to build your own Percona Server Debian/Ubuntu (dpkg) packages, you first need to start with a source tarball, either from the Percona website or by generating your own by following the instructions above( Installing Percona Server for MySQL from the Git Source Tree).</p> <p>Extract the source tarball:</p> <pre><code>$ tar xfz percona-server-5.7.10-3.tar.gz\n$ cd percona-server-5.7.10-3\n</code></pre> <p>Copy the debian packaging into the debian directory:</p> <pre><code>$ cp -ap build-ps/debian debian\n</code></pre> <p>Update the changelog for your distribution (here we update for the unstable distribution - sid), setting the version number appropriately. The trailing one in the version number is the revision of the Debian packaging.</p> <pre><code>$ dch -D unstable --force-distribution -v \"5.7.10-3-1\" \"Update to 5.7.10-3\"\n</code></pre> <p>Build the Debian source package:</p> <pre><code>$ dpkg-buildpackage -S\n</code></pre> <p>Use <code>sbuild</code> to build the binary package in a <code>chroot</code>:</p> <pre><code>$ sbuild -d sid percona-server-5.7_5.7.10_3-1.dsc\n</code></pre> <p>You can give different distribution options to <code>dch</code> and <code>sbuild</code> to build binary packages for all Debian and Ubuntu releases.</p> <p>Note</p> <p>The PAM Authentication Plugin is not built with the server by default. In order to build the Percona Server with the PAM plugin, add the  <code>-DWITH_PAM=ON</code> option.</p>"},{"location":"percona_xtradb.html","title":"The Percona XtraDB Storage Engine","text":"<p>Percona XtraDB is an enhanced version of the InnoDB storage engine, designed to better scale on modern hardware, and including a variety of other features useful in high performance environments. It is fully backwards compatible, and so can be used as a drop-in replacement for standard InnoDB.</p> <p>Percona XtraDB includes all of InnoDB \u2018s robust, reliable <code>ACID</code>-compliant design and advanced <code>MVCC</code> architecture, and builds on that solid foundation with more features, more ability to tune, more metrics, and more scalability. In particular, it is designed to scale better on many cores, to use memory more efficiently, and to be more convenient and useful. The new features are especially designed to alleviate some of InnoDB \u2018s limitations. We choose features and fixes based on customer requests and on our best judgment of real-world needs as a high-performance consulting company.</p> <p>Percona XtraDB engine will not have further binary releases, it is distributed as part of Percona Server for MySQL.</p>"},{"location":"ps-variables.html","title":"List of variables introduced in Percona Server 5.7","text":""},{"location":"ps-variables.html#system-variables","title":"System Variables","text":"Name Cmd-Line Option File Var Scope Dynamic Status audit_log_buffer_size Yes Yes Global No audit_log_file Yes Yes Global No audit_log_flush Yes Yes Global Yes audit_log_format Yes Yes Global No audit_log_handler Yes Yes Global No audit_log_policy Yes Yes Global Yes audit_log_rotate_on_size Yes Yes Global No audit_log_rotations Yes Yes Global No audit_log_strategy Yes Yes Global No audit_log_syslog_facility Yes Yes Global No audit_log_syslog_ident Yes Yes Global No audit_log_syslog_priority Yes Yes Global No binlog_space_limit Yes Yes Global Yes csv_mode Yes Yes Both Yes enforce_storage_engine Yes Yes Global No expand_fast_index_creation Yes No Both Yes extra_max_connections Yes Yes Global Yes extra_port Yes Yes Global No ft_query_extra_word_chars Yes Yes Both Yes have_backup_locks Yes No Global No have_backup_safe_binlog_info Yes No Global No have_snapshot_cloning Yes No Global No innodb_background_scrub_data_compressed Yes Yes Global Yes innodb_background_scrub_data_uncompressed Yes Yes Global Yes innodb_cleaner_lsn_age_factor Yes Yes Global Yes innodb_corrupt_table_action Yes Yes Global Yes innodb_default_encryption_key_id Yes Yes Session Yes innodb_empty_free_list_algorithm Yes Yes Global Yes innodb_encrypt_online_alter_logs Yes Yes Global Yes innodb_encrypt_tables Yes Yes Global Yes innodb_kill_idle_transaction Yes Yes Global Yes innodb_max_bitmap_file_size Yes Yes Global Yes innodb_max_changed_pages Yes Yes Global Yes innodb_online_encryption_rotate_key_age Yes Yes Global Yes Deprecated innodb_online_encryption_threads Yes Yes Global Yes Deprecated innodb_parallel_dblwr_encrypt Yes Yes Global Yes Deprecated 5.7.31-34 innodb_print_lock_wait_timeout_info Yes Yes Global Yes innodb_redo_log_encrypt Yes Yes Global Yes Deprecated: 5.7.31-34 innodb_scrub_log Yes Yes Global Yes innodb_scrub_log_speed Yes Yes Global Yes innodb_show_locks_held Yes Yes Global Yes innodb_show_verbose_locks Yes Yes Global Yes innodb_sys_tablespace_encrypt Yes Yes Global No Deprecated innodb_temp_tablespace_encrypt Yes Yes Global No Deprecated innodb_track_changed_pages Yes Yes Global No innodb_undo_log_encrypt Yes Yes Global Yes Deprecated innodb_use_global_flush_log_at_trx_commit Yes Yes Global Yes keyring_vault_config Yes Yes Global Yes keyring_vault_timeout Yes Yes Global Yes log_slow_filter Yes Yes Both Yes log_slow_rate_limit Yes Yes Both Yes log_slow_rate_type Yes Yes Global Yes log_slow_sp_statements Yes Yes Global Yes log_slow_verbosity Yes Yes Both Yes log_warnings_suppress Yes Yes Global Yes max_binlog_files Yes Yes Global Yes max_slowlog_files Yes Yes Global Yes max_slowlog_size Yes Yes Global Yes proxy_protocol_networks Yes Yes Global No pseudo_server_id Yes No Session Yes query_cache_strip_comments Yes Yes Global Yes query_response_time_flush Yes No Global No query_response_time_range_base Yes Yes Global Yes query_response_time_stats Yes Yes Global Yes slow_query_log_always_write_time Yes Yes Global Yes slow_query_log_use_global_control Yes Yes Global Yes thread_pool_high_prio_mode Yes Yes Both Yes thread_pool_high_prio_tickets Yes Yes Both Yes thread_pool_idle_timeout Yes Yes Global Yes thread_pool_max_threads Yes Yes Global Yes thread_pool_oversubscribe Yes Yes Global Yes thread_pool_size Yes Yes Global Yes thread_pool_stall_limit Yes Yes Global No thread_statistics Yes Yes Global Yes tokudb_alter_print_error tokudb_analyze_delete_fraction tokudb_analyze_in_background Yes Yes Both Yes tokudb_analyze_mode Yes Yes Both Yes tokudb_analyze_throttle Yes Yes Both Yes tokudb_analyze_time Yes Yes Both Yes tokudb_auto_analyze Yes Yes Both Yes tokudb_block_size tokudb_bulk_fetch tokudb_cache_size tokudb_cachetable_pool_threads Yes Yes Global No tokudb_cardinality_scale_percent tokudb_check_jemalloc tokudb_checkpoint_lock tokudb_checkpoint_on_flush_logs tokudb_checkpoint_pool_threads Yes Yes Global No tokudb_checkpointing_period tokudb_cleaner_iterations tokudb_cleaner_period tokudb_client_pool_threads Yes Yes Global No tokudb_commit_sync tokudb_compress_buffers_before_eviction Yes Yes Global No tokudb_create_index_online tokudb_data_dir tokudb_debug tokudb_directio tokudb_disable_hot_alter tokudb_disable_prefetching tokudb_disable_slow_alter tokudb_empty_scan tokudb_enable_partial_eviction Yes Yes Global No tokudb_fanout Yes Yes Both Yes tokudb_fs_reserve_percent tokudb_fsync_log_period tokudb_hide_default_row_format tokudb_killed_time tokudb_last_lock_timeout tokudb_load_save_space tokudb_loader_memory_size tokudb_lock_timeout tokudb_lock_timeout_debug tokudb_log_dir tokudb_max_lock_memory tokudb_optimize_index_fraction tokudb_optimize_index_name tokudb_optimize_throttle tokudb_pk_insert_mode tokudb_prelock_empty tokudb_read_block_size tokudb_read_buf_size tokudb_read_status_frequency :ref:`tokudb_row_formatref tokudb_rpl_check_readonly tokudb_rpl_lookup_rows tokudb_rpl_lookup_rows_delay tokudb_rpl_unique_checks tokudb_rpl_unique_checks_delay tokudb_strip_frm_data Yes Yes Global No tokudb_support_xa tokudb_tmp_dir tokudb_version tokudb_write_status_frequency userstat Yes Yes Global Yes version_comment Yes Yes Global Yes version_suffix Yes Yes Global Yes"},{"location":"ps-variables.html#status-variables","title":"Status variables","text":"Name Var Type Var Scope Binlog_snapshot_file String Global Binlog_snapshot_position Numeric Global Com_lock_binlog_for_backup Numeric Both Com_lock_tables_for_backup Numeric Both Com_show_client_statistics Numeric Both Com_show_index_statistics Numeric Both Com_show_table_statistics Numeric Both Com_show_thread_statistics Numeric Both Com_show_user_statistics Numeric Both Com_unlock_binlog Numeric Both Innodb_background_log_sync Numeric Global Innodb_buffer_pool_pages_LRU_flushed Numeric Global Innodb_buffer_pool_pages_made_not_young Numeric Global Innodb_buffer_pool_pages_made_young Numeric Global Innodb_buffer_pool_pages_old Numeric Global Innodb_checkpoint_age Numeric Global Innodb_checkpoint_max_age Numeric Global Innodb_ibuf_free_list Numeric Global Innodb_ibuf_segment_size Numeric Global Innodb_lsn_current Numeric Global Innodb_lsn_flushed Numeric Global Innodb_lsn_last_checkpoint Numeric Global Innodb_master_thread_active_loops Numeric Global Innodb_master_thread_idle_loops Numeric Global Innodb_max_trx_id Numeric Global Innodb_mem_adaptive_hash Numeric Global Innodb_mem_dictionary Numeric Global Innodb_oldest_view_low_limit_trx_id Numeric Global Innodb_purge_trx_id Numeric Global Innodb_purge_undo_no Numeric Global Threadpool_idle_threads Numeric Global Threadpool_threads Numeric Global Tokudb_DB_OPENS Tokudb_DB_CLOSES Tokudb_DB_OPEN_CURRENT Tokudb_DB_OPEN_MAX Tokudb_LEAF_ENTRY_MAX_COMMITTED_XR Tokudb_LEAF_ENTRY_MAX_PROVISIONAL_XR Tokudb_LEAF_ENTRY_EXPANDED Tokudb_LEAF_ENTRY_MAX_MEMSIZE Tokudb_LEAF_ENTRY_APPLY_GC_BYTES_IN Tokudb_LEAF_ENTRY_APPLY_GC_BYTES_OUT Tokudb_LEAF_ENTRY_NORMAL_GC_BYTES_IN Tokudb_LEAF_ENTRY_NORMAL_GC_BYTES_OUT Tokudb_CHECKPOINT_PERIOD Tokudb_CHECKPOINT_FOOTPRINT Tokudb_CHECKPOINT_LAST_BEGAN Tokudb_CHECKPOINT_LAST_COMPLETE_BEGAN Tokudb_CHECKPOINT_LAST_COMPLETE_ENDED Tokudb_CHECKPOINT_DURATION Tokudb_CHECKPOINT_DURATION_LAST Tokudb_CHECKPOINT_LAST_LSN Tokudb_CHECKPOINT_TAKEN Tokudb_CHECKPOINT_FAILED Tokudb_CHECKPOINT_WAITERS_NOW Tokudb_CHECKPOINT_WAITERS_MAX Tokudb_CHECKPOINT_CLIENT_WAIT_ON_MO Tokudb_CHECKPOINT_CLIENT_WAIT_ON_CS Tokudb_CHECKPOINT_BEGIN_TIME Tokudb_CHECKPOINT_LONG_BEGIN_TIME Tokudb_CHECKPOINT_LONG_BEGIN_COUNT Tokudb_CHECKPOINT_END_TIME Tokudb_CHECKPOINT_LONG_END_TIME Tokudb_CHECKPOINT_LONG_END_COUNT Tokudb_CACHETABLE_MISS Tokudb_CACHETABLE_MISS_TIME Tokudb_CACHETABLE_PREFETCHES Tokudb_CACHETABLE_SIZE_CURRENT Tokudb_CACHETABLE_SIZE_LIMIT Tokudb_CACHETABLE_SIZE_WRITING Tokudb_CACHETABLE_SIZE_NONLEAF Tokudb_CACHETABLE_SIZE_LEAF Tokudb_CACHETABLE_SIZE_ROLLBACK Tokudb_CACHETABLE_SIZE_CACHEPRESSURE Tokudb_CACHETABLE_SIZE_CLONED Tokudb_CACHETABLE_EVICTIONS Tokudb_CACHETABLE_CLEANER_EXECUTIONS Tokudb_CACHETABLE_CLEANER_PERIOD Tokudb_CACHETABLE_CLEANER_ITERATIONS Tokudb_CACHETABLE_WAIT_PRESSURE_COUNT Tokudb_CACHETABLE_WAIT_PRESSURE_TIME Tokudb_CACHETABLE_LONG_WAIT_PRESSURE_COUNT Tokudb_CACHETABLE_LONG_WAIT_PRESSURE_TIME Tokudb_CACHETABLE_POOL_CLIENT_NUM_THREADS Tokudb_CACHETABLE_POOL_CLIENT_NUM_THREADS_ACTIVE Tokudb_CACHETABLE_POOL_CLIENT_QUEUE_SIZE Tokudb_CACHETABLE_POOL_CLIENT_MAX_QUEUE_SIZE Tokudb_CACHETABLE_POOL_CLIENT_TOTAL_ITEMS_PROCESSED Tokudb_CACHETABLE_POOL_CLIENT_TOTAL_EXECUTION_TIME Tokudb_CACHETABLE_POOL_CACHETABLE_NUM_THREADS Tokudb_CACHETABLE_POOL_CACHETABLE_NUM_THREADS_ACTIVE Tokudb_CACHETABLE_POOL_CACHETABLE_QUEUE_SIZE Tokudb_CACHETABLE_POOL_CACHETABLE_MAX_QUEUE_SIZE Tokudb_CACHETABLE_POOL_CACHETABLE_TOTAL_ITEMS_PROCESSED Tokudb_CACHETABLE_POOL_CACHETABLE_TOTAL_EXECUTION_TIME Tokudb_CACHETABLE_POOL_CHECKPOINT_NUM_THREADS Tokudb_CACHETABLE_POOL_CHECKPOINT_NUM_THREADS_ACTIVE Tokudb_CACHETABLE_POOL_CHECKPOINT_QUEUE_SIZE Tokudb_CACHETABLE_POOL_CHECKPOINT_MAX_QUEUE_SIZE Tokudb_CACHETABLE_POOL_CHECKPOINT_TOTAL_ITEMS_PROCESSED Tokudb_CACHETABLE_POOL_CHECKPOINT_TOTAL_EXECUTION_TIME Tokudb_LOCKTREE_MEMORY_SIZE Tokudb_LOCKTREE_MEMORY_SIZE_LIMIT Tokudb_LOCKTREE_ESCALATION_NUM Tokudb_LOCKTREE_ESCALATION_SECONDS Tokudb_LOCKTREE_LATEST_POST_ESCALATION_MEMORY_SIZE Tokudb_LOCKTREE_OPEN_CURRENT Tokudb_LOCKTREE_PENDING_LOCK_REQUESTS Tokudb_LOCKTREE_STO_ELIGIBLE_NUM Tokudb_LOCKTREE_STO_ENDED_NUM Tokudb_LOCKTREE_STO_ENDED_SECONDS Tokudb_LOCKTREE_WAIT_COUNT Tokudb_LOCKTREE_WAIT_TIME Tokudb_LOCKTREE_LONG_WAIT_COUNT Tokudb_LOCKTREE_LONG_WAIT_TIME Tokudb_LOCKTREE_TIMEOUT_COUNT Tokudb_LOCKTREE_WAIT_ESCALATION_COUNT Tokudb_LOCKTREE_WAIT_ESCALATION_TIME Tokudb_LOCKTREE_LONG_WAIT_ESCALATION_COUNT Tokudb_LOCKTREE_LONG_WAIT_ESCALATION_TIME Tokudb_DICTIONARY_UPDATES Tokudb_DICTIONARY_BROADCAST_UPDATES Tokudb_DESCRIPTOR_SET Tokudb_MESSAGES_IGNORED_BY_LEAF_DUE_TO_MSN Tokudb_TOTAL_SEARCH_RETRIES Tokudb_SEARCH_TRIES_GT_HEIGHT Tokudb_SEARCH_TRIES_GT_HEIGHTPLUS3 Tokudb_LEAF_NODES_FLUSHED_NOT_CHECKPOINT Tokudb_LEAF_NODES_FLUSHED_NOT_CHECKPOINT_BYTES Tokudb_LEAF_NODES_FLUSHED_NOT_CHECKPOINT_UNCOMPRESSED_BYTES Tokudb_LEAF_NODES_FLUSHED_NOT_CHECKPOINT_SECONDS Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_NOT_CHECKPOINT Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_NOT_CHECKPOINT_BYTES Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_NOT_CHECKPOINT_UNCOMPRESSE Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_NOT_CHECKPOINT_SECONDS Tokudb_LEAF_NODES_FLUSHED_CHECKPOINT Tokudb_LEAF_NODES_FLUSHED_CHECKPOINT_BYTES Tokudb_LEAF_NODES_FLUSHED_CHECKPOINT_UNCOMPRESSED_BYTES Tokudb_LEAF_NODES_FLUSHED_CHECKPOINT_SECONDS Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_CHECKPOINT Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_CHECKPOINT_BYTES Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_CHECKPOINT_UNCOMPRESSED_BY Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_CHECKPOINT_SECONDS Tokudb_LEAF_NODE_COMPRESSION_RATIO Tokudb_NONLEAF_NODE_COMPRESSION_RATIO Tokudb_OVERALL_NODE_COMPRESSION_RATIO Tokudb_NONLEAF_NODE_PARTIAL_EVICTIONS Tokudb_NONLEAF_NODE_PARTIAL_EVICTIONS_BYTES Tokudb_LEAF_NODE_PARTIAL_EVICTIONS Tokudb_LEAF_NODE_PARTIAL_EVICTIONS_BYTES Tokudb_LEAF_NODE_FULL_EVICTIONS Tokudb_LEAF_NODE_FULL_EVICTIONS_BYTES Tokudb_NONLEAF_NODE_FULL_EVICTIONS Tokudb_NONLEAF_NODE_FULL_EVICTIONS_BYTES Tokudb_LEAF_NODES_CREATED Tokudb_NONLEAF_NODES_CREATED Tokudb_LEAF_NODES_DESTROYED Tokudb_NONLEAF_NODES_DESTROYED Tokudb_MESSAGES_INJECTED_AT_ROOT_BYTES Tokudb_MESSAGES_FLUSHED_FROM_H1_TO_LEAVES_BYTES Tokudb_MESSAGES_IN_TREES_ESTIMATE_BYTES Tokudb_MESSAGES_INJECTED_AT_ROOT Tokudb_BROADCASE_MESSAGES_INJECTED_AT_ROOT Tokudb_BASEMENTS_DECOMPRESSED_TARGET_QUERY Tokudb_BASEMENTS_DECOMPRESSED_PRELOCKED_RANGE Tokudb_BASEMENTS_DECOMPRESSED_PREFETCH Tokudb_BASEMENTS_DECOMPRESSED_FOR_WRITE Tokudb_BUFFERS_DECOMPRESSED_TARGET_QUERY Tokudb_BUFFERS_DECOMPRESSED_PRELOCKED_RANGE Tokudb_BUFFERS_DECOMPRESSED_PREFETCH Tokudb_BUFFERS_DECOMPRESSED_FOR_WRITE Tokudb_PIVOTS_FETCHED_FOR_QUERY Tokudb_PIVOTS_FETCHED_FOR_QUERY_BYTES Tokudb_PIVOTS_FETCHED_FOR_QUERY_SECONDS Tokudb_PIVOTS_FETCHED_FOR_PREFETCH Tokudb_PIVOTS_FETCHED_FOR_PREFETCH_BYTES Tokudb_PIVOTS_FETCHED_FOR_PREFETCH_SECONDS Tokudb_PIVOTS_FETCHED_FOR_WRITE Tokudb_PIVOTS_FETCHED_FOR_WRITE_BYTES Tokudb_PIVOTS_FETCHED_FOR_WRITE_SECONDS Tokudb_BASEMENTS_FETCHED_TARGET_QUERY Tokudb_BASEMENTS_FETCHED_TARGET_QUERY_BYTES Tokudb_BASEMENTS_FETCHED_TARGET_QUERY_SECONDS Tokudb_BASEMENTS_FETCHED_PRELOCKED_RANGE Tokudb_BASEMENTS_FETCHED_PRELOCKED_RANGE_BYTES Tokudb_BASEMENTS_FETCHED_PRELOCKED_RANGE_SECONDS Tokudb_BASEMENTS_FETCHED_PREFETCH Tokudb_BASEMENTS_FETCHED_PREFETCH_BYTES Tokudb_BASEMENTS_FETCHED_PREFETCH_SECONDS Tokudb_BASEMENTS_FETCHED_FOR_WRITE Tokudb_BASEMENTS_FETCHED_FOR_WRITE_BYTES Tokudb_BASEMENTS_FETCHED_FOR_WRITE_SECONDS Tokudb_BUFFERS_FETCHED_TARGET_QUERY Tokudb_BUFFERS_FETCHED_TARGET_QUERY_BYTES Tokudb_BUFFERS_FETCHED_TARGET_QUERY_SECONDS Tokudb_BUFFERS_FETCHED_PRELOCKED_RANGE Tokudb_BUFFERS_FETCHED_PRELOCKED_RANGE_BYTES Tokudb_BUFFERS_FETCHED_PRELOCKED_RANGE_SECONDS Tokudb_BUFFERS_FETCHED_PREFETCH Tokudb_BUFFERS_FETCHED_PREFETCH_BYTES Tokudb_BUFFERS_FETCHED_PREFETCH_SECONDS Tokudb_BUFFERS_FETCHED_FOR_WRITE Tokudb_BUFFERS_FETCHED_FOR_WRITE_BYTES Tokudb_BUFFERS_FETCHED_FOR_WRITE_SECONDS Tokudb_LEAF_COMPRESSION_TO_MEMORY_SECONDS Tokudb_LEAF_SERIALIZATION_TO_MEMORY_SECONDS Tokudb_LEAF_DECOMPRESSION_TO_MEMORY_SECONDS Tokudb_LEAF_DESERIALIZATION_TO_MEMORY_SECONDS Tokudb_NONLEAF_COMPRESSION_TO_MEMORY_SECONDS Tokudb_NONLEAF_SERIALIZATION_TO_MEMORY_SECONDS Tokudb_NONLEAF_DECOMPRESSION_TO_MEMORY_SECONDS Tokudb_NONLEAF_DESERIALIZATION_TO_MEMORY_SECONDS Tokudb_PROMOTION_ROOTS_SPLIT Tokudb_PROMOTION_LEAF_ROOTS_INJECTED_INTO Tokudb_PROMOTION_H1_ROOTS_INJECTED_INTO Tokudb_PROMOTION_INJECTIONS_AT_DEPTH_0 Tokudb_PROMOTION_INJECTIONS_AT_DEPTH_1 Tokudb_PROMOTION_INJECTIONS_AT_DEPTH_2 Tokudb_PROMOTION_INJECTIONS_AT_DEPTH_3 Tokudb_PROMOTION_INJECTIONS_LOWER_THAN_DEPTH_3 Tokudb_PROMOTION_STOPPED_NONEMPTY_BUFFER Tokudb_PROMOTION_STOPPED_AT_HEIGHT_1 Tokudb_PROMOTION_STOPPED_CHILD_LOCKED_OR_NOT_IN_MEMORY Tokudb_PROMOTION_STOPPED_CHILD_NOT_FULLY_IN_MEMORY Tokudb_PROMOTION_STOPPED_AFTER_LOCKING_CHILD Tokudb_BASEMENT_DESERIALIZATION_FIXED_KEY Tokudb_BASEMENT_DESERIALIZATION_VARIABLE_KEY Tokudb_PRO_RIGHTMOST_LEAF_SHORTCUT_SUCCESS Tokudb_PRO_RIGHTMOST_LEAF_SHORTCUT_FAIL_POS Tokudb_RIGHTMOST_LEAF_SHORTCUT_FAIL_REACTIVE Tokudb_CURSOR_SKIP_DELETED_LEAF_ENTRY Tokudb_FLUSHER_CLEANER_TOTAL_NODES Tokudb_FLUSHER_CLEANER_H1_NODES Tokudb_FLUSHER_CLEANER_HGT1_NODES Tokudb_FLUSHER_CLEANER_EMPTY_NODES Tokudb_FLUSHER_CLEANER_NODES_DIRTIED Tokudb_FLUSHER_CLEANER_MAX_BUFFER_SIZE Tokudb_FLUSHER_CLEANER_MIN_BUFFER_SIZE Tokudb_FLUSHER_CLEANER_TOTAL_BUFFER_SIZE Tokudb_FLUSHER_CLEANER_MAX_BUFFER_WORKDONE Tokudb_FLUSHER_CLEANER_MIN_BUFFER_WORKDONE Tokudb_FLUSHER_CLEANER_TOTAL_BUFFER_WORKDONE Tokudb_FLUSHER_CLEANER_NUM_LEAF_MERGES_STARTED Tokudb_FLUSHER_CLEANER_NUM_LEAF_MERGES_RUNNING Tokudb_FLUSHER_CLEANER_NUM_LEAF_MERGES_COMPLETED Tokudb_FLUSHER_CLEANER_NUM_DIRTIED_FOR_LEAF_MERGE Tokudb_FLUSHER_FLUSH_TOTAL Tokudb_FLUSHER_FLUSH_IN_MEMORY Tokudb_FLUSHER_FLUSH_NEEDED_IO Tokudb_FLUSHER_FLUSH_CASCADES Tokudb_FLUSHER_FLUSH_CASCADES_1 Tokudb_FLUSHER_FLUSH_CASCADES_2 Tokudb_FLUSHER_FLUSH_CASCADES_3 Tokudb_FLUSHER_FLUSH_CASCADES_4 Tokudb_FLUSHER_FLUSH_CASCADES_5 Tokudb_FLUSHER_FLUSH_CASCADES_GT_5 Tokudb_FLUSHER_SPLIT_LEAF Tokudb_FLUSHER_SPLIT_NONLEAF Tokudb_FLUSHER_MERGE_LEAF Tokudb_FLUSHER_MERGE_NONLEAF Tokudb_FLUSHER_BALANCE_LEAF Tokudb_HOT_NUM_STARTED Tokudb_HOT_NUM_COMPLETED Tokudb_HOT_NUM_ABORTED Tokudb_HOT_MAX_ROOT_FLUSH_COUNT Tokudb_TXN_BEGIN Tokudb_TXN_BEGIN_READ_ONLY Tokudb_TXN_COMMITS Tokudb_TXN_ABORTS Tokudb_LOGGER_NEXT_LSN Tokudb_LOGGER_WRITES Tokudb_LOGGER_WRITES_BYTES Tokudb_LOGGER_WRITES_UNCOMPRESSED_BYTES Tokudb_LOGGER_WRITES_SECONDS Tokudb_LOGGER_WAIT_LONG Tokudb_LOADER_NUM_CREATED Tokudb_LOADER_NUM_CURRENT Tokudb_LOADER_NUM_MAX Tokudb_MEMORY_MALLOC_COUNT Tokudb_MEMORY_FREE_COUNT Tokudb_MEMORY_REALLOC_COUNT Tokudb_MEMORY_MALLOC_FAIL Tokudb_MEMORY_REALLOC_FAIL Tokudb_MEMORY_REQUESTED Tokudb_MEMORY_USED Tokudb_MEMORY_FREED Tokudb_MEMORY_MAX_REQUESTED_SIZE Tokudb_MEMORY_LAST_FAILED_SIZE Tokudb_MEM_ESTIMATED_MAXIMUM_MEMORY_FOOTPRINT Tokudb_MEMORY_MALLOCATOR_VERSION Tokudb_MEMORY_MMAP_THRESHOLD Tokudb_FILESYSTEM_THREADS_BLOCKED_BY_FULL_DISK Tokudb_FILESYSTEM_FSYNC_TIME Tokudb_FILESYSTEM_FSYNC_NUM Tokudb_FILESYSTEM_LONG_FSYNC_TIME Tokudb_FILESYSTEM_LONG_FSYNC_NUM"},{"location":"ps-versions-comparison.html","title":"List of features available in Percona Server for MySQL releases","text":"Percona Server for MySQL 5.7 Percona Server for MySQL 8.0 Improved Buffer Pool Scalability Improved Buffer Pool Scalability Improved InnoDB I/O Scalability Improved InnoDB I/O Scalability Multiple Adaptive Hash Search Partitions Multiple Adaptive Hash Search Partitions Atomic write support for Fusion-io devices Atomic write support for Fusion-io devices Query Cache Enhancements Feature not implemented Improved NUMA support Feature not implemented Thread Pool Thread Pool Suppress Warning Messages Suppress Warning Messages Ability to change the database for mysqlbinlog Ability to change the database for mysqlbinlog Fixed Size for the Read Ahead Area Fixed Size for the Read Ahead Area Improved MEMORY Storage Engine Improved MEMORY Storage Engine Restricting the number of binlog files Restricting the number of binlog files Ignoring missing tables in mysqldump Ignoring missing tables in mysqldump Too Many Connections Warning Too Many Connections Warning Handle Corrupted Tables Handle Corrupted Tables Lock-Free SHOW SLAVE STATUS Lock-Free SHOW REPLICA STATUS Expanded Fast Index Creation Expanded Fast Index Creation Percona Toolkit UDFs Percona Toolkit UDFs Support for Fake Changes Support for Fake Changes Kill Idle Transactions Kill Idle Transactions XtraDB changed page tracking XtraDB changed page tracking Enforcing Storage Engine Replaced with upstream implementation Utility user Utility user Extending the secure-file-priv server option Extending the secure-file-priv server option Expanded Program Option Modifiers Feature not implemented PAM Authentication Plugin PAM Authentication Plugin Log Archiving for XtraDB Log Archiving for XtraDB User Statistics User Statistics Slow Query Log Slow Query Log Count InnoDB Deadlocks Count InnoDB Deadlocks Log All Client Commands (syslog) Log All Client Commands (syslog) Response Time Distribution Feature not implemented Show Storage Engines Show Storage Engines Show Lock Names Show Lock Names Process List Process List Misc. INFORMATION_SCHEMA Tables Misc. INFORMATION_SCHEMA Tables Extended Show Engine InnoDB Status Extended Show Engine InnoDB Status Thread Based Profiling Thread Based Profiling XtraDB Performance Improvements for I/O-Bound Highly-Concurrent Workloads XtraDB Performance Improvements for I/O-Bound Highly-Concurrent Workloads Page cleaner thread tuning Page cleaner thread tuning Statement Timeout Statement Timeout Extended SELECT INTO OUTFILE/DUMPFILE Extended SELECT INTO OUTFILE/DUMPFILE Per-query variable statement Per-query variable statement Extended mysqlbinlog Extended mysqlbinlog Slow Query Log Rotation and Expiration Slow Query Log Rotation and Expiration Metrics for scalability measurement Feature not implemented Audit Log Audit Log Backup Locks Backup Locks CSV engine mode for a standard-compliant quote and comma parsing CSV engine mode for a standard-compliant quote and comma parsing Super read-only Super read-only"},{"location":"ps-versions-comparison.html#other-reading","title":"Other Reading","text":"<ul> <li> <p>What Is New in MySQL 5.7</p> </li> <li> <p>What Is New in MySQL 8.0</p> </li> </ul>"},{"location":"server-version-numbers.html","title":"Understand version numbers","text":"<p>A version number identifies the product release. The product contains the latest Generally Available (GA) features at the time of that release.</p> 5.7.39 -42. 2 Base version Minor build version Custom build <p>Percona uses semantic version numbering, which follows the pattern of base version, minor build version, and an optional custom build. Percona assigns unique, non-negative integers in increasing order for each minor build release. The version number combines the base MySQL 5.7 version number, the minor build version, and the custom build version, if needed.</p> <p>For example, the version numbers for Percona Server for MySQL 5.7.39-42.2 define the following information:</p> <ul> <li> <p>Base version - the leftmost numbers indicate MySQL 5.7 version used as a base. An increase in base version resets the minor build version to 0.</p> </li> <li> <p>Minor build version - an internal number that increases by one every time Percona Server for MySQL is released.</p> </li> <li> <p>Custom build version - an optional number assigned to custom builds used for bug fixes. The software features, unless they\u2019re included in the bug fix, don\u2019t change.</p> </li> </ul> <p>Percona Server for MySQL 5.7.18-14, 5.7.18-15, and 5.7.18-16 are multiple releases based on MySQL 5.7.18. </p>"},{"location":"trademark-policy.html","title":"Trademark policy","text":"<p>This Trademark Policy is to ensure that users of Percona-branded products or services know that what they receive has really been developed, approved, tested and maintained by Percona. Trademarks help to prevent confusion in the marketplace, by distinguishing one company\u2019s or person\u2019s products and services from another\u2019s.</p> <p>Percona owns a number of marks, including but not limited to Percona, XtraDB, Percona XtraDB, XtraBackup, Percona XtraBackup, Percona Server, and Percona Live, plus the distinctive visual icons and logos associated with these marks. Both the unregistered and registered marks of Percona are protected.</p> <p>Use of any Percona trademark in the name, URL, or other identifying characteristic of any product, service, website, or other use is not permitted without Percona\u2019s written permission with the following three limited exceptions.</p> <p>First, you may use the appropriate Percona mark when making a nominative fair use reference to a bona fide Percona product.</p> <p>Second, when Percona has released a product under a version of the GNU General Public License (\u201cGPL\u201d), you may use the appropriate Percona mark when distributing a verbatim copy of that product in accordance with the terms and conditions of the GPL.</p> <p>Third, you may use the appropriate Percona mark to refer to a distribution of GPL-released  Percona software that has been modified with minor changes for the sole purpose of allowing the software to operate on an operating system or hardware platform for which Percona has not yet released the software, provided that those third party changes do not affect the behavior, functionality, features, design or performance of the software. Users who acquire this Percona-branded software receive substantially exact implementations of the Percona software.</p> <p>Percona reserves the right to revoke this authorization at any time in its sole discretion.  For example, if Percona believes that your modification is beyond the scope of the limited license granted in this Policy or that your use of the Percona mark is detrimental to Percona, Percona will revoke this authorization.  Upon revocation, you must immediately cease using the applicable Percona mark.  If you do not immediately cease using the Percona mark upon revocation, Percona may take action to protect its rights and interests in the Percona mark.  Percona does not grant any license to use any Percona mark for any other modified versions of Percona software; such use will require our prior written permission.</p> <p>Neither trademark law nor any of the exceptions set forth in this Trademark Policy permit you to truncate, modify or otherwise use any Percona mark as part of your own brand.  For example, if XYZ creates a modified version of the Percona Server, XYZ may not brand that modification as \u201cXYZ Percona Server\u201d or \u201cPercona XYZ Server\u201d, even if that modification otherwise complies with the third exception noted above.</p> <p>In all cases, you must comply with applicable law, the underlying license, and this Trademark Policy, as amended from time to time.  For instance, any mention of Percona trademarks should include the full trademarked name, with proper spelling and capitalization, along with attribution of ownership to Percona Inc.  For example, the full proper name for XtraBackup is Percona XtraBackup. However, it is acceptable to omit the word \u201cPercona\u201d for brevity on the second and subsequent uses, where such omission does not cause confusion.</p> <p>In the event of doubt as to any of the conditions or exceptions outlined in this Trademark Policy, please contact trademarks@percona.com for assistance and we will do our very best to be helpful.</p>"},{"location":"upgrade-distribution.html","title":"Performing a Distribution upgrade in-place on a System with installed Percona packages","text":"<p>The recommended process for performing a distribution upgrade on a system with the Percona packages installed is the following:</p> <ol> <li> <p>Record the installed Percona packages</p> </li> <li> <p>Backup the data and configurations</p> </li> <li> <p>Uninstall the Percona packages without removing the configurations or data</p> </li> <li> <p>Perform the upgrade by following the distribution upgrade instructions</p> </li> <li> <p>Reboot the system</p> </li> <li> <p>Install the Percona packages intended for the upgraded version of the distribution</p> </li> </ol>"},{"location":"upgrade-repos.html","title":"Upgrading using the Percona repositories","text":"<p>The easiest and recommended way of installing - where possible - is by using the Percona repositories.</p>"},{"location":"upgrade-repos.html#deb-based-distributions","title":"<code>DEB</code>-based distributions","text":"<p>Note</p> <p>Following commands will need to be run either as a root user or with sudo.</p> <p>Having done the full backup (or dump if possible), stop the server:</p> <pre><code>$ service mysql stop\n</code></pre> <p>and proceed to do the modifications needed in your configuration file, as explained at the beginning of this guide.</p> <p>Note</p> <p>If you\u2019re running Debian/Ubuntu system with systemd as the default system and service manager you can invoke the above command with systemctl instead of service. Currently, both are supported.</p> <p>Then install the new server with:</p> <pre><code>$ apt install percona-server-server-5.7\n</code></pre> <p>If you\u2019re using Percona Server for MySQL 5.6 with TokuDB you\u2019ll need to specify the TokuDB package as well:</p> <pre><code>$ apt install percona-server-server-5.7 percona-server-tokudb-5.7\n</code></pre> <p>The installation script will NOT run automatically mysql_upgrade as it was the case in previous versions. You\u2019ll need to run the command manually and restart the service after it\u2019s finished.</p> <p><pre><code>$ mysql_upgrade\n</code></pre> The output should be similar to the following:</p> <p><pre><code>Checking if update is needed.\nChecking server version.\nRunning queries to upgrade MySQL server.\nChecking system database.\nmysql.columns_priv                                 OK\nmysql.db                                           OK\nmysql.engine_cost                                  OK\n...\nUpgrade process completed successfully.\nChecking if update is needed.\n</code></pre> Restart the service.</p> <pre><code>$ service mysql restart\n</code></pre> <p>Note that this procedure is the same for upgrading from MySQL 5.6 or 5.7 to Percona Server for MySQL 5.7.</p>"},{"location":"upgrade-repos.html#rpm-based-distributions","title":"<code>RPM</code>-based distributions","text":"<p>Note</p> <p>Following commands will need to be run either as a root user or with sudo.</p> <p>Having done the full backup (and dump if possible), stop the server:</p> <p>Note</p> <p>If you\u2019re running RHEL/CentOS system with systemd as the default system and service manager you can invoke the above command with systemctl instead of service. Currently, both are supported.</p> <pre><code>$ service mysql stop\n</code></pre> <p>Check your installed packages with:</p> <pre><code>$ rpm -qa | grep Percona-Server\n</code></pre> <p>The output should be similar to the following: </p> <pre><code>Percona-Server-shared-56-5.6.28-rel76.1.el7.x86_64\nPercona-Server-server-56-5.6.28-rel76.1.el7.x86_64\nPercona-Server-devel-56-5.6.28-rel76.1.el7.x86_64\nPercona-Server-client-56-5.6.28-rel76.1.el7.x86_64\nPercona-Server-test-56-5.6.28-rel76.1.el7.x86_64\nPercona-Server-56-debuginfo-5.6.28-rel76.1.el7.x86_64\n</code></pre> <p>After checking, proceed to remove them without dependencies:</p> <pre><code>$ rpm -qa | grep Percona-Server | xargs rpm -e --nodeps\n</code></pre> <p>It is important that you remove it without dependencies as many  packages may depend on these packages, since they replace <code>mysql</code>, and  will be  removed if omitted.</p> <p>Note that this procedure is the same for upgrading from MySQL 5.6 or 5.7 to Percona Server for MySQL 5.7: just grep <code>'^mysql-'</code> instead of <code>Percona-Server</code> and remove them.</p> <p>Install <code>Percona-Server-server-57</code>:</p> <pre><code>$ yum install Percona-Server-server-57\n</code></pre> <p>Percona Server for MySQL 5.6 with TokuDB, specify the TokuDB package as  well when doing the upgrade:</p> <pre><code>$ yum install Percona-Server-server-57 Percona-Server-tokudb-57\n</code></pre> <p>Once installed, proceed to modify your configuration file - <code>my.cnf</code> - and reinstall the plugins if necessary.</p> <p>Note</p> <p>If you\u2019re using TokuDB storage engine you\u2019ll need to comment out all the TokuDB specific variables in your configuration file(s) before starting the server, otherwise server won\u2019t be able to start. RHEL/CentOS 7 automatically backs up the previous configuration file to <code>/etc/my.cnf.rpmsave</code> and installs the default <code>my.cnf</code>. After upgrade/install process completes you can move the old configuration file back (after you remove all the unsupported system variables).</p> <p>You can now start the <code>mysql</code> service:</p> <pre><code>$ service mysql start\n</code></pre> <p>and use <code>mysql_upgrade</code> to migrate to the new grant tables, it will rebuild the indexes needed and do the modifications needed:</p> <p>Note</p> <p>If you\u2019re using TokuDB storage engine, re-enable the storage engine plugin by running the: <code>ps-admin --enable-tokudb</code> before running <code>mysql_upgrade</code> otherwise you\u2019ll get errors.</p> <pre><code>$ mysql_upgrade\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Checking if update is needed.\nChecking server version.\nRunning queries to upgrade MySQL server.\nChecking system database.\nmysql.columns_priv                                 OK\nmysql.db                                           OK\n...\nUpgrade process completed successfully.\nChecking if update is needed.\n</code></pre> <p>Once this is done, just restart the server as usual:</p> <pre><code>$ service mysql restart\n</code></pre> <p>After the service has been successfully restarted you can use the new Percona Server for MySQL 5.7.</p>"},{"location":"upgrade-standalone.html","title":"Upgrading using Standalone Packages","text":"<p>You can also upgrade to Percona Server for MySQL using standalone  packages.</p>"},{"location":"upgrade-standalone.html#deb-based-distributions","title":"DEB-based distributions","text":"<p>After taking a full backup and dump, if possible, stop the server:</p> <pre><code>$ sudo /etc/init.d/mysql stop\n</code></pre> <p>Remove the installed packages with their dependencies:</p> <pre><code>$ sudo apt autoremove percona-server-server-5.6 percona-server-client-5.6\n</code></pre> <p>Once removed, proceed to do the modifications needed in your  configuration file. </p> <p>Then, download the following packages for your architecture:</p> <ul> <li> <p><code>percona-server-server-5.7</code></p> </li> <li> <p><code>percona-server-client-5.7</code></p> </li> <li> <p><code>percona-server-common-5.7</code></p> </li> <li> <p><code>libperconaserverclient20</code></p> </li> </ul> <p>Following example will download Percona Server for MySQL Percona Server for MySQL 5.7.10-3 release packages for Debian 8.0:</p> <pre><code>$ wget https://www.percona.com/downloads/Percona-Server-5.7/Percona-Server-5.7.10-3/binary/debian/jessie/x86_64/Percona-Server-5.7.10-3-r63dafaf-jessie-x86_64-bundle.tar\n</code></pre> <p>You should then unpack the bundle to get the packages:</p> <pre><code>$ tar xvf Percona-Server-5.7.10-3-r63dafaf-jessie-x86_64-bundle.tar\n</code></pre> <p>After you unpack the bundle, run <code>ls</code> to view the available packages:</p> <p><pre><code>$ ls *.deb\n</code></pre> The output should be similar to the following:</p> <pre><code>libperconaserverclient20-dev_5.7.10-3-1.jessie_amd64.deb\nlibperconaserverclient20_5.7.10-3-1.jessie_amd64.deb\npercona-server-5.7-dbg_5.7.10-3-1.jessie_amd64.deb\npercona-server-client-5.7_5.7.10-3-1.jessie_amd64.deb\npercona-server-common-5.7_5.7.10-3-1.jessie_amd64.deb\npercona-server-server-5.7_5.7.10-3-1.jessie_amd64.deb\npercona-server-source-5.7_5.7.10-3-1.jessie_amd64.deb\npercona-server-test-5.7_5.7.10-3-1.jessie_amd64.deb\npercona-server-tokudb-5.7_5.7.10-3-1.jessie_amd64.deb\n</code></pre> <p>Now you can install Percona Server for MySQL by running:</p> <pre><code>$ sudo dpkg -i *.deb\n</code></pre> <p>This will install all the packages from the bundle. Another option is to download/specify only the packages you need for running Percona Server for MySQL installation (<code>libperconaserverclient20_5.7.10-3-1.jessie_amd64.deb</code>, <code>percona-server-client-5.7_5.7.10-3-1.jessie_amd64.deb</code>, <code>percona-server-common-5.7_5.7.10-3-1.jessie_amd64.deb</code>, and <code>percona-server-server-5.7_5.7.10-3-1.jessie_amd64.deb</code>. Optionally you can install <code>percona-server-tokudb-5.7_5.7.10-3-1.jessie_amd64.deb</code> if you want TokuDB storage engine).</p> <p>Note</p> <p>Percona Server for MySQL 5.7 comes with the TokuDB storage engine. You can find more information on how to install and enable the TokuDB storage in the TokuDB Installation guide.</p> <p>Warning</p> <p>When installing packages manually, resolve all the dependencies and install any missing packages. The following packages should be installed before installing Percona Server for MySQL 5.7: <code>libmecab2</code>, <code>libjemalloc1</code>, <code>zlib1g-dev</code>, and <code>libaio1</code>.</p> <p>The installation script will not run automatically mysql_upgrade, so you\u2019ll need to run it yourself and restart the service afterwards.</p>"},{"location":"upgrade-standalone.html#rpm-based-distributions","title":"RPM-based distributions","text":"<p>Having done the full backup (and dump if possible), stop the server:</p> <pre><code>$ service mysql stop\n</code></pre> <p>and check your installed packages:</p> <p><pre><code>$ rpm -qa | grep Percona-Server\n</code></pre> The output should be similar to the following:</p> <pre><code>Percona-Server-shared-56-5.6.28-rel76.1.el6.x86_64\nPercona-Server-server-56-5.6.28-rel76.1.el6.x86_64\nPercona-Server-client-56-5.6.28-rel76.1.el6.x86_64\nPercona-Server-tokudb-56-5.6.28-rel76.1.el6.x86_64\n</code></pre> <p>You may have a fourth, <code>shared-compat</code>, which is for compatibility purposes.</p> <p>After checked that, proceed to remove them without dependencies:</p> <pre><code>$ rpm -qa | grep Percona-Server | xargs rpm -e --nodeps\n</code></pre> <p>It is important that you remove it without dependencies as many packages may depend on these (as they replace <code>mysql</code>) and will be removed if omitted.</p> <p>Note that this procedure is the same for upgrading from MySQL 5.6 to Percona Server for MySQL 5.7, just grep <code>'^mysql-'</code> instead of <code>Percona-Server</code> and remove them.</p> <p>Download the packages of the desired series for your architecture from the download page. The easiest way is to download bundle which contains all the packages. Following example will download Percona Server for MySQL 5.7.10-3 release packages for CentOS 7:</p> <pre><code>$ wget https://www.percona.com/downloads/Percona-Server-5.7/Percona-Server-5.7.10-3/binary/redhat/7/x86_64/Percona-Server-5.7.10-3-r63dafaf-el7-x86_64-bundle.tar\n</code></pre> <p>You should then unpack the bundle to get the packages:</p> <pre><code>$ tar xvf Percona-Server-5.7.10-3-r63dafaf-el7-x86_64-bundle.tar\n</code></pre> <p>After you unpack the bundle you should see the following packages:</p> <p><pre><code>$ ls *.rpm\n</code></pre> The output should be similar to the following:</p> <pre><code>Percona-Server-57-debuginfo-5.7.10-3.1.el7.x86_64.rpm\nPercona-Server-client-57-5.7.10-3.1.el7.x86_64.rpm\nPercona-Server-devel-57-5.7.10-3.1.el7.x86_64.rpm\nPercona-Server-server-57-5.7.10-3.1.el7.x86_64.rpm\nPercona-Server-shared-57-5.7.10-3.1.el7.x86_64.rpm\nPercona-Server-shared-compat-57-5.7.10-3.1.el7.x86_64.rpm\nPercona-Server-test-57-5.7.10-3.1.el7.x86_64.rpm\nPercona-Server-tokudb-57-5.7.10-3.1.el7.x86_64.rpm\n</code></pre> <p>Now you can install Percona Server for MySQL 5.7 by running:</p> <pre><code>rpm -ivh Percona-Server-server-57-5.7.10-3.1.el7.x86_64.rpm \\\nPercona-Server-client-57-5.7.10-3.1.el7.x86_64.rpm \\\nPercona-Server-shared-57-5.7.10-3.1.el7.x86_64.rpm\n</code></pre> <p>This will install only packages required to run the Percona Server for MySQL 5.7. Optionally you can install TokuDB storage engine by adding the <code>Percona-Server-tokudb-57-5.7.10-3.1.el7.x86_64.rpm</code> to the command above. You can find more information on how to install and enable the TokuDB storage in the TokuDB Installation guide.</p> <p>To install all the packages (for debugging, testing, etc.) you should run:</p> <pre><code>$ rpm -ivh *.rpm\n</code></pre> <p>Note</p> <p>When installing packages manually like this, you\u2019ll need to make sure to resolve all the dependencies and install missing packages yourself.</p> <p>Once installed, proceed to modify your configuration file - <code>my.cnf</code> - and install the plugins if necessary. If you\u2019re using TokuDB storage engine you\u2019ll need to comment out all the TokuDB specific variables in your configuration file(s) before starting the server, otherwise server won\u2019t be able to start. RHEL/CentOS 7 automatically backs up the previous configuration file to <code>/etc/my.cnf.rpmsave</code> and installs the default <code>my.cnf</code>. After upgrade/install process completes you can move the old configuration file back (after you remove all the unsupported system variables).</p> <p>As the schema of the grant table has changed, the server must be started without reading them:</p> <pre><code>$ service mysql start\n</code></pre> <p>and use <code>mysql_upgrade</code> to migrate to the new grant tables, it will rebuild the indexes needed and do the modifications needed:</p> <p>Note</p> <p>If you\u2019re using TokuDB storage engine you\u2019ll need re-enable the storage engine plugin by running the: <code>ps-admin --enable-tokudb</code> before running <code>mysql_upgrade</code> otherwise you\u2019ll get errors.</p> <pre><code>$ mysql_upgrade\n</code></pre> <p>After this is done, just restart the server as usual:</p> <pre><code>$ service mysql restart\n</code></pre>"},{"location":"upgrade.html","title":"Upgrade from earlier versions","text":"<p>An upgrade is supported from 5.6 to 5.7. We recommend that you upgrade to the latest version of 5.6 before upgrading to 5.7. You cannot skip versions, for example, you cannot upgrade from 5.5 to 5.7. </p> <p>The following upgrade docs are available:</p> <ul> <li> <p>Upgrading guide from 5.6 to 5.7</p> </li> <li> <p>Upgrade in-place with install Percona packages</p> </li> <li> <p>Upgrade using Percona repositories</p> </li> <li> <p>Upgrade using standalone packages</p> </li> </ul>"},{"location":"upgrading_guide_56_57.html","title":"Percona Server In-Place Upgrading Guide: From 5.6 to 5.7","text":"<p>In-place upgrades are those which are done using the existing data in the server. Generally speaking, this is stopping the server, installing the new server and starting it with the same data files. While they may not be suitable for high-complexity environments, they may be adequate for many scenarios.</p> <p>The following is a summary of the more relevant changes in the 5.7 series. It\u2019s strongly recommended to that you read the following guides as they contain the list of incompatible changes that could cause automatic upgrade to fail:</p> <ul> <li> <p>Changed in Percona Server 5.7</p> </li> <li> <p>Upgrading MySQL</p> </li> <li> <p>Upgrading from MySQL 5.6 to 5.7</p> </li> </ul> <p>Warning</p> <p>Upgrade from 5.6 to 5.7 on a crashed instance is not recommended. If the server instance has crashed, crash recovery should be run before proceeding with the upgrade.</p>"},{"location":"upstream-bug-fixes.html","title":"List of upstream MySQL bugs fixed in Percona Server for MySQL  5.7","text":"<p>|  * Upstream Bug</p> <pre><code>[#93917](http://bugs.mysql.com/bug.php?id=93917) - Wrong binlog entry for BLOB on a blackhole intermediary master\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#5353</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.26-29</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#93708](http://bugs.mysql.com/bug.php?id=93708) - Page Cleaner will sleep for long time if clock changes\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#5221</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.26-29</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code> |\n</code></pre> <p>|  * Upstream Bug</p> <p>#92850 - Bad select+order by+limit performance in 5.7</p> </li> <li> <p>JIRA bug</p> <p>#4949</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.25-28</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>           |\n</code></pre> <p>|  * Upstream Bug</p> <p>#92809 - Inconsistent ResultSet for different Execution Plans</p> </li> <li> <p>JIRA bug</p> <p>#4907</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.25-28</p> </li> <li> <p>Upstream Fix</p> <p>5.7.27</p> <pre><code>                          |\n</code></pre> <p>|  * Upstream Bug</p> <p>#92108 - Deadlock by concurrent show binlogs, pfs session_variables table \u2026</p> </li> <li> <p>JIRA bug</p> <p>#4716</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.25-28</p> </li> <li> <p>Upstream Fix</p> <p>5.7.22</p> <pre><code>            |\n</code></pre> <p>|  * Upstream Bug</p> <p>#91541 - Flush status statement adds twice to global values</p> </li> <li> <p>JIRA bug</p> <p>#4570</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.23-23</p> </li> <li> <p>Upstream Fix</p> <p>5.7.26</p> <pre><code>                                      |\n</code></pre> <p>|  * Upstream Bug</p> <p>#91423 - Can\u2019t run mysql on Ubuntu systems with long recovery time</p> </li> <li> <p>JIRA bug</p> <p>#4546</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.23-23</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>        |\n</code></pre> <p>|  * Upstream Bug</p> <p>#91091 - A simple SELECT on a table with CHARSET=euckr COLLATE=euckr_bin \u2026</p> </li> <li> <p>JIRA bug</p> <p>#4513</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.23-23</p> </li> <li> <p>Upstream Fix</p> <p>5.7.24</p> <p>| |  * Upstream Bug</p> <p>#90264 - Some file operations in mf_iocache2.c are not instrumented</p> </li> <li> <p>JIRA bug</p> <p>#3937</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.21-21</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                 |\n</code></pre> <p>|  * Upstream Bug</p> <p>#90238 - Comparison of uninitailized memory in log_in_use</p> </li> <li> <p>JIRA bug</p> <p>#3925</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.21-21</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                           |\n</code></pre> <p>|  * Upstream Bug</p> <p>#89916 - hp_test1/hp_test2 are not built unless WITH_EMBEDDED_SERVER is defined</p> </li> <li> <p>JIRA bug</p> <p>#3845</p> </li> <li> <p>Upstream State</p> <p>Won\u2019t fix</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.21-21</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                  |\n</code></pre> <p>|  * Upstream Bug</p> <p>#89822 - InnoDB retries open on EINTR error only if innodb_use_native_aio is \u2026</p> </li> <li> <p>JIRA bug</p> <p>#3843</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.21-21</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <p>| |  * Upstream Bug</p> <p>#89646 - Clang warnings in 5.7.21</p> </li> <li> <p>JIRA bug</p> <p>#3814</p> </li> <li> <p>Upstream State</p> <p>Won\u2019t fix</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.21-21</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                                                |\n</code></pre> <p>|  * Upstream Bug</p> <p>#89598 - plugin_mecab.cc:54:19: warning: unused variable \u2018bundle_mecab\u2019</p> </li> <li> <p>JIRA bug</p> <p>#3804</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.21-20</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                             |\n</code></pre> <p>|  * Upstream Bug</p> <p>#89422 - Dangerous enum-ulong casts in sql_formatter_options</p> </li> <li> <p>JIRA bug</p> <p>#3780</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.21-20</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>              |\n</code></pre> <p>|  * Upstream Bug</p> <p>#89421 - Missing mutex_unlock in Slave_reporting_capability::va_report</p> </li> <li> <p>JIRA bug</p> <p>#3780</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.21-20</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                              |\n</code></pre> <p>|  * Upstream Bug</p> <p>#89420 - Enforcing C++03 mode in non debug builds</p> </li> <li> <p>JIRA bug</p> <p>#3780</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.21-20</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                         |\n</code></pre> <p>|  * Upstream Bug</p> <p>#89205 - gap locks on READ COMMITTED cause by page split</p> </li> <li> <p>JIRA bug</p> <p>#1130</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.22-22</p> </li> <li> <p>Upstream Fix</p> <p>5.7.20</p> <pre><code>                                         |\n</code></pre> <p>|  * Upstream Bug</p> <p>#88720 -  Inconsistent and unsafe FLUSH behavior in terms of replication</p> </li> <li> <p>JIRA bug</p> <p>#1827</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-02-11)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.25-28</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#88057](http://bugs.mysql.com/bug.php?id=88057) - Intermediary slave does not log master changes with\u2026\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#1119</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.20-19</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>              |\n</code></pre> <p>|  * Upstream Bug</p> <p>#87065 - Release lock on table statistics after query plan created</p> </li> <li> <p>JIRA bug</p> <p>#2503</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.20-18</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>         |\n</code></pre> <p>|  * Upstream Bug</p> <p>#86260 - Assert on KILL\u2019ing a stored routine invocation</p> </li> <li> <p>JIRA bug</p> <p>#1091</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-16</p> </li> <li> <p>Upstream Fix</p> <p>5.7.22</p> <pre><code>                                 |\n</code></pre> <p>|  * Upstream Bug</p> <p>#86209 - audit plugin + MB collation connection + PREPARE stmt parse error crash\u2026</p> </li> <li> <p>JIRA bug</p> <p>#1089</p> </li> <li> <p>Upstream State</p> <p>N/A</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-14</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>             |\n</code></pre> <p>|  * Upstream Bug</p> <p>#86164 - Fulltext search can not find word which contains punctuation marks</p> </li> <li> <p>JIRA bug</p> <p>#2501</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.21-20</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>|\n</code></pre> <p>|  * Upstream Bug</p> <p>#86016 - Make MTR show core dump stacktraces from unit tests too</p> </li> <li> <p>JIRA bug</p> <p>#2499</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-16</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code> |\n</code></pre> <p>|  * Upstream Bug</p> <p>#85838 - rpl_diff.inc in 5.7 does not compare data from different servers</p> </li> <li> <p>JIRA bug</p> <p>#2257</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-14</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                  |\n</code></pre> <p>|  * Upstream Bug</p> <p>#85835 - server crash n-gram full text searching</p> </li> <li> <p>JIRA bug</p> <p>#237</p> </li> <li> <p>Upstream State</p> <p>N/A</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-15</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                               |\n</code></pre> <p>|  * Upstream Bug</p> <p>#85678 - field-t deletes Fake_TABLE objects through base TABLE pointer w/o \u2026</p> </li> <li> <p>JIRA bug</p> <p>#2253</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-14</p> </li> <li> <p>Upstream Fix</p> <p>5.7.19</p> <pre><code>            |\n</code></pre> <p>|  * Upstream Bug</p> <p>#85671 - segfault-t failing under recent AddressSanitizer</p> </li> <li> <p>JIRA bug</p> <p>#2252</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-14</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                  |\n</code></pre> <p>|  * Upstream Bug</p> <p>#85258 - DROP TEMPORARY TABLE creates a transaction in binary log on read only\u2026</p> </li> <li> <p>JIRA bug</p> <p>#1785</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-14</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>            |\n</code></pre> <p>|  * Upstream Bug</p> <p>#85158 - heartbeats/fakerotate cause a forced sync_master_info</p> </li> <li> <p>JIRA bug</p> <p>#1812</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.20-19</p> </li> <li> <p>Upstream Fix</p> <p>5.7.26</p> <pre><code>                                    |\n</code></pre> <p>|  * Upstream Bug</p> <p>#85141 - Write/fsync amplification w/ duplicate GTIDs</p> </li> <li> <p>JIRA bug</p> <p>#1786</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-14</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>            |\n</code></pre> <p>|  * Upstream Bug</p> <p>#84958 -  InnoDB\u2019s MVCC has O(N^2) behaviors</p> </li> <li> <p>JIRA bug</p> <p>#4712</p> </li> <li> <p>JIRA bug</p> <p>#5450</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.26-29</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                              |\n</code></pre> <p>|  * Upstream Bug</p> <p>#84736 - 5.7 range optimizer crash</p> </li> <li> <p>JIRA bug</p> <p>#1055</p> </li> <li> <p>Upstream State</p> <p>N/A</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.17-12</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                                            |\n</code></pre> <p>|  * Upstream Bug</p> <p>#84437 - super-read-only does not allow FLUSH LOGS on 5.7</p> </li> <li> <p>JIRA bug</p> <p>#1772</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.17-12</p> </li> <li> <p>Upstream Fix</p> <p>5.7.18</p> <pre><code>                               |\n</code></pre> <p>|  * Upstream Bug</p> <p>#84420 - stopwords and ngram indexes</p> </li> <li> <p>JIRA bug</p> <p>#1802</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.20-18</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                       |\n</code></pre> <p>|  * Upstream Bug</p> <p>#84415 - slave don\u2019t report Seconds_Behind_Master when running \u2026</p> </li> <li> <p>JIRA bug</p> <p>#1770</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-14</p> </li> <li> <p>Upstream Fix</p> <p>5.7.22</p> <pre><code>                        |\n</code></pre> <p>|  * Upstream Bug</p> <p>#84366 - InnoDB index dives do not detect concurrent tree changes, return bogus\u2026</p> </li> <li> <p>JIRA bug</p> <p>#1089</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.17-11</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#84350](http://bugs.mysql.com/bug.php?id=84350) - Error 1290 executing flush logs in read-only slave\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#1044</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.17-12</p> </li> <li> <p>Upstream Fix</p> <p>5.7.18</p> <pre><code>                                     |\n</code></pre> <p>|  * Upstream Bug</p> <p>#83814 - Add support for OpenSSL 1.1</p> </li> <li> <p>JIRA bug</p> <p>#1105</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-16</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                                               |\n</code></pre> <p>|  * Upstream Bug</p> <p>#83648 - Assertion failure in thread x in file fts0que.cc line 3659</p> </li> <li> <p>JIRA bug</p> <p>#1023</p> </li> <li> <p>Upstream State</p> <p>N/A</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.17-12</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                   |\n</code></pre> <p>|  * Upstream Bug</p> <p>#83232 -  replication breaks after bug #74145 happens in master</p> </li> <li> <p>JIRA bug</p> <p>#1017</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.24-26</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                              |\n</code></pre> <p>|  * Upstream Bug</p> <p>#83124 - Bug 81657 fix merge to 5.6 broken</p> </li> <li> <p>JIRA bug</p> <p>#1750</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.15-9</p> </li> <li> <p>Upstream Fix</p> <p>5.7.17</p> <pre><code>                                                       |\n</code></pre> <p>|  * Upstream Bug</p> <p>#83073 - GCC 5 and 6 miscompile mach_parse_compressed</p> </li> <li> <p>JIRA bug</p> <p>#1745</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.15-9</p> </li> <li> <p>Upstream Fix</p> <p>5.7.17</p> <pre><code>                                            |\n</code></pre> <p>|  * Upstream Bug</p> <p>#83003 - Using temporary tables on slaves increases GTID sequence number</p> </li> <li> <p>JIRA bug</p> <p>#964</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.17-11</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                            |\n</code></pre> <p>|  * Upstream Bug</p> <p>#82980 - Multi-threaded slave leaks worker threads in case of thread create \u2026</p> </li> <li> <p>JIRA bug</p> <p>#2193</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.15-9</p> </li> <li> <p>Upstream Fix</p> <p>5.7.20</p> <pre><code>                    |\n</code></pre> <p>|  * Upstream Bug</p> <p>#82940 - mysqld crashes itself when creating index</p> </li> <li> <p>JIRA bug</p> <p>#3410</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.26-29</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                       |\n</code></pre> <p>|  * Upstream Bug</p> <p>#82935 - Cipher ECDHE-RSA-AES128-GCM-SHA256 listed in man/Ssl_cipher_list, not\u2026</p> </li> <li> <p>JIRA bug</p> <p>#1737</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.15-9</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#82886](http://bugs.mysql.com/bug.php?id=82886) - Server may crash due to a glibc bug in handling short-lived detached \u2026\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#1006</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.15-9</p> </li> <li> <p>Upstream Fix</p> <p>5.7.16</p> <pre><code>                  |\n</code></pre> <p>|  * Upstream Bug</p> <p>#82307 - Memory leaks in unit tests</p> </li> <li> <p>JIRA bug</p> <p>#2157</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.14-7</p> </li> <li> <p>Upstream Fix</p> <p>5.7.18</p> <pre><code>                                                              |\n</code></pre> <p>|  * Upstream Bug</p> <p>#82283 - main.mysqlbinlog_debug fails with a LeakSanitizer error</p> </li> <li> <p>JIRA bug</p> <p>#2156</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.14-7</p> </li> <li> <p>Upstream Fix</p> <p>5.7.19</p> <pre><code>                                 |\n</code></pre> <p>|  * Upstream Bug</p> <p>#82026 - Stack buffer overflow with \u2013ssl-cipher= <li> <p>JIRA bug</p> <p>#2155</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.14-7</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                           |\n</code></pre> <p>|  * Upstream Bug</p> <p>#82019 - Is client library supposed to retry EINTR indefinitely or not</p> </li> <li> <p>JIRA bug</p> <p>#1720</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.14-7</p> </li> <li> <p>Upstream Fix</p> <p>5.7.15</p> <pre><code>                           |\n</code></pre> <p>|  * Upstream Bug</p> <p>#81814 - InnoDB adaptive hash index uses a bad partitioning algorithm for the \u2026</p> </li> <li> <p>JIRA bug</p> <p>#2498</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-14</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> <p>| |  * Upstream Bug</p> <pre><code>[#81810](http://bugs.mysql.com/bug.php?id=81810) - Inconsistent sort order for blob/text between InnoDB and filesort\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#1799</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-14</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                         |\n</code></pre> <p>|  * Upstream Bug</p> <p>#81714 - mysqldump get_view_structure does not free MYSQL_RES in one error path</p> </li> <li> <p>JIRA bug</p> <p>#2152</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.13-6</p> </li> <li> <p>Upstream Fix</p> <p>5.7.20</p> <pre><code>                  |\n</code></pre> <p>|  * Upstream Bug</p> <p>#81675 - mysqlbinlog does not free the existing connection before opening new \u2026</p> </li> <li> <p>JIRA bug</p> <p>#1718</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>5.7.12-6</p> </li> <li> <p>Upstream Fix</p> <p>5.7.15</p> <pre><code>                                           |\n</code></pre> <p>|  * Upstream Bug</p> <p>#81657 - DBUG_PRINT in THD::decide_logging_format prints incorrectly, access \u2026</p> </li> <li> <p>JIRA bug</p> <p>#2150</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>5.7.12-6</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                               |\n</code></pre> <p>|  * Upstream Bug</p> <p>#81467 - innodb_fts.sync_block test unstable due to slow query log nondeterminism</p> </li> <li> <p>JIRA bug</p> <p>#2232</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.17-12</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#80962](http://bugs.mysql.com/bug.php?id=80962) - Replication does not work when @@GLOBAL.SERVER_UUID is missing on the\u2026\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#1684</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.12-5</p> </li> <li> <p>Upstream Fix</p> <p>5.7.13</p> <pre><code>                   |\n</code></pre> <p>|  * Upstream Bug</p> <p>#80607 - main.log_tables-big unstable on loaded hosts</p> </li> <li> <p>JIRA bug</p> <p>#2141</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.11-4</p> </li> <li> <p>Upstream Fix</p> <p>5.7.18</p> <pre><code>                                             |\n</code></pre> <p>|  * Upstream Bug</p> <p>#80606 - my_write, my_pwrite no longer safe to call from THD-less server utility\u2026</p> </li> <li> <p>JIRA bug</p> <p>#970</p> </li> <li> <p>Upstream State</p> <p>N/A</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.11-4</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                        |\n</code></pre> <p>|  * Upstream Bug</p> <p>#80496 - buf_dblwr_init_or_load_pages now returns an error code, but caller not\u2026</p> </li> <li> <p>JIRA bug</p> <p>#3384</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.11-4</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#80288](http://bugs.mysql.com/bug.php?id=80288) - missing innodb_numa_interleave\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#974</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.12-5</p> </li> <li> <p>Upstream Fix</p> <p>5.7.16</p> <pre><code>                                                            |\n</code></pre> <p>|  * Upstream Bug</p> <p>#80053 - Assertion in binlog coordinator on slave with 2 2pc handler log_slave \u2026</p> </li> <li> <p>JIRA bug</p> <p>#3361</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-2</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#79894](http://bugs.mysql.com/bug.php?id=79894) - Page cleaner worker threads are not instrumented for performance schema\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#3356</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-2</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#79703](http://bugs.mysql.com/bug.php?id=79703) - Spin rounds per wait will be negative in InnoDB status if spin waits &gt;\u2026\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#1684</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-2</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                     |\n</code></pre> <p>|  * Upstream Bug</p> <p>#79610 - Failed DROP DATABASE due FK constraint on master breaks slave</p> </li> <li> <p>JIRA bug</p> <p>#1683</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.14-7</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                               |\n</code></pre> <p>|  * Upstream Bug</p> <p>#79569 - Some \u2013big-test tests were forgotten to update in 5.7.10</p> </li> <li> <p>JIRA bug</p> <p>#3339</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-2</p> </li> <li> <p>Upstream Fix</p> <p>5.7.11</p> <pre><code>                                  |\n</code></pre> <p>|  * Upstream Bug</p> <p>#79117 - \u201cchange_user\u201d command should be aware of preceding \u201cerror\u201d command</p> </li> <li> <p>JIRA bug</p> <p>#659</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>5.7.12</p> <pre><code>                        |\n</code></pre> <p>|  * Upstream Bug</p> <p>#78894 - buf_pool_resize can lock less in checking whether AHI is on or off</p> </li> <li> <p>JIRA bug</p> <p>#3340</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>|\n</code></pre> <p>|  * Upstream Bug</p> <p>#77684 - DROP TABLE IF EXISTS may brake replication if slave has replication \u2026</p> </li> <li> <p>JIRA bug</p> <p>#1639</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>5.7.12</p> <pre><code>                    |\n</code></pre> <p>|  * Upstream Bug</p> <p>#77591 - ALTER TABLE does not allow to change NULL/NOT NULL if foreign key exists</p> </li> <li> <p>JIRA bug</p> <p>#1635</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#77399](http://bugs.mysql.com/bug.php?id=77399) - Deadlocks missed by INFORMATION_SCHEMA.INNODB_METRICS lock_deadlocks \u2026\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#1635</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <p>| |  * Upstream Bug</p> <p>#76418 - Server crashes when querying partitioning table MySQL_5.7.14</p> </li> <li> <p>JIRA bug</p> <p>#1050</p> </li> <li> <p>Upstream State</p> <p>N/A</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-15</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                  |\n</code></pre> <p>|  * Upstream Bug</p> <p>#76142 - InnoDB tablespace import fails when importing table w/ different data \u2026</p> </li> <li> <p>JIRA bug</p> <p>#1697</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.13-6</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#75534](http://bugs.mysql.com/bug.php?id=75534) - Solve buffer pool mutex contention by splitting it\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>Improved Buffer Pool Scalability</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>               |\n</code></pre> <p>|  * Upstream Bug</p> <p>#75504 - btr_search_guess_on_hash makes found block young twice?</p> </li> <li> <p>JIRA bug</p> <p>#2454</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>           |\n</code></pre> <p>|  * Upstream Bug</p> <p>#75480 - Selecting wrong pos with SHOW BINLOG EVENTS causes a potentially \u2026</p> </li> <li> <p>JIRA bug</p> <p>#1600</p> </li> <li> <p>Upstream State</p> <p>N/A</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                             |\n</code></pre> <p>|  * Upstream Bug</p> <p>#75311 - Error for SSL cipher is unhelpful</p> </li> <li> <p>JIRA bug</p> <p>#1779</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.17-12</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                |\n</code></pre> <p>|  * Upstream Bug</p> <p>#75189 - engines suite tests depending on InnoDB implementation details</p> </li> <li> <p>JIRA bug</p> <p>#2103</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>    |\n</code></pre> <p>|  * Upstream Bug</p> <p>#74637 - make dirty page flushing more adaptive</p> </li> <li> <p>JIRA bug</p> <p>Multi-threaded asynchronous LRU flusher</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-3</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#73418](http://bugs.mysql.com/bug.php?id=73418) - Add \u2013manual-lldb option to mysql-test-run.pl\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#2448</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                      |\n</code></pre> <p>|  * Upstream Bug</p> <p>#72615 - MTR \u2013mysqld=\u2013default-storage-engine=foo incompatible w/ dynamically\u2026</p> </li> <li> <p>JIRA bug</p> <p>#2071</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <p>| |  * Upstream Bug</p> <p>#72475 - Binlog events with binlog_format=MIXED are unconditionally logged in \u2026</p> </li> <li> <p>JIRA bug</p> <p>#151</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                       |\n</code></pre> <p>|  * Upstream Bug</p> <p>#72466 - More memory overhead per page in the InnoDB buffer pool</p> </li> <li> <p>JIRA bug</p> <p>#1689</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.12-5</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>           |\n</code></pre> <p>|  * Upstream Bug</p> <p>#72123 - Spurious lock_wait_timeout_thread wakeup in lock_wait_suspend_thread()</p> </li> <li> <p>JIRA bug</p> <p>#2504</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-16</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#72108](http://bugs.mysql.com/bug.php?id=72108) - Hard to read history file\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#2066</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                         |\n</code></pre> <p>|  * Upstream Bug</p> <p>#71761 - ANALYZE TABLE should remove its table from background stat processing\u2026</p> </li> <li> <p>JIRA bug</p> <p>#1749</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.15-9</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <p>| |  * Upstream Bug</p> <p>#71759 - memory leak with string thread variable that set memalloc flag</p> </li> <li> <p>JIRA bug</p> <p>#1004</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.15-9</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                              |\n</code></pre> <p>|  * Upstream Bug</p> <p>#71411 - buf_flush_LRU() does not return correct number in case of compressed \u2026</p> </li> <li> <p>JIRA bug</p> <p>#1461</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <p>| |  * Upstream Bug</p> <p>#71270 - Failures to end bulk insert for partitioned tables handled incorrectly</p> </li> <li> <p>JIRA bug</p> <p>#700</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <p>| |  * Upstream Bug</p> <p>#71217 - Threadpool - add thd_wait_begin/thd_wait_end to the network IO functions</p> </li> <li> <p>JIRA bug</p> <p>#1343</p> </li> <li> <p>Upstream State</p> <p>Open (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <p>| |  * Upstream Bug</p> <p>#71183 - os_file_fsync() should handle fsync() returning EINTR</p> </li> <li> <p>JIRA bug</p> <p>#1461</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>             |\n</code></pre> <p>|  * Upstream Bug</p> <p>#71091 - CSV engine does not properly process \u201c\u201d, in quotes</p> </li> <li> <p>JIRA bug</p> <p>#153</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                 |\n</code></pre> <p>|  * Upstream Bug</p> <p>#70500 - Page cleaner should perform LRU flushing regardless of server activity</p> </li> <li> <p>JIRA bug</p> <p>#1428</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <p>| |  * Upstream Bug</p> <p>#70490 - Suppression is too strict on some systems</p> </li> <li> <p>JIRA bug</p> <p>#2038</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>5.7.20</p> <pre><code>                                                |\n</code></pre> <p>|  * Upstream Bug</p> <p>#69991 - MySQL client is broken without readline</p> </li> <li> <p>JIRA bug</p> <p>#1467</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                           |\n</code></pre> <p>|  * Upstream Bug</p> <p>#69639 - mysql failed to build with dtrace Sun D 1.11</p> </li> <li> <p>JIRA bug</p> <p>#1392</p> </li> <li> <p>Upstream State</p> <p>Unsupported</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                           |\n</code></pre> <p>|  * Upstream Bug</p> <p>#69258 - does buf_LRU_buf_pool_running_out need to lock buffer pool mutexes</p> </li> <li> <p>JIRA bug</p> <p>#1414</p> </li> <li> <p>Upstream State</p> <p>Not a bug</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                       |\n</code></pre> <p>|  * Upstream Bug</p> <p>#69232 - buf_dblwr-&gt;mutex can be splited into two</p> </li> <li> <p>JIRA bug</p> <p>Parallel doublewrite buffer</p> </li> <li> <p>Upstream State</p> <p>No Feedback (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.11-4</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code> |\n</code></pre> <p>|  * Upstream Bug</p> <p>#69170 - buf_flush_LRU is lazy</p> </li> <li> <p>JIRA bug</p> <p>#2430</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                             |\n</code></pre> <p>|  * Upstream Bug</p> <p>#69146 - Needless log flush order mutex acquisition in buf_pool_get_oldest_mod\u2026</p> </li> <li> <p>JIRA bug</p> <p>#2418</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <p>| |  * Upstream Bug</p> <p>#68714 - Remove literal statement digest values from perfschema tests</p> </li> <li> <p>JIRA bug</p> <p>#1340</p> </li> <li> <p>Upstream State</p> <p>Not a bug</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                             |\n</code></pre> <p>|  * Upstream Bug</p> <p>#68481 - InnoDB LRU flushing for MySQL 5.6 needs work</p> </li> <li> <p>JIRA bug</p> <p>#2432</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                      |\n</code></pre> <p>|  * Upstream Bug</p> <p>#68052 - SSL Certificate Subject ALT Names with IPs not respected with \u2013ssl-ver\u2026</p> </li> <li> <p>JIRA bug</p> <p>#1076</p> </li> <li> <p>Upstream State</p> <p>Closed</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.18-16</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                    |\n</code></pre> <p>|  * Upstream Bug</p> <p>#67808 - in innodb engine, double write and multi-buffer pool instance reduce \u2026</p> </li> <li> <p>JIRA bug</p> <p>Parallel doublewrite buffer</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.11-4</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> </li> </ul> <p>| |  * Upstream Bug</p> <pre><code>[#63130](http://bugs.mysql.com/bug.php?id=63130) - CMake-based check for the presence of a system readline library is not\u2026\n</code></pre> <ul> <li> <p>JIRA bug</p> <p>#1467</p> </li> <li> <p>Upstream State</p> <p>Can\u2019t Repeat</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                  |\n</code></pre> <p>|  * Upstream Bug</p> <p>#57583 - fast index create not used during \u201calter table foo engine=innodb\u201d</p> </li> <li> <p>JIRA bug</p> <p>#2113</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                    |\n</code></pre> <p>|  * Upstream Bug</p> <p>#53645 - SHOW GRANTS not displaying all the applicable grants</p> </li> <li> <p>JIRA bug</p> <p>#191</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                  |\n</code></pre> <p>|  * Upstream Bug</p> <p>#53588 - Blackhole : Specified key was too long; max key length is 1000 bytes</p> </li> <li> <p>JIRA bug</p> <p>#1126</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server 5.7.20-19</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                          |\n</code></pre> <p>|  * Upstream Bug</p> <p>#49120 - mysqldump should have flag to delay creating indexes for innodb plugin\u2026</p> </li> <li> <p>JIRA bug</p> <p>#2619</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>              |\n</code></pre> <p>|  * Upstream Bug</p> <p>#42415 - UPDATE/DELETE with LIMIT clause unsafe for SBL even with ORDER BY PK \u2026</p> </li> <li> <p>JIRA bug</p> <p>#44</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                 |\n</code></pre> <p>|  * Upstream Bug</p> <p>#39833 - CREATE INDEX does full table copy on TEMPORARY table</p> </li> <li> <p>JIRA bug</p> <p>N/A</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                   |\n</code></pre> <p>|  * Upstream Bug</p> <p>#35125 - Allow the ability to set the server_id for a connection for logging to\u2026</p> </li> <li> <p>Launchpad BP</p> <p>Blueprint</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>      |\n</code></pre> <p>|  * Upstream Bug</p> <p>#25007 - memory tables with dynamic rows format</p> </li> <li> <p>JIRA bug</p> <p>#2407</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                                               |\n</code></pre> <p>|  * Upstream Bug</p> <p>#20001 - Support for temp-tables in INFORMATION_SCHEMA</p> </li> <li> <p>JIRA bug</p> <p>Temporary tables</p> </li> <li> <p>Upstream State</p> <p>Verified (checked on 2019-05-21)</p> </li> <li> <p>Fix Released</p> <p>Percona Server for MySQL 5.7.10-1</p> </li> <li> <p>Upstream Fix</p> <p>N/A</p> <pre><code>                             |\n</code></pre> </li> </ul>"},{"location":"diagnostics/innodb_fragmentation_count.html","title":"InnoDB Page Fragmentation Counters","text":"<p>InnoDB page fragmentation is caused by random insertion or deletion from a secondary index. This means that the physical ordering of the index pages on the disk is not the same as the index ordering of the records on the pages.  Some pages take more space and full table scan queries can take more time to finish.</p> <p>To provide more information about the InnoDB page fragmentation Percona Server for MySQL now provides the following counters as status variables: Innodb_scan_pages_contiguous, Innodb_scan_pages_disjointed, Innodb_scan_data_size, Innodb_scan_deleted_recs_size, and Innodb_scan_pages_total_seek_distance.</p>"},{"location":"diagnostics/innodb_fragmentation_count.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server 5.7.20-18: Feature Implemented</li> </ul>"},{"location":"diagnostics/innodb_fragmentation_count.html#status-variables","title":"Status Variables","text":""},{"location":"diagnostics/innodb_fragmentation_count.html#innodb_scan_pages_contiguous","title":"<code>Innodb_scan_pages_contiguous</code>","text":"Option Description Scope Session Data type Numeric <p>This variable shows the number of contiguous page reads inside a query.</p>"},{"location":"diagnostics/innodb_fragmentation_count.html#innodb_scan_pages_disjointed","title":"<code>Innodb_scan_pages_disjointed</code>","text":"Option Description Scope Session Data type Numeric <p>This variable shows the number of disjointed page reads inside a query.</p>"},{"location":"diagnostics/innodb_fragmentation_count.html#innodb_scan_data_size","title":"<code>Innodb_scan_data_size</code>","text":"Option Description Scope Session Data type Numeric <p>This variable shows the size of data in all InnoDB pages read inside a query (in bytes) - calculated as the sum of <code>page_get_data_size(page)</code> for every page scanned.</p>"},{"location":"diagnostics/innodb_fragmentation_count.html#innodb_scan_deleted_recs_size","title":"<code>Innodb_scan_deleted_recs_size</code>","text":"Option Description Scope Session Data type Numeric <p>This variable shows the size of deleted records (marked as <code>deleted</code> in <code>page_delete_rec_list_end()</code>) in all InnoDB pages read inside a query (in bytes) - calculated as the sum of <code>page_header_get_field(page, PAGE_GARBAGE)</code> for every page scanned.</p>"},{"location":"diagnostics/innodb_fragmentation_count.html#innodb_scan_pages_total_seek_distance","title":"<code>Innodb_scan_pages_total_seek_distance</code>","text":"Option Description Scope Session Data type Numeric <p>This variable shows the total seek distance when moving between pages.</p>"},{"location":"diagnostics/innodb_fragmentation_count.html#related-reading","title":"Related Reading","text":"<ul> <li> <p>InnoDB: look after fragmentation</p> </li> <li> <p>Defragmenting a Table</p> </li> </ul>"},{"location":"diagnostics/innodb_show_status.html","title":"Extended Show Engine InnoDB Status","text":"<p>This feature reorganizes the output of <code>SHOW ENGINE INNODB STATUS</code> for better readability and prints the amount of memory used by the internal hash tables. In addition, new variables are available to control the output.</p> <p>This feature modified the <code>SHOW ENGINE INNODB STATUS</code> command as follows:</p> <ul> <li>Added two variables to control <code>SHOW ENGINE INNODB STATUS</code> information presented (bugfix for upstream bug #29126):<ul> <li><code>innodb_show_verbose_locks</code> - Whether to show locked records</li> <li><code>innodb_show_locks_held</code> - The number of locks held to print for each InnoDB transaction</li> </ul> </li> <li>Added extended information about the InnoDB internal hash table sizes, in bytes, in the <code>BUFFER POOL AND MEMORY</code> section, also added a buffer pool size in bytes.</li> <li>Added additional LOG section information.</li> </ul>"},{"location":"diagnostics/innodb_show_status.html#version-changes","title":"Version changes","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6.</li> </ul>"},{"location":"diagnostics/innodb_show_status.html#other-information","title":"Other Information","text":"<ul> <li>Author / Origin: Baron Schwartz, http://lists.mysql.com/internals/35174</li> </ul>"},{"location":"diagnostics/innodb_show_status.html#system-variables","title":"System Variables","text":""},{"location":"diagnostics/innodb_show_status.html#innodb_show_verbose_locks","title":"<code>innodb_show_verbose_locks</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type ULONG Default 0 Range 0 - 1 <p>Specifies to show records locked in <code>SHOW ENGINE INNODB STATUS</code>. The default is <code>0</code>, which means only the higher-level information about the lock, for example, which table and index is locked, etc., is printed. If set to <code>1</code> enables the traditional InnoDB behavior. The locked records are dumped into the output.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_show_locks_held","title":"<code>innodb_show_locks_held</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type ULONG Default 10 Range 0 - 1000 <p>Specifies the number of locks held to print for each InnoDB transaction in <code>SHOW ENGINE INNODB STATUS</code>.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_print_lock_wait_timeout_info","title":"<code>innodb_print_lock_wait_timeout_info</code>","text":"<p>Implemented in Percona Server 5.7.20-18.</p> Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Boolean Default OFF <p>Makes InnoDB write information about all lock wait timeout errors into the log file.</p> <p>This allows to find out details about the failed transaction, and, most importantly, the blocking transaction. The query string can be obtained from the <code>EVENTS_STATEMENTS_CURRENT</code> table, based on the <code>PROCESSLIST_ID</code> field, which corresponds to <code>thread_id</code> from the log output.</p> <p>Taking into account that blocking a transaction is often a multiple statement one, the following query can be used to obtain blocking thread statements history:</p> <pre><code>SELECT s.SQL_TEXT FROM performance_schema.events_statements_history s\nINNER JOIN performance_schema.threads t ON t.THREAD_ID = s.THREAD_ID\nWHERE t.PROCESSLIST_ID = %d\nUNION\nSELECT s.SQL_TEXT FROM performance_schema.events_statements_current s\nINNER JOIN performance_schema.threads t ON t.THREAD_ID = s.THREAD_ID\nWHERE t.PROCESSLIST_ID = %d;\n</code></pre> <p>Note</p> <p>The <code>PROCESSLIST_ID</code> in this example is exactly the thread id from the error log output.</p>"},{"location":"diagnostics/innodb_show_status.html#status-variables","title":"Status Variables","text":"<p>The status variables here contain information available in the output of <code>SHOW ENGINE INNODB STATUS</code>, organized by the sections <code>SHOW ENGINE INNODB STATUS</code> displays. If you are familiar with the output of <code>SHOW ENGINE INNODB STATUS</code>, you will probably already recognize the information these variables contain.</p>"},{"location":"diagnostics/innodb_show_status.html#background-thread","title":"BACKGROUND THREAD","text":"<p>The following variables contain information in the <code>BACKGROUND THREAD</code> section of the output from <code>SHOW ENGINE INNODB STATUS</code>. An example of that output is:</p> <pre><code>-----------------\nBACKGROUND THREAD\n-----------------\nsrv_master_thread loops: 1 srv_active, 0 srv_shutdown, 11844 srv_idle\nsrv_master_thread log flush and writes: 11844\n</code></pre> <p>InnoDB has a source thread that performs background tasks depending on the server state, once per second. If the server is under workload, the source thread runs the following: performs background table drops; performs change buffer merge, adaptively; flushes the redo log to disk; evicts tables from the dictionary cache if needed to satisfy its size limit; makes a checkpoint. If the server is idle: performs background table drops, flushes, and/or checkpoints the redo log if needed due to the checkpoint age; performs change buffer merge at full I/O capacity; evicts tables from the dictionary cache if needed, and makes a checkpoint.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_master_thread_active_loops","title":"<code>Innodb_master_thread_active_loops</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the number of times the above one-second loop was executed for active server states.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_master_thread_idle_loops","title":"<code>Innodb_master_thread_idle_loops</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the number of times the above one-second loop was executed for idle server states.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_background_log_sync","title":"<code>Innodb_background_log_sync</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the number of times the InnoDB source thread has written and flushed the redo log.</p>"},{"location":"diagnostics/innodb_show_status.html#semaphores","title":"SEMAPHORES","text":"<p>The following variables contain information in the <code>SEMAPHORES</code> section of the output from <code>SHOW ENGINE INNODB STATUS</code>. An example of that output is:</p> <pre><code>----------\nSEMAPHORES\n----------\nOS WAIT ARRAY INFO: reservation count 9664, signal count 11182\nMutex spin waits 20599, rounds 223821, OS waits 4479\nRW-shared spins 5155, OS waits 1678; RW-excl spins 5632, OS waits 2592\nSpin rounds per wait: 10.87 mutex, 15.01 RW-shared, 27.19 RW-excl\n</code></pre>"},{"location":"diagnostics/innodb_show_status.html#insert-buffer-and-adaptive-hash-index","title":"INSERT BUFFER AND ADAPTIVE HASH INDEX","text":"<p>The following variables contain information in the <code>INSERT BUFFER AND ADAPTIVE HASH INDEX</code> section of the output from <code>SHOW ENGINE INNODB STATUS</code>. An example of that output is:</p> <pre><code>-------------------------------------\nINSERT BUFFER AND ADAPTIVE HASH INDEX\n-------------------------------------\nIbuf: size 1, free list len 6089, seg size 6091,\n44497 inserts, 44497 merged recs, 8734 merges\n0.00 hash searches/s, 0.00 non-hash searches/s\n</code></pre>"},{"location":"diagnostics/innodb_show_status.html#innodb_ibuf_free_list","title":"<code>Innodb_ibuf_free_list</code>","text":"Option Description Scope Global Data type Numeric"},{"location":"diagnostics/innodb_show_status.html#innodb_ibuf_segment_size","title":"<code>Innodb_ibuf_segment_size</code>","text":"Option Description Scope Global Data type Numeric"},{"location":"diagnostics/innodb_show_status.html#log","title":"LOG","text":"<p>The following variables contain information in the <code>LOG</code> section of the output from <code>SHOW ENGINE INNODB STATUS</code>. An example of that output is:</p> <pre><code>LOG\n---\nLog sequence number 10145937666\nLog flushed up to   10145937666\nPages flushed up to 10145937666\nLast checkpoint at  10145937666\nMax checkpoint age    80826164\nCheckpoint age target 78300347\nModified age          0\nCheckpoint age        0\n0 pending log writes, 0 pending chkp writes\n9 log i/o's done, 0.00 log i/o's/second\nLog tracking enabled\nLog tracked up to   10145937666\nMax tracked LSN age 80826164\n</code></pre>"},{"location":"diagnostics/innodb_show_status.html#innodb_lsn_current","title":"<code>Innodb_lsn_current</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the current log sequence number.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_lsn_flushed","title":"<code>Innodb_lsn_flushed</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the current maximum LSN that has been written and flushed to disk.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_lsn_last_checkpoint","title":"<code>Innodb_lsn_last_checkpoint</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the LSN of the latest completed checkpoint.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_checkpoint_age","title":"<code>Innodb_checkpoint_age</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the current InnoDB checkpoint age, for example, the difference between the current LSN and the LSN of the last completed checkpoint.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_checkpoint_max_age","title":"<code>Innodb_checkpoint_max_age</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the maximum allowed checkpoint age above which the redo log is close to full and a checkpoint must happen before any further redo log writes.</p>"},{"location":"diagnostics/innodb_show_status.html#buffer-pool-and-memory","title":"BUFFER POOL AND MEMORY","text":"<p>The following variables contain information in the <code>BUFFER POOL AND MEMORY</code> section of the output from <code>SHOW ENGINE INNODB STATUS</code>. An example of that output is:</p> <pre><code>----------------------\nBUFFER POOL AND MEMORY\n----------------------\nTotal memory allocated 137363456; in additional pool allocated 0\nTotal memory allocated by read views 88\nInternal hash tables (constant factor + variable factor)\n    Adaptive hash index 2266736         (2213368 + 53368)\n    Page hash           139112 (buffer pool 0 only)\n    Dictionary cache    729463  (554768 + 174695)\n    File system         824800  (812272 + 12528)\n    Lock system         333248  (332872 + 376)\n    Recovery system     0       (0 + 0)\nDictionary memory allocated 174695\nBuffer pool size        8191\nBuffer pool size, bytes 134201344\nFree buffers            7481\nDatabase pages          707\nOld database pages      280\nModified db pages       0\nPending reads 0\nPending writes: LRU 0, flush list 0 single page 0\nPages made young 0, not young 0\n0.00 youngs/s, 0.00 non-youngs/s\nPages read 707, created 0, written 1\n0.00 reads/s, 0.00 creates/s, 0.00 writes/s\nNo buffer pool page gets since the last printout\nPages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s\nLRU len: 707, unzip_LRU len: 0\n</code></pre>"},{"location":"diagnostics/innodb_show_status.html#innodb_mem_adaptive_hash","title":"<code>Innodb_mem_adaptive_hash</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the current size, in bytes, of the adaptive hash index.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_mem_dictionary","title":"<code>Innodb_mem_dictionary</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the current size, in bytes, of the InnoDB in-memory data dictionary info.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_mem_total","title":"<code>Innodb_mem_total</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the total amount of memory, in bytes, InnoDB has allocated in the process heap memory.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_buffer_pool_pages_lru_flushed","title":"<code>Innodb_buffer_pool_pages_LRU_flushed</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the total number of buffer pool pages that have been flushed from the LRU list, for example, too old pages which had to be flushed in order to make buffer pool room to read in new data pages.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_buffer_pool_pages_made_not_young","title":"<code>Innodb_buffer_pool_pages_made_not_young</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the number of times a buffer pool page was not marked as accessed recently in the LRU list because of innodb_old_blocks_time variable setting.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_buffer_pool_pages_made_young","title":"<code>Innodb_buffer_pool_pages_made_young</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the number of times a buffer pool page was moved to the young end of the LRU list due to its access, to prevent its eviction from the buffer pool.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_buffer_pool_pages_old","title":"<code>Innodb_buffer_pool_pages_old</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the total number of buffer pool pages which are considered to be old according to the Making the Buffer Pool Scan Resistant manual page.</p>"},{"location":"diagnostics/innodb_show_status.html#transactions","title":"TRANSACTIONS","text":"<p>The following variables contain information in the <code>TRANSACTIONS</code> section of the output from <code>SHOW INNODB STATUS</code>. An example of that output is:</p> <pre><code>------------\nTRANSACTIONS\n------------\nTrx id counter F561FD\nPurge done for trx's n:o &lt; F561EB undo n:o &lt; 0\nHistory list length 19\nLIST OF TRANSACTIONS FOR EACH SESSION:\n---TRANSACTION 0, not started, process no 993, OS thread id 140213152634640\nmysql thread id 15933, query id 32109 localhost root\nshow innodb status\n---TRANSACTION F561FC, ACTIVE 29 sec, process no 993, OS thread id 140213152769808 updating or deleting\nmysql tables in use 1, locked 1\n</code></pre>"},{"location":"diagnostics/innodb_show_status.html#innodb_max_trx_id","title":"<code>Innodb_max_trx_id</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the next free transaction id number.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_oldest_view_low_limit_trx_id","title":"<code>Innodb_oldest_view_low_limit_trx_id</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the highest transaction id, above which the current oldest open read view does not see any transaction changes. Zero if there is no open view.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_purge_trx_id","title":"<code>Innodb_purge_trx_id</code>","text":"Option Description Scope Global Data type Numeric <p>This variable shows the oldest transaction id whose records have not been purged yet.</p>"},{"location":"diagnostics/innodb_show_status.html#innodb_purge_undo_no","title":"<code>Innodb_purge_undo_no</code>","text":"Option Description Scope Global Data type Numeric"},{"location":"diagnostics/innodb_show_status.html#information_schema-tables","title":"INFORMATION_SCHEMA Tables","text":"<p>The following table contains information about the oldest active transaction in the system.</p>"},{"location":"diagnostics/innodb_show_status.html#information_schemaxtradb_read_view","title":"<code>INFORMATION_SCHEMA.XTRADB_READ_VIEW</code>","text":"Column Name Description <code>READ_VIEW_LOW_LIMIT_TRX_NUMBER</code> This is the highest transactions number at the time the view was created. <code>READ_VIEW_UPPER_LIMIT_TRX_ID</code> This is the highest transactions ID at the time the view was created. This means that it should not see newer transactions with IDs bigger than or equal to that value. <code>READ_VIEW_LOW_LIMIT_TRX_ID</code> This is the latest committed transaction ID at the time the oldest view was created. This means that it should see all transactions with IDs smaller than or equal to that value. <p>The following table contains information about the memory usage for InnoDB/XtraDB hash tables.</p>"},{"location":"diagnostics/innodb_show_status.html#information_schemaxtradb_internal_hash_tables","title":"<code>INFORMATION_SCHEMA.XTRADB_INTERNAL_HASH_TABLES</code>","text":"Column Name Description <code>INTERNAL_HASH_TABLE_NAME</code> Hash table name <code>TOTAL_MEMORY</code> Total amount of memory <code>CONSTANT_MEMORY</code> Constant memory <code>VARIABLE_MEMORY</code> Variable memory"},{"location":"diagnostics/innodb_show_status.html#other-reading","title":"Other reading","text":"<ul> <li> <p>SHOW INNODB STATUS walk through</p> </li> <li> <p>Table locks in SHOW INNODB STATUS</p> </li> </ul>"},{"location":"diagnostics/libcoredumper.html","title":"Using libcoredumper","text":"<ul> <li> <p>Availability</p> <p>This feature is tech preview quality.</p> </li> </ul> <p>This feature was implemented in Percona Server for MySQL 5.7.31-34 and has been tested against this version\u2019s supported operating systems.</p> Operating Systems Versions Debian 9, 10 Red Hat Enterprise Linux and derivatives 6, 7, 8 Ubuntu 16.04, 18.04, 20.04 <p>The tool is experimental and may not be tested against newer operating systems in the future.</p> <p>The documented moment of a computer when either the computer or an application crashed is a core dump file. Developers examine the dump as one of the tasks when searching for the cause of a failure.</p> <p>The <code>libcoredumper</code> is a free and Open Source fork of <code>google-coredumper</code>, enhanced to work on newer Linux versions, and GCC and CLANG.</p> <p>You should test it before putting this tool into production.</p>"},{"location":"diagnostics/libcoredumper.html#enabling-the-libcoredumper","title":"Enabling the <code>libcoredumper</code>","text":"<p>Enable core dumps for troubleshooting purposes.</p> <p>To enable the <code>libcoredumper</code>, add the <code>coredumper</code> variable to the <code>mysqld</code> section of <code>my.cnf</code>. This variable is independent of the older <code>core-file</code> variable.</p> <p>The variable can have the following possible values:</p> Value Description Blank The core dump is saved under MySQL datadir and named <code>core</code>. A path ending with <code>/</code> The core dump is saved under the specified directory and named <code>core</code>. Full path with a filename The core dump is saved under the specified directory and with the specified filename <p>Restart the server.</p>"},{"location":"diagnostics/libcoredumper.html#verifying-the-libcoredumper-is-active","title":"Verifying the <code>libcoredumper</code> is Active","text":"<p>MySQL writes to the log when generating a core file and delegates the core dump operation to the Linux kernel. An example of the log message is the following:</p> <pre><code>Writing a core file\n</code></pre> <p>MySQL using the <code>libcoredumper</code> to generate the file creates the following message in the log:</p> <pre><code>Writing a core file using lib coredumper\n</code></pre> <p>Every core file adds a crash timestamp instead of a PID for the following reasons:</p> <ul> <li>Lets you correlate the core file with a crash. MySQL prints a UTC timestamp on the crash log.</li> </ul> <pre><code>10:02:09 UTC - mysqld got signal 11;\n</code></pre> <ul> <li>Lets you keep multiple core files.</li> </ul> <p>Note</p> <p>For example, the operators and containers run as PID 1. If the process ID identified the core file, each container crash generates a core dump that overwrites the previous core file.</p>"},{"location":"diagnostics/libcoredumper.html#disabling-the-libcoredumper","title":"Disabling the libcoredumper","text":"<p>You can disable the libcoredumper. A core file may contain sensitive data and takes disk space.</p> <p>To disable the <code>libcoredumper</code> you must do the following:</p> <ol> <li> <p>In the <code>mysqld</code> section of my.cnf, remove the <code>libcoredumper</code> variable.</p> </li> <li> <p>Restart the server.</p> </li> </ol>"},{"location":"diagnostics/misc_info_schema_tables.html","title":"Misc. INFORMATION_SCHEMA Tables","text":"<p>This page lists the <code>INFORMATION_SCHEMA</code> tables added to standard MySQL by Percona Server for MySQL that don\u2019t exist elsewhere in the documentation.</p>"},{"location":"diagnostics/misc_info_schema_tables.html#temporary-tables","title":"Temporary tables","text":"<p>Note</p> <p>This feature implementation is considered ALPHA quality.</p> <p>Only the temporary tables that were explicitly created with CREATE TEMPORARY TABLE or ALTER TABLE are shown, and not the ones created to process complex queries.</p>"},{"location":"diagnostics/misc_info_schema_tables.html#information_schemaglobal_temporary_tables","title":"<code>INFORMATION_SCHEMA.GLOBAL_TEMPORARY_TABLES</code>","text":"<p>Percona Server for MySQL 5.7.10-1 - Feature ported from Percona Server for MySQL 5.6</p> Column Name Description <code>SESSION_ID</code> \u2018MySQL connection id\u2019 <code>TABLE_SCHEMA</code> \u2018Schema in which the temporary table is created\u2019 <code>TABLE_NAME</code> \u2018Name of the temporary table\u2019 <code>ENGINE</code> \u2018Engine of the temporary table\u2019 <code>NAME</code> \u2018Internal name of the temporary table\u2019 <code>TABLE_ROWS</code> \u2018Number of rows of the temporary table\u2019 <code>AVG_ROW_LENGTH</code> \u2018Average row length of the temporary table\u2019 <code>DATA_LENGTH</code> \u2018Size of the data (Bytes)\u2019 <code>INDEX_LENGTH</code> \u2018Size of the indexes (Bytes)\u2019 <code>CREATE_TIME</code> \u2018Date and time of the creation of the temporary table\u2019 <code>UPDATE_TIME</code> \u2018Date and time of the latest update of the temporary table\u2019 <p>This table holds information on the temporary tables existing for all connections. You don\u2019t need the <code>SUPER</code> privilege to query this table.</p>"},{"location":"diagnostics/misc_info_schema_tables.html#information_schematemporary_tables","title":"<code>INFORMATION_SCHEMA.TEMPORARY_TABLES</code>","text":"<p>Percona Server for MySQL 5.7.10-1 - Feature ported from Percona Server for MySQL 5.6</p> Column Name Description <code>SESSION_ID</code> \u2018MySQL connection id\u2019 <code>TABLE_SCHEMA</code> \u2018Schema in which the temporary table is created\u2019 <code>TABLE_NAME</code> \u2018Name of the temporary table\u2019 <code>ENGINE</code> \u2018Engine of the temporary table\u2019 <code>NAME</code> \u2018Internal name of the temporary table\u2019 <code>TABLE_ROWS</code> \u2018Number of rows of the temporary table\u2019 <code>AVG_ROW_LENGTH</code> \u2018Average row length of the temporary table\u2019 <code>DATA_LENGTH</code> \u2018Size of the data (Bytes)\u2019 <code>INDEX_LENGTH</code> \u2018Size of the indexes (Bytes)\u2019 <code>CREATE_TIME</code> \u2018Date and time of the creation of the temporary table\u2019 <code>UPDATE_TIME</code> \u2018Date and time of the latest update of the temporary table\u2019 <p>This table holds information on the temporary tables existing for the running connection.</p>"},{"location":"diagnostics/misc_info_schema_tables.html#multiple-rollback-segments","title":"Multiple Rollback Segments","text":"<p>Percona Server for MySQL, in addition to the upstream multiple rollback segment implementation, provides the additional Information Schema table: <code>INFORMATION_SCHEMA.XTRADB_RSEG</code>.</p>"},{"location":"diagnostics/misc_info_schema_tables.html#information_schema-tables","title":"<code>INFORMATION_SCHEMA</code> Tables","text":"<p>This feature provides the following table:</p>"},{"location":"diagnostics/misc_info_schema_tables.html#information_schemaxtradb_rseg","title":"<code>INFORMATION_SCHEMA.XTRADB_RSEG</code>","text":"Column Name Description <code>rseg_id</code> \u2018rollback segment id\u2019 <code>space_id</code> \u2018space where the segment placed\u2019 <code>physical_page_size</code> \u2018physical page size\u2019 <code>logical_page_size</code> \u2018logical page size\u2019 <code>is_compressed</code> \u2018is the page compressed\u2019 <code>page_no</code> \u2018page number of the segment header\u2019 <code>max_size</code> \u2018max size in pages\u2019 <code>curr_size</code> \u2018current size in pages\u2019 <p>This table shows information about all the rollback segments (the default segment and the extra segments).</p> <p>Here is an example of output with <code>innodb_rollback_segments = 8</code>:</p> <pre><code>mysql&gt; SELECT * FROM INFORMATION_SCHEMA.XTRADB_RSEG;\n</code></pre> <p>The output resembles the following:</p> <pre><code>+---------+----------+--------------------+-------------------+---------------+---------+------------+-----------+\n| rseg_id | space_id | physical_page_size | logical_page_size | is_compressed | page_no | max_size   | curr_size |\n+---------+----------+--------------------+-------------------+---------------+---------+------------+-----------+\n|       0 |        0 |              16384 |             16384 |             0 |       6 | 4294967294 |         2 |\n|       1 |       24 |              16384 |             16384 |             0 |       3 | 4294967294 |         1 |\n|       2 |       24 |              16384 |             16384 |             0 |       4 | 4294967294 |         1 |\n|       3 |       24 |              16384 |             16384 |             0 |       5 | 4294967294 |         1 |\n|       4 |       24 |              16384 |             16384 |             0 |       6 | 4294967294 |         1 |\n|       5 |       24 |              16384 |             16384 |             0 |       7 | 4294967294 |         1 |\n|       6 |       24 |              16384 |             16384 |             0 |       8 | 4294967294 |         1 |\n|       7 |       24 |              16384 |             16384 |             0 |       9 | 4294967294 |         1 |\n|       8 |       24 |              16384 |             16384 |             0 |      10 | 4294967294 |         1 |\n+---------+----------+--------------------+-------------------+---------------+---------+------------+-----------+\n9 rows in set (0.00 sec)\n</code></pre>"},{"location":"diagnostics/process_list.html","title":"Process List","text":"<p>This page describes Percona changes to both the standard MySQL <code>SHOW PROCESSLIST</code> command and the standard MySQL <code>INFORMATION_SCHEMA</code> table <code>PROCESSLIST</code>.</p>"},{"location":"diagnostics/process_list.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"diagnostics/process_list.html#information_schema-tables","title":"INFORMATION_SCHEMA Tables","text":"<p><code>INFORMATION_SCHEMA.PROCESSLIST</code></p> <p>This table implements modifications to the standard MySQL <code>INFORMATION_SCHEMA</code> table <code>PROCESSLIST</code>.</p> Column Name Description \u2018ID\u2019 \u2018The connection identifier.\u2019 \u2018USER\u2019 \u2018The MySQL user who issued the statement.\u2019 \u2018HOST\u2019 \u2018The host name of the client issuing the statement.\u2019 \u2018DB\u2019 \u2018The default database, if one is selected, otherwise NULL.\u2019 \u2018COMMAND\u2019 \u2018The type of command the thread is executing.\u2019 \u2018TIME\u2019 \u2018The time in seconds that the thread has been in its current state.\u2019 \u2018STATE\u2019 \u2018An action, event, or state that indicates what the thread is doing.\u2019 \u2018INFO\u2019 \u2018The statement that the thread is executing, or NULL if it is not executing any statement.\u2019 \u2018TIME_MS\u2019 \u2018The time in milliseconds that the thread has been in its current state.\u2019 \u2018ROWS_EXAMINED\u2019 \u2018The number of rows examined by the statement being executed (NOTE: This column is not updated for each examined row so it does not necessarily show an up-to-date value while the statement is executing. It only shows a correct value after the statement has completed.).\u2019 \u2018ROWS_SENT\u2019 \u2018The number of rows sent by the statement being executed.\u2019 \u2018TID\u2019 \u2018The Linux Thread ID. For Linux, this corresponds to light-weight process ID (LWP ID) and can be seen in the ps -L output. In case when Thread Pool is enabled, \u201cTID\u201d is not null for only currently executing statements and statements received via \u201cextra\u201d connection.\u2019"},{"location":"diagnostics/process_list.html#example-output","title":"Example Output","text":"<p>Table PROCESSLIST:</p> <pre><code>mysql&gt; SELECT * FROM INFORMATION_SCHEMA.PROCESSLIST;\n</code></pre> <p>The output could be similar to the following:</p> <pre><code>+----+------+-----------+--------------------+---------+------+-----------+---------------------------+---------+-----------+---------------+\n| ID | USER | HOST      | DB                 | COMMAND | TIME | STATE     | INFO                      | TIME_MS | ROWS_SENT | ROWS_EXAMINED |\n+----+------+-----------+--------------------+---------+------+-----------+---------------------------+---------+-----------+---------------+\n| 12 | root | localhost | information_schema | Query   |    0 | executing | select * from processlist |       0 |         0 |             0 |\n+----+------+-----------+--------------------+---------+------+-----------+---------------------------+---------+-----------+---------------+\n</code></pre>"},{"location":"diagnostics/response_time_distribution.html","title":"Response Time Distribution","text":"<p>The slow query log provides exact information about queries that take a long time to execute. Sometimes there are a large number of queries that each take a short amount of time to execute. This feature provides a tool for analyzing that information by counting and displaying the number of queries according to the length of time they took to execute. The query execution time begins afterthe initial locks are acquired. The user can define time intervals that divide the range from 0 to positive infinity into smaller intervals and then collect the number of commands whose execution times fall into each of those intervals.</p> <p>Note that in a replication environment, the server will not take into account any queries executed by the replica SQL threads (whether they are slow or not) for the time distribution.</p> <p>Each interval is described as:</p> <pre><code>(range_base ^ n; range_base ^ (n+1)]\n</code></pre> <p>The range_base is some positive number (see Limitations). The interval is defined as the difference between two nearby powers of the range base.</p> <p>For example, if the range base=10, we have the following intervals:</p> <pre><code>(0; 10 ^ -6], (10 ^ -6; 10 ^ -5], (10 ^ -5; 10 ^ -4], ..., (10 ^ -1; 10 ^1], (10^1; 10^2]...(10^7; positive infinity]\n</code></pre> <p>or</p> <pre><code>(0; 0.000001], (0.000001; 0.000010], (0.000010; 0.000100], ..., (0.100000; 1.0]; (1.0; 10.0]...(1000000; positive infinity]\n</code></pre> <p>For each interval, a count is made of the queries with execution times that fell into that interval.</p> <p>You can select the range of the intervals by changing the range base. For example, for base range=2 we have the following intervals:</p> <pre><code>(0; 2 ^ -19], (2 ^ -19; 2 ^ -18], (2 ^ -18; 2 ^ -17], ..., (2 ^ -1; 2 ^1], (2 ^ 1; 2 ^ 2]...(2 ^ 25; positive infinity]\n</code></pre> <p>or</p> <pre><code>(0; 0.000001], (0.000001, 0.000003], ..., (0.25; 0.5], (0.5; 2], (2; 4]...(8388608; positive infinity]\n</code></pre> <p>Small numbers look strange (i.e., don\u2019t look like powers of 2), because we lose precision on division when the ranges are calculated at runtime. In the resulting table, you look at the high boundary of the range.</p> <p>For example, you may see:</p> <pre><code>+----------------+-------+------------+\n|      time      | count |    total   |\n+----------------+-------+------------|\n|       0.000001 |     0 |   0.000000 |\n|       0.000010 |    17 |   0.000094 |\n|       0.000100 |  4301 |   0.236555 |\n|       0.001000 |  1499 |   0.824450 |\n|       0.010000 | 14851 |  81.680502 |\n|       0.100000 |  8066 | 443.635693 |\n|       1.000000 |     0 |   0.000000 |\n|      10.000000 |     0 |   0.000000 |\n|     100.000000 |     1 |  55.937094 |\n|    1000.000000 |     0 |   0.000000 |\n|   10000.000000 |     0 |   0.000000 |\n|  100000.000000 |     0 |   0.000000 |\n| 1000000.000000 |     0 |   0.000000 |\n| TOO LONG QUERY |     0 |   0.000000 |\n+----------------+-------+------------+\n</code></pre> <p>This means there were:</p> <pre><code>* 17 queries with 0.000001 &lt; query execution time &lt; = 0.000010 seconds; total execution time of the 17 queries = 0.000094 seconds\n\n* 4301 queries with 0.000010 &lt; query execution time &lt; = 0.000100 seconds; total execution time of the 4301 queries = 0.236555 seconds\n\n* 1499 queries with 0.000100 &lt; query execution time &lt; = 0.001000 seconds; total execution time of the 1499 queries = 0.824450 seconds\n\n* 14851 queries with 0.001000 &lt; query execution time &lt; = 0.010000 seconds; total execution time of the 14851 queries = 81.680502 seconds\n\n* 8066 queries with 0.010000 &lt; query execution time &lt; = 0.100000 seconds; total execution time of the 8066 queries = 443.635693 seconds\n\n* 1 query with 10.000000 &lt; query execution time &lt; = 100.0000 seconds; total execution time of the 1 query = 55.937094 seconds\n</code></pre>"},{"location":"diagnostics/response_time_distribution.html#logging-the-queries-in-separate-read-and-write-tables","title":"Logging the queries in separate <code>READ</code> and <code>WRITE</code> tables","text":"<p>Percona Server for MySQL is now able to log the queries response times into separate <code>READ</code> and <code>WRITE</code> <code>INFORMATION_SCHEMA</code> tables. The two new tables are named INFORMATION_SCHEMA.QUERY_RESPONSE_TIME_READ and INFORMATION_SCHEMA.QUERY_RESPONSE_TIME_WRITE respectively. The decision on whether a query is a <code>read</code> or a <code>write</code> is based on the type of the command. Thus, for example, an <code>UPDATE ... WHERE &lt;condition&gt;</code> is always logged as a <code>write</code> query even if <code>&lt;condition&gt;</code> is always false and thus no actual writes happen during its execution.</p> <p>Following SQL commands will be considered as <code>WRITE</code> queries and will be logged into the INFORMATION_SCHEMA.QUERY_RESPONSE_TIME_WRITE table: <code>CREATE_TABLE</code>, <code>CREATE_INDEX</code>, <code>ALTER_TABLE</code>, <code>TRUNCATE</code>, <code>DROP_TABLE</code>, <code>LOAD</code>, <code>CREATE_DB</code>, <code>DROP_DB</code>, <code>ALTER_DB</code>, <code>RENAME_TABLE</code>, <code>DROP_INDEX</code>, <code>CREATE_VIEW</code>, <code>DROP_VIEW</code>, <code>CREATE_TRIGGER</code>, <code>DROP_TRIGGER</code>, <code>CREATE_EVENT</code>, <code>ALTER_EVENT</code>, <code>DROP_EVENT</code>, <code>UPDATE</code>, <code>UPDATE_MULTI</code>, <code>INSERT</code>, <code>INSERT_SELECT</code>, <code>DELETE</code>, <code>DELETE_MULTI</code>, <code>REPLACE</code>, <code>REPLACE_SELECT</code>, <code>CREATE_USER</code>, <code>RENAME_USER</code>, <code>DROP_USER</code>, <code>ALTER_USER</code>, <code>GRANT</code>, <code>REVOKE</code>, <code>REVOKE_ALL</code>, <code>OPTIMIZE</code>, <code>CREATE_FUNCTION</code>, <code>CREATE_PROCEDURE</code>, <code>CREATE_SPFUNCTION</code>, <code>DROP_PROCEDURE</code>, <code>DROP_FUNCTION</code>, <code>ALTER_PROCEDURE</code>, <code>ALTER_FUNCTION</code>, <code>INSTALL_PLUGIN</code>, and <code>UNINSTALL_PLUGIN</code>. Commands not listed here are considered as <code>READ</code> queries and will be logged into the INFORMATION_SCHEMA.QUERY_RESPONSE_TIME_READ table.</p>"},{"location":"diagnostics/response_time_distribution.html#installing-the-plugins","title":"Installing the plugins","text":"<p>In order to enable this feature you\u2019ll need to install the necessary plugins:</p> <pre><code>mysql&gt; INSTALL PLUGIN QUERY_RESPONSE_TIME_AUDIT SONAME 'query_response_time.so';\n</code></pre> <p>This plugin is used for gathering statistics.</p> <pre><code>mysql&gt; INSTALL PLUGIN QUERY_RESPONSE_TIME SONAME 'query_response_time.so';\n</code></pre> <p>This plugin provides the interface (INFORMATION_SCHEMA.QUERY_RESPONSE_TIME) to output gathered statistics.</p> <pre><code>mysql&gt; INSTALL PLUGIN QUERY_RESPONSE_TIME_READ SONAME 'query_response_time.so';\n</code></pre> <p>This plugin provides the interface (INFORMATION_SCHEMA.QUERY_RESPONSE_TIME_READ) to output gathered statistics.</p> <pre><code>mysql&gt; INSTALL PLUGIN QUERY_RESPONSE_TIME_WRITE SONAME 'query_response_time.so';\n</code></pre> <p>This plugin provides the interface (INFORMATION_SCHEMA.QUERY_RESPONSE_TIME_WRITE) to output gathered statistics.</p> <p>You can check if plugins are installed correctly by running:</p> <pre><code>mysql&gt; SHOW PLUGINS;\n</code></pre> <p>The output could be similar to the following:</p> <pre><code>...\n| QUERY_RESPONSE_TIME         | ACTIVE   | INFORMATION SCHEMA | query_response_time.so | GPL     |\n| QUERY_RESPONSE_TIME_AUDIT   | ACTIVE   | AUDIT              | query_response_time.so | GPL     |\n| QUERY_RESPONSE_TIME_READ    | ACTIVE   | INFORMATION SCHEMA | query_response_time.so | GPL     |\n| QUERY_RESPONSE_TIME_WRITE   | ACTIVE   | INFORMATION SCHEMA | query_response_time.so | GPL     |\n+-----------------------------+----------+--------------------+------------------------+---------+\n</code></pre>"},{"location":"diagnostics/response_time_distribution.html#usage","title":"Usage","text":"<p>To start collecting query time metrics, query_response_time_stats should be enabled:</p> <pre><code>SET GLOBAL query_response_time_stats = on;\n</code></pre> <p>And to make it persistent, add the same to <code>my.cnf</code>:</p> <pre><code>[mysqld]\nquery_response_time_stats = on\n</code></pre>"},{"location":"diagnostics/response_time_distribution.html#select","title":"SELECT","text":"<p>You can get the distribution using the query:</p> <pre><code>mysql&gt; SELECT * from INFORMATION_SCHEMA.QUERY_RESPONSE_TIME;\n</code></pre> <p>The output could be similar to the following:</p> <pre><code>time                   count   total\n0.000001               0       0.000000\n0.000010               0       0.000000\n0.000100               1       0.000072\n0.001000               0       0.000000\n0.010000               0       0.000000\n0.100000               0       0.000000\n1.000000               0       0.000000\n10.000000              8       47.268416\n100.000000             0       0.000000\n1000.000000            0       0.000000\n10000.000000           0       0.000000\n100000.000000          0       0.000000\n1000000.000000         0       0.000000\nTOO LONG QUERY         0       0.000000\n</code></pre> <p>You can write a complex query like:</p> <pre><code>SELECT c.count, c.time,\n(SELECT SUM(a.count) FROM INFORMATION_SCHEMA.QUERY_RESPONSE_TIME as a WHERE a.count != 0) as query_count,\n(SELECT COUNT(*)     FROM INFORMATION_SCHEMA.QUERY_RESPONSE_TIME as b WHERE b.count != 0) as not_zero_region_count,\n(SELECT COUNT(*)     FROM INFORMATION_SCHEMA.QUERY_RESPONSE_TIME) as region_count\nFROM INFORMATION_SCHEMA.QUERY_RESPONSE_TIME as c WHERE c.count &gt; 0;\n</code></pre> <p>Note</p> <p>If query_response_time_stats is ON, the execution times for these two <code>SELECT</code> queries will also be collected.</p>"},{"location":"diagnostics/response_time_distribution.html#flush","title":"FLUSH","text":"<p>Flushing can be done by setting the query_response_time_flush to <code>ON</code> (or <code>1</code>):</p> <pre><code>mysql&gt; SET GLOBAL query_response_time_flush='ON';\n</code></pre> <p><code>FLUSH</code> does two things:</p> <ul> <li> <p>Clears the collected times from the INFORMATION_SCHEMA.QUERY_RESPONSE_TIME, INFORMATION_SCHEMA.QUERY_RESPONSE_TIME_READ, and INFORMATION_SCHEMA.QUERY_RESPONSE_TIME_WRITE tables</p> </li> <li> <p>Reads the value of query_response_time_range_base and uses it to set the range base for the table</p> </li> </ul> <p>Note</p> <p>The execution time for the <code>FLUSH</code> query will also be collected.</p>"},{"location":"diagnostics/response_time_distribution.html#stored-procedures","title":"Stored procedures","text":"<p>Stored procedure calls count as a single query.</p>"},{"location":"diagnostics/response_time_distribution.html#collect-time-point","title":"Collect time point","text":"<p>Time is collected after query execution completes (before clearing data structures).</p>"},{"location":"diagnostics/response_time_distribution.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"diagnostics/response_time_distribution.html#system-variables","title":"System Variables","text":""},{"location":"diagnostics/response_time_distribution.html#query_response_time_flush","title":"<code>query_response_time_flush</code>","text":"Option Description Command-line Yes Config file No Scope Global Dynamic No Data type Boolean Default OFF Range OFF/ON <p>Setting this variable to <code>ON</code> will flush the statistics and re-read the query_response_time_range_base.</p>"},{"location":"diagnostics/response_time_distribution.html#query_response_time_range_base","title":"<code>query_response_time_range_base</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 10 Range 2-1000 <p>Sets up the logarithm base for the scale.</p> <p>Note</p> <p>The variable takes effect only after this command has been executed:</p> <pre><code>mysql&gt; SET GLOBAL query_response_time_flush=1;\n</code></pre>"},{"location":"diagnostics/response_time_distribution.html#query_response_time_stats","title":"<code>query_response_time_stats</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Boolean Default OFF Range ON/OFF <p>This global variable enables and disables collection of query times.</p>"},{"location":"diagnostics/response_time_distribution.html#query_response_time_session_stats","title":"<code>query_response_time_session_stats</code>","text":"Option Description Command-line No Config file No Scope Session Dynamic Yes Data type Text Default GLOBAL Range ON/OFF/GLOBAL <p>This variable enables and disables collection of query times on session level, thus customizing QRT behavior for individual connections. By default, its value is GLOBAL, which means that its value is taken from the query_response_time_stats variable.</p>"},{"location":"diagnostics/response_time_distribution.html#information_schema-tables","title":"INFORMATION_SCHEMA Tables","text":""},{"location":"diagnostics/response_time_distribution.html#information_schemaquery_response_time","title":"<code>INFORMATION_SCHEMA.QUERY_RESPONSE_TIME</code>","text":"Column Name Description \u2018VARCHAR TIME\u2019 \u2018Interval range in which the query occurred\u2019 \u2018INT(11) COUNT\u2019 \u2018Number of queries with execution times that fell into that interval\u2019 \u2018VARCHAR TOTAL\u2019 \u2018Total execution time of the queries \u2018"},{"location":"diagnostics/response_time_distribution.html#information_schemaquery_response_time_read","title":"<code>INFORMATION_SCHEMA.QUERY_RESPONSE_TIME_READ</code>","text":"Column Name Description \u2018VARCHAR TIME\u2019 \u2018Interval range in which the query occurred\u2019 \u2018INT(11) COUNT\u2019 \u2018Number of queries with execution times that fell into that interval\u2019 \u2018VARCHAR TOTAL\u2019 \u2018Total execution time of the queries \u2018"},{"location":"diagnostics/response_time_distribution.html#information_schemaquery_response_time_write","title":"<code>INFORMATION_SCHEMA.QUERY_RESPONSE_TIME_WRITE</code>","text":"Column Name Description \u2018VARCHAR TIME\u2019 \u2018Interval range in which the query occurred\u2019 \u2018INT(11) COUNT\u2019 \u2018Number of queries with execution times that fell into that interval\u2019 \u2018VARCHAR TOTAL\u2019 \u2018Total execution time of the queries \u2018"},{"location":"diagnostics/scalability_metrics_plugin.html","title":"Metrics for scalability measurement","text":"<p>Note</p> <p>This feature has been deprecated in Percona Server for MySQL Percona Server for MySQL 5.7.16-10. Users who have installed this plugin but are not using its capability are advised to uninstall the plugin due to known crashing bugs.</p> <p>Percona Server for MySQL has implemented extra scalability metrics. These metrics allow using Little\u2019s Law, queuing theory, and Universal Scalability Law to gain insights into server performance. This feature is implemented as a plugin.</p>"},{"location":"diagnostics/scalability_metrics_plugin.html#installation","title":"Installation","text":"<p>Scalability Metrics plugin is shipped with Percona Server for MySQL, but it is not installed by default. To enable the plugin you must run the following command:</p> <pre><code>INSTALL PLUGIN scalability_metrics SONAME 'scalability_metrics.so';\n</code></pre> <p>You can check if the plugin is loaded correctly by running:</p> <pre><code>SHOW PLUGINS;\n</code></pre> <p>The plugin should be listed in the output:</p> <pre><code>+--------------------------------+----------+--------------------+------------------------+---------+\n| Name                           | Status   | Type               | Library                | License |\n+--------------------------------+----------+--------------------+------------------------+---------+\n...\n| scalability_metrics            | ACTIVE   | AUDIT              | scalability_metrics.so | GPL     |\n+--------------------------------+----------+--------------------+------------------------+---------+\n</code></pre>"},{"location":"diagnostics/scalability_metrics_plugin.html#system-variables","title":"System Variables","text":""},{"location":"diagnostics/scalability_metrics_plugin.html#scalability_metrics_control","title":"<code>scalability_metrics_control</code>","text":"Option Description Command-line Yes Scope Global Dynamic Yes Data type String Default OFF Values OFF, ON, RESET <p>This variable can be used to enable and disable the collection of metrics for scalability measurement. By setting the value to <code>RESET</code> all counters will be reset while continuing to count metrics.</p>"},{"location":"diagnostics/scalability_metrics_plugin.html#status-variables","title":"Status Variables","text":""},{"location":"diagnostics/scalability_metrics_plugin.html#scalability_metrics_elapsedtime","title":"<code>scalability_metrics_elapsedtime</code>","text":"Option Description Data type Numeric <p>This status variable shows total time elapsed, in microseconds, since metrics collection was started.</p>"},{"location":"diagnostics/scalability_metrics_plugin.html#scalability_metrics_queries","title":"<code>scalability_metrics_queries</code>","text":"Option Description Data type Numeric <p>This status variable shows number of completed queries since metrics collection was started.</p>"},{"location":"diagnostics/scalability_metrics_plugin.html#scalability_metrics_concurrency","title":"<code>scalability_metrics_concurrency</code>","text":"Option Description Data type Numeric <p>This status variable shows number of queries currently executed.</p>"},{"location":"diagnostics/scalability_metrics_plugin.html#scalability_metrics_totaltime","title":"<code>scalability_metrics_totaltime</code>","text":"Option Description Data type Numeric <p>This status variable shows total execution time of all queries, including the in-progress time of currently executing queries, in microseconds (ie. if two queries executed with 1 second of response time each, the result is 2 seconds).</p>"},{"location":"diagnostics/scalability_metrics_plugin.html#scalability_metrics_busytime","title":"<code>scalability_metrics_busytime</code>","text":"Option Description Data type Numeric <p>This counter accounts the non-idle server time, that is, time when at least one query was executing.</p>"},{"location":"diagnostics/scalability_metrics_plugin.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li> <p>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</p> </li> <li> <p>Percona Server for MySQL 5.7.16-10: Feature has been deprecated.</p> </li> </ul>"},{"location":"diagnostics/scalability_metrics_plugin.html#other-reading","title":"Other Reading","text":"<ul> <li> <p>Fundamental performance and scalability instrumentation</p> </li> <li> <p>Forecasting MySQL Scalability with the Universal Scalability Law Whitepaper</p> </li> </ul>"},{"location":"diagnostics/show_engines.html","title":"Show Storage Engines","text":"<p>This feature changes the comment field displayed when the <code>SHOW STORAGE ENGINES</code> command is executed and XtraDB is the storage engine.</p> <p>Before the change:</p> <pre><code>mysql&gt; show storage engines;\n</code></pre> <p>The output could be similar to the following:</p> <pre><code>+------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| Engine     | Support | Comment                                                        | Transactions | XA   | Savepoints |\n+------------+---------+----------------------------------------------------------------+--------------+------+------------+\n| InnoDB     | YES     | Supports transactions, row-level locking, and foreign keys     | YES          | YES  | YES        |\n...\n+------------+---------+----------------------------------------------------------------+--------------+------+------------+\n</code></pre> <p>After the change:</p> <pre><code>mysql&gt; show storage engines;\n</code></pre> <p>The output could be similar to the following:</p> <pre><code>+------------+---------+----------------------------------------------------------------------------+--------------+------+------------+\n| Engine     | Support | Comment                                                                    | Transactions |   XA | Savepoints |\n+------------+---------+----------------------------------------------------------------------------+--------------+------+------------+\n| InnoDB     | YES     | Percona-XtraDB, Supports transactions, row-level locking, and foreign keys |          YES | YES  | YES        |\n...\n+------------+---------+----------------------------------------------------------------------------+--------------+------+------------+\n</code></pre>"},{"location":"diagnostics/show_engines.html#version-specific-information","title":"Version-Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"diagnostics/slow_extended.html","title":"Slow Query Log","text":"<p>This feature adds microsecond time resolution and additional statistics to the slow query log output. It lets you enable or disable the slow query log at runtime, adds logging for the replica SQL thread, and adds fine-grained control over what and how much to log into the slow query log.</p> <p>You can use Percona-Toolkit\u2019s pt-query-digest tool to aggregate similar queries together and report on those that consume the most execution time.</p>"},{"location":"diagnostics/slow_extended.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6.</li> </ul>"},{"location":"diagnostics/slow_extended.html#system-variables","title":"System Variables","text":""},{"location":"diagnostics/slow_extended.html#log_slow_filter","title":"<code>log_slow_filter</code>","text":"Option Description Command-line Yes Config file Yes Scope Global, Session Dynamic Yes <p>Filters the slow log by the query\u2019s execution plan. The value is a comma-delimited string, and can contain any combination of the following values:</p> <ul> <li> <p><code>qc_miss</code>: The query was not found in the query cache.</p> </li> <li> <p><code>full_scan</code>: The query performed a full table scan.</p> </li> <li> <p><code>full_join</code>: The query performed a full join (a join without indexes).</p> </li> <li> <p><code>tmp_table</code>: The query created an implicit internal temporary table.</p> </li> <li> <p><code>tmp_table_on_disk</code>: The query\u2019s temporary table was stored on disk.</p> </li> <li> <p><code>filesort</code>: The query used a filesort.</p> </li> <li> <p><code>filesort_on_disk</code>: The filesort was performed on disk.</p> </li> </ul> <p>Values are OR\u2019ed together. If the string is empty, then the filter is disabled. If it is not empty, then queries will only be logged to the slow log if their execution plan matches one of the types of plans present in the filter.</p> <p>For example, to log only queries that perform a full table scan, set the value to <code>full_scan</code>. To log only queries that use on-disk temporary storage for intermediate results, set the value to <code>tmp_table_on_disk,filesort_on_disk</code>.</p>"},{"location":"diagnostics/slow_extended.html#log_slow_rate_type","title":"<code>log_slow_rate_type</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Enumerated Default session Range session, query <p>Specifies semantic of log_slow_rate_limit - <code>session</code> or <code>query</code>.</p>"},{"location":"diagnostics/slow_extended.html#log_slow_rate_limit","title":"<code>log_slow_rate_limit</code>","text":"Option Description Command-line Yes Config file Yes Scope Global, session Dynamic Yes Default 1 Range 1-1000 <p>Behavior of this variable depends from log_slow_rate_type.</p> <p>Specifies that only a fraction of <code>session/query</code> should be logged. Logging is enabled for every nth <code>session/query</code>. By default, n is 1, so logging is enabled for every <code>session/query</code>. Please note: when log_slow_rate_type is <code>session</code> rate limiting is disabled for the replication thread.</p> <p>Logging all queries might consume I/O bandwidth and cause the log file to grow large.</p> <ul> <li> <p>When log_slow_rate_type is <code>session</code>, this option lets you log full sessions, so you have complete records of sessions for later analysis; but you can rate-limit the number of sessions that are logged. Note that this feature will not work well if your application uses any type of connection pooling or persistent connections. Note that you change log_slow_rate_limit in <code>session</code> mode, you should reconnect for get effect.</p> </li> <li> <p>When log_slow_rate_type is <code>query</code>, this option lets you log just some queries for later analysis. For example, if you set the value to 100, then one percent of queries will be logged.</p> </li> </ul> <p>Note that every query has global unique <code>query_id</code> and every connection can has it own (session) log_slow_rate_limit. Decision \u201clog or no\u201d calculated in following manner:</p> <ul> <li> <p>if <code>log_slow_rate_limit</code> is 1 - log every query</p> </li> <li> <p>If <code>log_slow_rate_limit</code> &gt; 1 - randomly log every 1/<code>log_slow_rate_limit</code> query.</p> </li> </ul> <p>This allows flexible setup logging behavior.</p> <p>For example, if you set the value to 100, then one percent of <code>sessions/queries</code> will be logged. In Percona Server for MySQL information about the log_slow_rate_limit has been added to the slow query log. This means that if the log_slow_rate_limit is effective it will be reflected in the slow query log for each written query. Example of the output looks like this:</p> <pre><code>Log_slow_rate_type: query  Log_slow_rate_limit: 10\n</code></pre>"},{"location":"diagnostics/slow_extended.html#log_slow_sp_statements","title":"<code>log_slow_sp_statements</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Boolean Default TRUE Range TRUE/FALSE <p>If <code>TRUE</code>, statements executed by stored procedures are logged to the slow if it is open.</p> <p>Percona Server for MySQL implemented improvements for logging of stored procedures to the slow query log:</p> <ul> <li> <p>Each query from a stored procedure is now logged to the slow query log individually</p> </li> <li> <p><code>CALL</code> itself isn\u2019t logged to the slow query log anymore as this would be counting twice for the same query which would lead to incorrect results</p> </li> <li> <p>Queries that were called inside of stored procedures are annotated in the slow query log with the stored procedure name in which they run.</p> </li> </ul> <p>Example of the improved stored procedure slow query log entry:</p> <pre><code>mysql&gt; DELIMITER //\nmysql&gt; CREATE PROCEDURE improved_sp_log()\nBEGIN\nSELECT * FROM City;\nSELECT * FROM Country;\nEND//\nmysql&gt; DELIMITER ;\nmysql&gt; CALL improved_sp_log();\n</code></pre> <p>When we check the slow query log after running the stored procedure, with <code>log_slow_sp_statements</code> set to <code>TRUE</code>, it should look like this:</p> <pre><code># Time: 150109 11:38:55\n# User@Host: root[root] @ localhost []\n# Thread_id: 40  Schema: world  Last_errno: 0  Killed: 0\n# Query_time: 0.012989  Lock_time: 0.000033  Rows_sent: 4079  Rows_examined: 4079  Rows_affected: 0  Rows_read: 4079\n# Bytes_sent: 161085\n# Stored routine: world.improved_sp_log\nSET timestamp=1420803535;\nSELECT * FROM City;\n# User@Host: root[root] @ localhost []\n# Thread_id: 40  Schema: world  Last_errno: 0  Killed: 0\n# Query_time: 0.001413  Lock_time: 0.000017  Rows_sent: 4318  Rows_examined: 4318  Rows_affected: 0  Rows_read: 4318\n# Bytes_sent: 194601\n# Stored routine: world.improved_sp_log\nSET timestamp=1420803535;\n</code></pre> <p>If variable log_slow_sp_statements is set to <code>FALSE</code>:</p> <ul> <li> <p>Entry is added to a slow-log for a <code>CALL</code> statement only and not for any of the individual statements run in that stored procedure</p> </li> <li> <p>Execution time is reported for the <code>CALL</code> statement as the total execution time of the <code>CALL</code> including all its statements</p> </li> </ul> <p>If we run the same stored procedure with the variable log_slow_sp_statements is set to <code>FALSE</code> slow query log should look like this:</p> <pre><code># Time: 150109 11:51:42\n# User@Host: root[root] @ localhost []\n# Thread_id: 40  Schema: world  Last_errno: 0  Killed: 0\n# Query_time: 0.013947  Lock_time: 0.000000  Rows_sent: 4318  Rows_examined: 4318  Rows_affected: 0  Rows_read: 4318\n# Bytes_sent: 194612\nSET timestamp=1420804302;\nCALL improved_sp_log();\n</code></pre> <p>Note</p> <p>Support for logging stored procedures doesn\u2019t involve triggers, so they won\u2019t be logged even if this feature is enabled.</p>"},{"location":"diagnostics/slow_extended.html#log_slow_verbosity","title":"<code>log_slow_verbosity</code>","text":"Option Description Command-line Yes Config file Yes Scope Global, session Dynamic Yes <p>Specifies how much information to include in your slow log. The value is a comma-delimited string, and can contain any combination of the following values:</p> <ul> <li> <p><code>microtime</code>: Log queries with microsecond precision.</p> </li> <li> <p><code>query_plan</code>: Log information about the query\u2019s execution plan.</p> </li> <li> <p><code>innodb</code>: Log InnoDB statistics.</p> </li> <li> <p><code>minimal</code>: Equivalent to enabling just <code>microtime</code>.</p> </li> <li> <p><code>standard</code>: Equivalent to enabling <code>microtime,query_plan</code>.</p> </li> <li> <p><code>full</code>: Equivalent to all other values OR\u2019ed together without the <code>profiling</code> and <code>profiling_use_getrusage</code> options.</p> </li> <li> <p><code>profiling</code>: Enables profiling of all queries in all connections.</p> </li> <li> <p><code>profiling_use_getrusage</code>: Enables usage of the getrusage function.</p> </li> </ul> <p>Values are OR\u2019ed together.</p> <p>For example, to enable microsecond query timing and InnoDB statistics, set this option to <code>microtime,innodb</code> or <code>standard</code>. To turn all options on, set the option to <code>full</code>.</p>"},{"location":"diagnostics/slow_extended.html#slow_query_log_use_global_control","title":"<code>slow_query_log_use_global_control</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Default None <p>Specifies which variables have global scope instead of local. For such variables, the global variable value is used in the current session, but without copying this value to the session value. Value is a \u201cflag\u201d variable - you can specify multiple values separated by commas</p> <ul> <li> <p><code>none</code>: All variables use local scope</p> </li> <li> <p><code>log_slow_filter</code>: Global variable log_slow_filter has effect (instead of local)</p> </li> <li> <p><code>log_slow_rate_limit</code>: Global variable log_slow_rate_limit has effect (instead of local)</p> </li> <li> <p><code>log_slow_verbosity</code>: Global variable log_slow_verbosity has effect (instead of local)</p> </li> <li> <p><code>long_query_time</code>: Global variable long_query_time has effect (instead of local)</p> </li> <li> <p><code>min_examined_row_limit</code>: Global variable <code>min_examined_row_limit</code> has effect (instead of local)</p> </li> <li> <p><code>all</code>: Global variables has effect (instead of local)</p> </li> </ul>"},{"location":"diagnostics/slow_extended.html#slow_query_log_always_write_time","title":"<code>slow_query_log_always_write_time</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Default 10 <p>This variable can be used to specify the query execution time after which the query will be written to the slow query log. It can be used to specify an additional execution time threshold for the slow query log, that, when exceeded, will cause a query to be logged unconditionally, that is, log_slow_rate_limit will not apply to it.</p>"},{"location":"diagnostics/slow_extended.html#other-information","title":"Other Information","text":""},{"location":"diagnostics/slow_extended.html#changes-to-the-log-format","title":"Changes to the Log Format","text":"<p>The feature adds more information to the slow log output. Here is a sample log entry:</p> <pre><code># Time: 130601  8:01:06.058915\n# User@Host: root[root] @ localhost []  Id:    42\n# Schema: imdb  Last_errno: 0  Killed: 0\n# Query_time: 7.725616  Lock_time: 0.000328  Rows_sent: 4  Rows_examined: 1543720  Rows_affected: 0\n# Bytes_sent: 272  Tmp_tables: 0  Tmp_disk_tables: 0  Tmp_table_sizes: 0\n# QC_Hit: No  Full_scan: Yes  Full_join: No  Tmp_table: No  Tmp_table_on_disk: No\n# Filesort: No  Filesort_on_disk: No  Merge_passes: 0\nSET timestamp=1370073666;\nSELECT id,title,production_year FROM title WHERE title = 'Bambi';\n</code></pre> <p>Another example (log_slow_verbosity <code>=profiling</code>):</p> <pre><code># Time: 130601  8:03:20.700441\n# User@Host: root[root] @ localhost []  Id:    43\n# Schema: imdb  Last_errno: 0  Killed: 0\n# Query_time: 7.815071  Lock_time: 0.000261  Rows_sent: 4  Rows_examined: 1543720  Rows_affected: 0\n# Bytes_sent: 272\n# Profile_starting: 0.000125 Profile_starting_cpu: 0.000120\nProfile_checking_permissions: 0.000021 Profile_checking_permissions_cpu: 0.000021\nProfile_Opening_tables: 0.000049 Profile_Opening_tables_cpu: 0.000048 Profile_init: 0.000048\nProfile_init_cpu: 0.000049 Profile_System_lock: 0.000049 Profile_System_lock_cpu: 0.000048\nProfile_optimizing: 0.000024 Profile_optimizing_cpu: 0.000024 Profile_statistics: 0.000036\nProfile_statistics_cpu: 0.000037 Profile_preparing: 0.000029 Profile_preparing_cpu: 0.000029\nProfile_executing: 0.000012 Profile_executing_cpu: 0.000012 Profile_Sending_data: 7.814583\nProfile_Sending_data_cpu: 7.811634 Profile_end: 0.000013 Profile_end_cpu: 0.000012\nProfile_query_end: 0.000014 Profile_query_end_cpu: 0.000014 Profile_closing_tables: 0.000023\nProfile_closing_tables_cpu: 0.000023 Profile_freeing_items: 0.000051\nProfile_freeing_items_cpu: 0.000050 Profile_logging_slow_query: 0.000006\nProfile_logging_slow_query_cpu: 0.000006\n# Profile_total: 7.815085 Profile_total_cpu: 7.812127\nSET timestamp=1370073800;\nSELECT id,title,production_year FROM title WHERE title = 'Bambi';\n</code></pre> <p>Notice that the <code>Killed: \\</code>` keyword is followed by zero when the query successfully completes. If the query was killed, the ``Killed:` keyword is followed by a number other than zero:</p> Killed Numeric Code Exception 0 NOT_KILLED 1 KILL_BAD_DATA 1053 ER_SERVER_SHUTDOWN (see MySQL Documentation) 1317 ER_QUERY_INTERRUPTED (see MySQL Documentation) 3024 ER_QUERY_TIMEOUT (see MySQL Documentation) Any other number KILLED_NO_VALUE (Catches all other cases)"},{"location":"diagnostics/slow_extended.html#connection-and-schema-identifier","title":"Connection and Schema Identifier","text":"<p>Each slow log entry now contains a connection identifier, so you can trace all the queries coming from a single connection. This is the same value that is shown in the Id column in <code>SHOW FULL PROCESSLIST</code> or returned from the <code>CONNECTION_ID()</code> function.</p> <p>Each entry also contains a schema name, so you can trace all the queries whose default database was set to a particular schema.</p> <pre><code># Id: 43  Schema: imdb\n</code></pre>"},{"location":"diagnostics/slow_extended.html#microsecond-time-resolution-and-extra-row-information","title":"Microsecond Time Resolution and Extra Row Information","text":"<p>This is the original functionality offered by the <code>microslow</code> feature. <code>Query_time</code> and <code>Lock_time</code> are logged with microsecond resolution.</p> <p>The feature also adds information about how many rows were examined for <code>SELECT</code> queries, and how many were analyzed and affected for <code>UPDATE</code>, <code>DELETE</code>, and <code>INSERT</code> queries,</p> <pre><code># Query_time: 0.962742  Lock_time: 0.000202  Rows_sent: 4  Rows_examined: 1543719  Rows_affected: 0\n</code></pre> <p>Values and context:</p> <ul> <li> <p><code>Rows_examined</code>: Number of rows scanned - <code>SELECT</code></p> </li> <li> <p><code>Rows_affected</code>: Number of rows changed - <code>UPDATE</code>, <code>DELETE</code>, <code>INSERT</code></p> </li> </ul>"},{"location":"diagnostics/slow_extended.html#memory-footprint","title":"Memory Footprint","text":"<p>The feature provides information about the amount of bytes sent for the result of the query and the number of temporary tables created for its execution - differentiated by whether they were created on memory or on disk - with the total number of bytes used by them.</p> <pre><code># Bytes_sent: 8053  Tmp_tables: 1  Tmp_disk_tables: 0  Tmp_table_sizes: 950528\n</code></pre> <p>Values and context:</p> <ul> <li> <p><code>Bytes_sent</code>: The amount of bytes sent for the result of the query</p> </li> <li> <p><code>Tmp_tables</code>: Number of temporary tables created on memory for the query</p> </li> <li> <p><code>Tmp_disk_tables</code>: Number of temporary tables created on disk for the query</p> </li> <li> <p><code>Tmp_table_sizes</code>: Total Size in bytes for all temporary tables used in the query</p> </li> </ul>"},{"location":"diagnostics/slow_extended.html#query-plan-information","title":"Query Plan Information","text":"<p>Each query can be executed in various ways. For example, it may use indexes or do a full table scan, or a temporary table may be needed. These are the things that you can usually see by running <code>EXPLAIN</code> on the query. The feature will now allow you to see the most important facts about the execution in the log file.</p> <pre><code># QC_Hit: No  Full_scan: Yes  Full_join: No  Tmp_table: No  Tmp_table_on_disk: No\n# Filesort: No  Filesort_on_disk: No  Merge_passes: 0\n</code></pre> <p>The values and their meanings are documented with the log_slow_filter option.</p>"},{"location":"diagnostics/slow_extended.html#innodb-usage-information","title":"InnoDB Usage Information","text":"<p>The final part of the output is the InnoDB usage statistics. MySQL currently shows many per-session statistics for operations with <code>SHOW SESSION STATUS</code>, but that does not include those of InnoDB, which are always global and shared by all threads. This feature lets you see those values for a given query.</p> <pre><code>#   InnoDB_IO_r_ops: 6415  InnoDB_IO_r_bytes: 105103360  InnoDB_IO_r_wait: 0.001279\n#   InnoDB_rec_lock_wait: 0.000000  InnoDB_queue_wait: 0.000000\n#   InnoDB_pages_distinct: 6430\n</code></pre> <p>Values:</p> <ul> <li> <p><code>innodb_IO_r_ops</code>: Counts the number of page read operations scheduled. The actual number of read operations may be different, but since this can be done asynchronously, there is no good way to measure it.</p> </li> <li> <p><code>innodb_IO_r_bytes</code>: Similar to innodb_IO_r_ops, but the unit is bytes.</p> </li> <li> <p><code>innodb_IO_r_wait</code>: Shows how long (in seconds) it took InnoDB to actually read the data from storage.</p> </li> <li> <p><code>innodb_rec_lock_wait</code>: Shows how long (in seconds) the query waited for row locks.</p> </li> <li> <p><code>innodb_queue_wait</code>: Shows how long (in seconds) the query spent either waiting to enter the InnoDB queue or inside that queue waiting for execution.</p> </li> <li> <p><code>innodb_pages_distinct</code>: Counts approximately the number of unique pages the query accessed. The approximation is based on a small hash array representing the entire buffer pool, because it could take a lot of memory to map all the pages. The inaccuracy grows with the number of pages accessed by a query, because there is a higher probability of hash collisions.</p> </li> </ul> <p>If the query did not use InnoDB tables, that information is written into the log instead of the above statistics.</p>"},{"location":"diagnostics/slow_extended.html#related-reading","title":"Related Reading","text":"<ul> <li> <p>Impact of logging on MySQL\u2019s performance</p> </li> <li> <p>log_slow_filter Usage</p> </li> <li> <p>Added microseconds to the slow query log event time</p> </li> </ul>"},{"location":"diagnostics/stacktrace.html","title":"Stacktrace","text":""},{"location":"diagnostics/stacktrace.html#stack-trace","title":"Stack Trace","text":"<p>Developers use the stack trace in the debug process, either an interactive investigation or during the post-mortem. No configuration is required to generate a stack trace.</p> <p>Implemented in Percona Server for MySQL 5.7.31-34, the stack trace adds the following:</p> Name Description Prints binary BuildID The Strip utility removes unneeded sections and debugging information to reduce the size. This method is standard with containers where the size of the image is essential. The BuildID lets you resolve the stack trace when the Strip utility removes the binary symbols table. Print the server version information The version information establishes the starting point for analysis. Some applications, such as MySQL, only print this information to a log on startup, and when the crash occurs, the size of the log may be large, rotated, or truncated."},{"location":"diagnostics/thread_based_profiling.html","title":"Thread Based Profiling","text":"<p>Percona Server for MySQL now uses thread based profiling by default, instead of process based profiling. This was implemented because with process based profiling, threads on the server, other than the one being profiled, can affect the profiling information.</p> <p>Thread based profiling is using the information provided by the kernel getrusage function. Since the 2.6.26 kernel version, thread based resource usage is available with the RUSAGE_THREAD. This means that the thread based profiling will be used if you\u2019re running the 2.6.26 kernel or newer, or if the RUSAGE_THREAD has been ported back.</p> <p>This feature is enabled by default if your system supports it, in other cases it uses process based profiling.</p>"},{"location":"diagnostics/thread_based_profiling.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"diagnostics/user_stats.html","title":"User Statistics","text":"<p>This feature adds several <code>INFORMATION_SCHEMA</code> tables, several commands, and the userstat variable. The tables and commands can be used to understand the server activity better and identify the source of the load.</p> <p>The functionality is disabled by default, and must be enabled by setting <code>userstat</code> to <code>ON</code>. It works by keeping several hash tables in memory. To avoid contention over global mutexes, each connection has its own local statistics, which are occasionally merged into the global statistics, and the local statistics are then reset to 0.</p>"},{"location":"diagnostics/user_stats.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6.</li> </ul>"},{"location":"diagnostics/user_stats.html#other-information","title":"Other Information","text":"<ul> <li>Author/Origin: Google; Percona added the <code>INFORMATION_SCHEMA</code> tables and the userstat variable.</li> </ul>"},{"location":"diagnostics/user_stats.html#system-variables","title":"System Variables","text":""},{"location":"diagnostics/user_stats.html#userstat","title":"<code>userstat</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type BOOLEAN Default OFF Range ON/OFF <p>Enables or disables collection of statistics. The default is <code>OFF</code>, meaning no statistics are gathered. This is to ensure that the statistics collection doesn\u2019t cause any extra load on the server unless desired.</p>"},{"location":"diagnostics/user_stats.html#thread_statistics","title":"<code>thread_statistics</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type BOOLEAN Default OFF Range ON/OFF <p>Enables or disables collection of thread statistics. The default is <code>OFF</code>, meaning no thread statistics are gathered. This is to ensure that the statistics collection doesn\u2019t cause any extra load on the server unless desired. Variable userstat needs to be enabled as well in order for thread statistics to be collected.</p>"},{"location":"diagnostics/user_stats.html#information_schema-tables","title":"INFORMATION_SCHEMA Tables","text":""},{"location":"diagnostics/user_stats.html#information_schemaclient_statistics","title":"<code>INFORMATION_SCHEMA.CLIENT_STATISTICS</code>","text":"Column Name Description \u2018CLIENT\u2019 \u2018The IP address or hostname from which the connection originated.\u2019 \u2018TOTAL_CONNECTIONS\u2019 \u2018The number of connections created for this client.\u2019 \u2018CONCURRENT_CONNECTIONS\u2019 \u2018The number of concurrent connections for this client.\u2019 \u2018CONNECTED_TIME\u2019 \u2018The cumulative number of seconds elapsed while there were connections from this client.\u2019 \u2018BUSY_TIME\u2019 \u2018The cumulative number of seconds there was activity on connections from this client.\u2019 \u2018CPU_TIME\u2019 \u2018The cumulative CPU time elapsed, in seconds, while servicing this client\u2019s connections.\u2019 \u2018BYTES_RECEIVED\u2019 \u2018The number of bytes received from this client\u2019s connections.\u2019 \u2018BYTES_SENT\u2019 \u2018The number of bytes sent to this client\u2019s connections.\u2019 \u2018BINLOG_BYTES_WRITTEN\u2019 \u2018The number of bytes written to the binary log from this client\u2019s connections.\u2019 \u2018ROWS_FETCHED\u2019 \u2018The number of rows fetched by this client\u2019s connections.\u2019 \u2018ROWS_UPDATED\u2019 \u2018The number of rows updated by this client\u2019s connections.\u2019 \u2018TABLE_ROWS_READ\u2019 \u2018The number of rows read from tables by this client\u2019s connections. (It may be different from ROWS_FETCHED.)\u2019 \u2018SELECT_COMMANDS\u2019 \u2018The number of SELECT commands executed from this client\u2019s connections.\u2019 \u2018UPDATE_COMMANDS\u2019 \u2018The number of UPDATE commands executed from this client\u2019s connections.\u2019 \u2018OTHER_COMMANDS\u2019 \u2018The number of other commands executed from this client\u2019s connections.\u2019 \u2018COMMIT_TRANSACTIONS\u2019 \u2018The number of COMMIT commands issued by this client\u2019s connections.\u2019 \u2018ROLLBACK_TRANSACTIONS\u2019 \u2018The number of ROLLBACK commands issued by this client\u2019s connections.\u2019 \u2018DENIED_CONNECTIONS\u2019 \u2018The number of connections denied to this client.\u2019 \u2018LOST_CONNECTIONS\u2019 \u2018The number of this client\u2019s connections that were terminated uncleanly.\u2019 \u2018ACCESS_DENIED\u2019 \u2018The number of times this client\u2019s connections issued commands that were denied.\u2019 \u2018EMPTY_QUERIES\u2019 \u2018The number of times this client\u2019s connections sent empty queries to the server.\u2019 <p>This table holds statistics about client connections. The Percona version of the feature restricts this table\u2019s visibility to users who have the <code>SUPER</code> or <code>PROCESS</code> privilege.</p> <p>Example:</p> <pre><code>mysql&gt; SELECT * FROM INFORMATION_SCHEMA.CLIENT_STATISTICS\\G\n</code></pre> <p>The output could be similar to the following:</p> <pre><code>*************************** 1. row ***************************\n                CLIENT: 10.1.12.30\n     TOTAL_CONNECTIONS: 20\nCONCURRENT_CONNECTIONS: 0\n        CONNECTED_TIME: 0\n             BUSY_TIME: 93\n              CPU_TIME: 48\n        BYTES_RECEIVED: 5031\n            BYTES_SENT: 276926\n  BINLOG_BYTES_WRITTEN: 217\n          ROWS_FETCHED: 81\n          ROWS_UPDATED: 0\n       TABLE_ROWS_READ: 52836023\n       SELECT_COMMANDS: 26\n       UPDATE_COMMANDS: 1\n        OTHER_COMMANDS: 145\n   COMMIT_TRANSACTIONS: 1\n ROLLBACK_TRANSACTIONS: 0\n    DENIED_CONNECTIONS: 0\n      LOST_CONNECTIONS: 0\n         ACCESS_DENIED: 0\n         EMPTY_QUERIES: 0\n</code></pre>"},{"location":"diagnostics/user_stats.html#information_schema-tables_1","title":"INFORMATION_SCHEMA Tables","text":""},{"location":"diagnostics/user_stats.html#information_schemaindex_statistics","title":"<code>INFORMATION_SCHEMA.INDEX_STATISTICS</code>","text":"Column Name Description \u2018TABLE_SCHEMA\u2019 \u2018The schema (database) name.\u2019 \u2018TABLE_NAME\u2019 \u2018The table name.\u2019 \u2018INDEX_NAME\u2019 \u2018The index name (as visible in SHOW CREATE TABLE).\u2019 \u2018ROWS_READ\u2019 \u2018The number of rows read from this index.\u2019 <p>This table shows statistics on index usage. An older version of the feature contained a single column that had the <code>TABLE_SCHEMA</code>, <code>TABLE_NAME</code> and <code>INDEX_NAME</code> columns concatenated together. The Percona version of the feature separates these into three columns. Users can see entries only for tables to which they have <code>SELECT</code> access.</p> <p>This table makes it possible to do many things that were difficult or impossible previously. For example, you can use it to find unused indexes and generate DROP commands to remove them.</p> <p>Example:</p> <pre><code>mysql&gt; SELECT * FROM INFORMATION_SCHEMA.INDEX_STATISTICS\nWHERE TABLE_NAME='tables_priv';\n</code></pre> <p>The output could be similar to the following:</p> <pre><code>+--------------+-----------------------+--------------------+-----------+\n| TABLE_SCHEMA | TABLE_NAME            | INDEX_NAME         | ROWS_READ |\n+--------------+-----------------------+--------------------+-----------+\n| mysql        | tables_priv           | PRIMARY            |         2 |\n+--------------+-----------------------+--------------------+-----------+\n</code></pre> <p>Note</p> <p>Current implementation of index statistics doesn\u2019t support partitioned tables.</p>"},{"location":"diagnostics/user_stats.html#information_schematable_statistics","title":"<code>INFORMATION_SCHEMA.TABLE_STATISTICS</code>","text":"Column Name Description \u2018TABLE_SCHEMA\u2019 \u2018The schema (database) name.\u2019 \u2018TABLE_NAME\u2019 \u2018The table name.\u2019 \u2018ROWS_READ\u2019 \u2018The number of rows read from the table.\u2019 \u2018ROWS_CHANGED\u2019 \u2018The number of rows changed in the table.\u2019 \u2018ROWS_CHANGED_X_INDEXES\u2019 \u2018The number of rows changed in the table, multiplied by the number of indexes changed.\u2019 <p>This table is similar in function to the <code>INDEX_STATISTICS</code> table.</p> <p>Example:</p> <pre><code>mysql&gt; SELECT * FROM INFORMATION_SCHEMA.TABLE_STATISTICS\nWHERE TABLE_NAME=``tables_priv``;\n</code></pre> <p>The output could be similar to the following:</p> <pre><code>+--------------+-------------------------------+-----------+--------------+------------------------+\n| TABLE_SCHEMA | TABLE_NAME                    | ROWS_READ | ROWS_CHANGED | ROWS_CHANGED_X_INDEXES |\n+--------------+-------------------------------+-----------+--------------+------------------------+\n| mysql        | tables_priv                   |         2 |            0 |                      0 |\n+--------------+-------------------------------+-----------+--------------+------------------------+\n</code></pre> <p>Note</p> <p>Current implementation of table statistics doesn\u2019t support partitioned tables.</p>"},{"location":"diagnostics/user_stats.html#information_schemathread_statistics","title":"<code>INFORMATION_SCHEMA.THREAD_STATISTICS</code>","text":"Column Name Description \u2018THREAD_ID\u2019 \u2018Thread ID\u2019 \u2018TOTAL_CONNECTIONS\u2019 \u2018The number of connections created from this thread.\u2019 \u2018CONCURRENT_CONNECTIONS\u2019 \u2018Always zero, will be removed in a future version.\u2019 \u2018CONNECTED_TIME\u2019 \u2018The cumulative number of seconds elapsed while there were connections from this thread.\u2019 \u2018BUSY_TIME\u2019 \u2018The cumulative number of seconds there was activity from this thread.\u2019 \u2018CPU_TIME\u2019 \u2018The cumulative CPU time elapsed while servicing this thread.\u2019 \u2018BYTES_RECEIVED\u2019 \u2018The number of bytes received from this thread.\u2019 \u2018BYTES_SENT\u2019 \u2018The number of bytes sent to this thread.\u2019 \u2018BINLOG_BYTES_WRITTEN\u2019 \u2018The number of bytes written to the binary log from this thread.\u2019 \u2018ROWS_FETCHED\u2019 \u2018The number of rows fetched by this thread.\u2019 \u2018ROWS_UPDATED\u2019 \u2018The number of rows updated by this thread.\u2019 \u2018TABLE_ROWS_READ\u2019 \u2018The number of rows read from tables by this tread.\u2019 \u2018SELECT_COMMANDS\u2019 \u2018The number of SELECT commands executed from this thread.\u2019 \u2018UPDATE_COMMANDS\u2019 \u2018The number of UPDATE commands executed from this thread.\u2019 \u2018OTHER_COMMANDS\u2019 \u2018The number of other commands executed from this thread.\u2019 \u2018COMMIT_TRANSACTIONS\u2019 \u2018The number of COMMIT commands issued by this thread.\u2019 \u2018ROLLBACK_TRANSACTIONS\u2019 \u2018The number of ROLLBACK commands issued by this thread.\u2019 \u2018DENIED_CONNECTIONS\u2019 \u2018The number of connections denied to this thread.\u2019 \u2018LOST_CONNECTIONS\u2019 \u2018The number of thread connections that were terminated uncleanly.\u2019 \u2018ACCESS_DENIED\u2019 \u2018The number of times this thread issued commands that were denied.\u2019 \u2018EMPTY_QUERIES\u2019 \u2018The number of times this thread sent empty queries to the server.\u2019 \u2018TOTAL_SSL_CONNECTIONS\u2019 \u2018The number of thread connections that used SSL.\u2019 <p>In order for this table to be populated with statistics, additional variable thread_statistics should be set to <code>ON</code>.</p>"},{"location":"diagnostics/user_stats.html#information_schemauser_statistics","title":"<code>INFORMATION_SCHEMA.USER_STATISTICS</code>","text":"Column Name Description \u2018USER\u2019 \u2018The username. The value #mysql_system_user# appears when there is no username (such as for the replica SQL thread).\u2019 \u2018TOTAL_CONNECTIONS\u2019 \u2018The number of connections created from this user.\u2019 \u2018CONCURRENT_CONNECTIONS\u2019 \u2018The number of concurrent connections for this user.\u2019 \u2018CONNECTED_TIME\u2019 \u2018The cumulative number of seconds elapsed while there were connections from this user.\u2019 \u2018BUSY_TIME\u2019 \u2018The cumulative number of seconds there was activity on connections from this user.\u2019 \u2018CPU_TIME\u2019 \u2018The cumulative CPU time elapsed, in seconds, while servicing this user\u2019s connections.\u2019 \u2018BYTES_RECEIVED\u2019 \u2018The number of bytes received from this user\u2019s connections.\u2019 \u2018BYTES_SENT\u2019 \u2018The number of bytes sent to this user\u2019s connections.\u2019 \u2018BINLOG_BYTES_WRITTEN\u2019 \u2018The number of bytes written to the binary log from this user\u2019s connections.\u2019 \u2018ROWS_FETCHED\u2019 \u2018The number of rows fetched by this user\u2019s connections.\u2019 \u2018ROWS_UPDATED\u2019 \u2018The number of rows updated by this user\u2019s connections.\u2019 \u2018TABLE_ROWS_READ\u2019 \u2018The number of rows read from tables by this user\u2019s connections. (It may be different from ROWS_FETCHED.)\u2019 \u2018SELECT_COMMANDS\u2019 \u2018The number of SELECT commands executed from this user\u2019s connections.\u2019 \u2018UPDATE_COMMANDS\u2019 \u2018The number of UPDATE commands executed from this user\u2019s connections.\u2019 \u2018OTHER_COMMANDS\u2019 \u2018The number of other commands executed from this user\u2019s connections.\u2019 \u2018COMMIT_TRANSACTIONS\u2019 \u2018The number of COMMIT commands issued by this user\u2019s connections.\u2019 \u2018ROLLBACK_TRANSACTIONS\u2019 \u2018The number of ROLLBACK commands issued by this user\u2019s connections.\u2019 \u2018DENIED_CONNECTIONS\u2019 \u2018The number of connections denied to this user.\u2019 \u2018LOST_CONNECTIONS\u2019 \u2018The number of this user\u2019s connections that were terminated uncleanly.\u2019 \u2018ACCESS_DENIED\u2019 \u2018The number of times this user\u2019s connections issued commands that were denied.\u2019 \u2018EMPTY_QUERIES\u2019 \u2018The number of times this user\u2019s connections sent empty queries to the server.\u2019 <p>This table contains information about user activity. The Percona version of the patch restricts this table\u2019s visibility to users who have the <code>SUPER</code> or <code>PROCESS</code> privilege.</p> <p>The table gives answers to questions such as which users cause the most load, and whether any users are being abusive. It also lets you measure how close to capacity the server may be. For example, you can use it to find out whether replication is likely to start falling behind.</p> <p>Example:</p> <pre><code>mysql&gt; SELECT * FROM INFORMATION_SCHEMA.USER_STATISTICS\\G\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>*************************** 1. row ***************************\n                  USER: root\n     TOTAL_CONNECTIONS: 5592\nCONCURRENT_CONNECTIONS: 0\n        CONNECTED_TIME: 6844\n             BUSY_TIME: 179\n              CPU_TIME: 72\n        BYTES_RECEIVED: 603344\n            BYTES_SENT: 15663832\n  BINLOG_BYTES_WRITTEN: 217\n          ROWS_FETCHED: 9793\n          ROWS_UPDATED: 0\n       TABLE_ROWS_READ: 52836023\n       SELECT_COMMANDS: 9701\n       UPDATE_COMMANDS: 1\n        OTHER_COMMANDS: 2614\n   COMMIT_TRANSACTIONS: 1\n ROLLBACK_TRANSACTIONS: 0\n    DENIED_CONNECTIONS: 0\n      LOST_CONNECTIONS: 0\n         ACCESS_DENIED: 0\n         EMPTY_QUERIES: 0\n</code></pre>"},{"location":"diagnostics/user_stats.html#commands-provided","title":"Commands Provided","text":"<ul> <li> <p><code>FLUSH CLIENT_STATISTICS</code></p> </li> <li> <p><code>FLUSH INDEX_STATISTICS</code></p> </li> <li> <p><code>FLUSH TABLE_STATISTICS</code></p> </li> <li> <p><code>FLUSH THREAD_STATISTICS</code></p> </li> <li> <p><code>FLUSH USER_STATISTICS</code></p> </li> </ul> <p>These commands discard the specified type of stored statistical information.</p> <ul> <li> <p><code>SHOW CLIENT_STATISTICS</code></p> </li> <li> <p><code>SHOW INDEX_STATISTICS</code></p> </li> <li> <p><code>SHOW TABLE_STATISTICS</code></p> </li> <li> <p><code>SHOW THREAD_STATISTICS</code></p> </li> <li> <p><code>SHOW USER_STATISTICS</code></p> </li> </ul> <p>These commands are another way to display the information you can get from the <code>INFORMATION_SCHEMA</code> tables. The commands accept <code>WHERE</code> clauses. They also accept but ignore <code>LIKE</code> clauses.</p>"},{"location":"diagnostics/user_stats.html#status-variables","title":"Status Variables","text":""},{"location":"diagnostics/user_stats.html#com_show_client_statistics","title":"<code>Com_show_client_statistics</code>","text":"Option Description Scope Global/Session Data type Numeric <p>The Com_show_client_statistics statement counter variable indicates the number of times the statement <code>SHOW CLIENT_STATISTICS</code> has been executed.</p>"},{"location":"diagnostics/user_stats.html#com_show_index_statistics","title":"<code>Com_show_index_statistics</code>","text":"Option Description Scope Global/Session Data type Numeric <p>The Com_show_index_statistics statement counter variable indicates the number of times the statement <code>SHOW INDEX_STATISTICS</code> has been executed.</p>"},{"location":"diagnostics/user_stats.html#com_show_table_statistics","title":"<code>Com_show_table_statistics</code>","text":"Option Description Scope Global/Session Data type Numeric <p>The Com_show_table_statistics statement counter variable indicates the number of times the statement <code>SHOW TABLE_STATISTICS</code> has been executed.</p>"},{"location":"diagnostics/user_stats.html#com_show_thread_statistics","title":"<code>Com_show_thread_statistics</code>","text":"Option Description Scope Global/Session Data type Numeric <p>The Com_show_thread_statistics statement counter variable indicates the number of times the statement <code>SHOW THREAD_STATISTICS</code> has been executed.</p>"},{"location":"diagnostics/user_stats.html#com_show_user_statistics","title":"<code>Com_show_user_statistics</code>","text":"Option Description Scope Global/Session Data type Numeric <p>The Com_show_user_statistics statement counter variable indicates the number of times the statement <code>SHOW USER_STATISTICS</code> has been executed.</p>"},{"location":"flexibility/binlogging_replication_improvements.html","title":"Binlogging and replication improvements","text":"<p>Due to continuous development, Percona Server for MySQL incorporated a number of improvements related to replication and binary logs handling. This resulted in replication specifics, which distinguishes it from MySQL.</p>"},{"location":"flexibility/binlogging_replication_improvements.html#temporary-tables-and-mixed-logging-format","title":"Temporary tables and mixed logging format","text":""},{"location":"flexibility/binlogging_replication_improvements.html#summary-of-the-fix","title":"Summary of the fix","text":"<p>As soon as some statement involving a temporary table was met when using the MIXED binlog format, MySQL was switching to the row-based logging of all statements till the end of the session or until all temporary tables used in this session were dropped. It is inconvenient in the case of long lasting connections, including replication-related ones. Percona Server for MySQL fixes the situation by switching between statement-based and row-based logging as necessary.</p>"},{"location":"flexibility/binlogging_replication_improvements.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Fix ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"flexibility/binlogging_replication_improvements.html#details","title":"Details","text":"<p>The mixed binary logging format supported by Percona Server for MySQL means that server runs in statement-based logging by default, but switches to row-based logging when replication would be unpredictable - in the case of a nondeterministic SQL statement that may cause data divergence if reproduced on a replica server. The switch is done upon any condition from the long list, and one of these conditions is the use of temporary tables.</p> <p>Temporary tables are never logged using row-based format, but any statement, that touches a temporary table, is logged in row mode. This way all the side effects that temporary tables may produce on non-temporary ones are intercepted.</p> <p>There is no need to use row logging format for any other statements solely because of the temp table presence. However MySQL was undertaking such an excessive precaution: once some statement with temporary table had appeared and the row-based logging was used, MySQL logged unconditionally all subsequent statements in row format.</p> <p>Percona Server have implemented more accurate behavior: instead of switching to row-based logging until the last temporary table is closed, the usual rules of row vs statement format apply, and presence of currently opened temporary tables is no longer considered. This change was introduced with the fix of a bug #151 (upstream #72475).</p>"},{"location":"flexibility/binlogging_replication_improvements.html#temporary-table-drops-and-binloging-on-gtid-enabled-server","title":"Temporary table drops and binloging on GTID-enabled server","text":""},{"location":"flexibility/binlogging_replication_improvements.html#summary-of-the-fix_1","title":"Summary of the fix","text":"<p>MySQL logs DROP statements for all temporary tables irrelative of the logging mode under which these tables were created. This produces binlog writes and errand GTIDs on replicas with row and mixed logging. Percona Server for MySQL fixes this by tracking the binlog format at temporary table create time and using it to decide whether a DROP should be logged or not.</p>"},{"location":"flexibility/binlogging_replication_improvements.html#version-specific-information_1","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.17-11: Fix ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"flexibility/binlogging_replication_improvements.html#details_1","title":"Details","text":"<p>Even with <code>read_only</code> mode enabled, the server permits some operations, including ones with temporary tables. With the previous fix, temporary table operations are not binlogged in row or mixed mode. But MySQL doesn\u2019t track what was the logging mode when temporary table was created, and therefore unconditionally logs <code>DROP</code> statements for all temporary tables. These <code>DROP</code> statements receive <code>IF EXISTS</code> addition, which is intended to make them harmless.</p> <p>Percona Server for MySQL have fixed this with the bug fixes #964, upstream #83003, and upstream #85258. Moreover, after all the binlogging fixes discussed so far nothing involving temporary tables is logged to binary log in row or mixed format, and so there is no need to consider <code>CREATE/DROP TEMPORARY TABLE</code> unsafe for use in stored functions, triggers, and multi-statement transactions in row/mixed format. Therefore an additional fix was introduced to mark creation and drop of temporary tables as unsafe inside transactions in statement-based replication only (bug fixed #1816, upstream #89467)).</p>"},{"location":"flexibility/binlogging_replication_improvements.html#safety-of-statements-with-a-limit-clause","title":"Safety of statements with a <code>LIMIT</code> clause","text":""},{"location":"flexibility/binlogging_replication_improvements.html#summary-of-the-fix_2","title":"Summary of the fix","text":"<p>MySQL considers all <code>UPDATE/DELETE/INSERT ... SELECT</code> statements with <code>LIMIT</code> clause to be unsafe, no matter wether they are really producing non-deterministic result or not, and switches from statement-based logging to row-based one. Percona Server for MySQL is more accurate, it acknowledges such instructions as safe when they include <code>ORDER BY PK</code> or <code>WHERE</code> condition. This fix has been ported from the upstream bug report #42415 (#44).</p>"},{"location":"flexibility/binlogging_replication_improvements.html#version-specific-information_2","title":"Version Specific Information","text":"<ul> <li>5.7.10.1: Fix ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"flexibility/binlogging_replication_improvements.html#performance-improvement-on-relay-log-position-update","title":"Performance improvement on relay log position update","text":""},{"location":"flexibility/binlogging_replication_improvements.html#summary-of-the-fix_3","title":"Summary of the fix","text":"<p>MySQL always updated relay log position in multi-source replications setups regardless of whether the committed transaction has already been executed or not. Percona Server omitts relay log position updates for the already logged GTIDs.</p>"},{"location":"flexibility/binlogging_replication_improvements.html#version-specific-information_3","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.18-14: Fix implemented in Percona Server for MySQL 5.7</li> </ul>"},{"location":"flexibility/binlogging_replication_improvements.html#details_2","title":"Details","text":"<p>Particularly, such unconditional relay log position updates caused additional fsync operations in case of <code>relay-log-info-repository=TABLE</code>, and with the higher number of channels transmitting such duplicate (already executed) transactions the situation became proportionally worse. Bug fixed #1786 (upstream #85141).</p>"},{"location":"flexibility/binlogging_replication_improvements.html#performance-improvement-on-source-and-connection-status-updates","title":"Performance improvement on source and connection status updates","text":""},{"location":"flexibility/binlogging_replication_improvements.html#summary-of-the-fix_4","title":"Summary of the fix","text":"<p>Replica nodes configured to update source status and connection information only on log file rotation did not experience the expected reduction in load. MySQL was additionaly updating this information in case of multi-source replication when replica had to skip the already executed GTID event.</p>"},{"location":"flexibility/binlogging_replication_improvements.html#version-specific-information_4","title":"Version Specific Information","text":"<ul> <li>Percona Server 5.7.20-19: Fix implemented in Percona Server for MySQL 5.7</li> </ul>"},{"location":"flexibility/binlogging_replication_improvements.html#details_3","title":"Details","text":"<p>The configuration with <code>master_info_repository=TABLE</code> and <code>sync_master_info=0</code> makes replica to update source status and connection information in this table on log file rotation and not after each sync_master_info event, but it didn\u2019t work on multi-source replication setups. Heartbeats sent to the replica to skip GTID events which it had already executed previously, were evaluated as relay log rotation events and reacted with <code>mysql.slave_master_info</code> table sync. This inaccuracy could produce huge (up to 5 times on some setups) increase in write load on the replica, before this problem was fixed in Percona Server for MySQL. Bug fixed #1812 (upstream #85158).</p>"},{"location":"flexibility/binlogging_replication_improvements.html#writing-flush-commands-to-the-binary-log","title":"Writing <code>FLUSH</code> Commands to the Binary Log","text":"<p><code>FLUSH</code> commands, such as <code>FLUSH SLOW LOGS</code>, are not written to the binary log if the system variable binlog_skip_flush_commands is set to ON.</p> <p>In addition, the following changes were implemented in the behavior of <code>read_only</code> and super_read_only modes:</p> <ul> <li> <p>When <code>read_only</code> is set to ON, any <code>FLUSH ...</code> command executed by a normal user (without the <code>SUPER</code> privilege) are not written to the binary log regardless of the value of the binlog_skip_flush_commands variable.</p> </li> <li> <p>When super_read_only is set to ON, any <code>FLUSH ...</code> command executed by any user (even by those with the <code>SUPER</code> privilege) are not written to the binary log regardless of the value of the binlog_skip_flush_commands variable.</p> </li> </ul> <p>An attempt to run a <code>FLUSH</code> command without either <code>SUPER</code> or <code>RELOAD</code> privileges results in the <code>ER_SPECIFIC_ACCESS_DENIED_ERROR</code> exception regardless of the value of the binlog_skip_flush_commands variable.</p>"},{"location":"flexibility/binlogging_replication_improvements.html#binlog_skip_flush_commands","title":"<code>binlog_skip_flush_commands</code>","text":"<p>Introduced in 5.6.43-84.3.</p> Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Default OFF <p>When binlog_skip_flush_command is set to ON, <code>FLUSH ...</code> commands are not written to the binary log. See Writing FLUSH Commands to the Binary Log for more information about what else affects the writing of <code>FLUSH</code> commands to the binary log.</p> <p>Note</p> <p><code>FLUSH LOGS</code>, <code>FLUSH BINARY LOGS</code>, <code>FLUSH TABLES WITH READ LOCK</code>, and <code>FLUSH TABLES ... FOR EXPORT</code> are not written to the binary log no matter what value the binlog_skip_flush_commands variable contains. The <code>FLUSH</code> command is not recorded to the binary log and the value of binlog_skip_flush_commands is ignored if the <code>FLUSH</code> command is run with the <code>NO_WRITE_TO_BINLOG</code> keyword (or its alias <code>LOCAL</code>).</p>"},{"location":"flexibility/binlogging_replication_improvements.html#limitations","title":"Limitations","text":"<p>Do not use one or more dot characters (.) when defining the values for the following variables:</p> <ul> <li> <p>log_bin</p> </li> <li> <p>log_bin_index</p> </li> </ul> <p>MySQL and XtraBackup handle the value in different ways and this difference causes unpredictable behavior.</p>"},{"location":"flexibility/compressed_columns.html","title":"Compressed columns with dictionaries","text":"<p>In Percona Server for MySQL 5.7.17-11 Percona Server for MySQL has been extended with a new per-column compression feature. It is a data type modifier, independent from user-level SQL and InnoDB data compression, that causes the data stored in the column to be compressed on writing to storage and decompressed on reading. For all other purposes, the data type is identical to the one without the modifier, i.e. no new data types are created. Compression is done by using the <code>zlib</code> library.</p> <p>Additionally, it is possible to pre-define a set of strings for each compressed column to achieve a better compression ratio on relatively small individual data items.</p> <p>This feature provides:</p> <ul> <li> <p>a better compression ratio for text data which consist of a large number of predefined words (e.g. JSON or XML) using compression methods with static dictionaries</p> </li> <li> <p>a way to select columns in the table to compress (in contrast to the InnoDB row compression method)</p> </li> </ul> <p>This feature is based on a patch provided by Weixiang Zhai.</p>"},{"location":"flexibility/compressed_columns.html#specifications","title":"Specifications","text":"<p>The feature is limited to InnoDB/XtraDB storage engine and to columns of the following data types:</p> <ul> <li> <p><code>BLOB</code> (including <code>TINYBLOB</code>, <code>MEDIUMBLOB</code>, <code>LONGBLOG</code>)</p> </li> <li> <p><code>TEXT</code> (including <code>TINYTEXT</code>, <code>MEDUUMTEXT</code>, <code>LONGTEXT</code>)</p> </li> <li> <p><code>VARCHAR</code> (including <code>NATIONAL VARCHAR</code>)</p> </li> <li> <p><code>VARBINARY</code></p> </li> <li> <p><code>JSON</code></p> </li> </ul> <p>The syntax to declare a compressed column is using an extension of an existing <code>COLUMN_FORMAT</code> modifier: <code>COLUMN_FORMAT COMPRESSED</code>. If this modifier is applied to an unsupported column type or storage engine, an error is returned.</p> <p>The compression can be specified:</p> <ul> <li> <p>when creating a table: <code>CREATE TABLE ... (..., foo BLOB COLUMN_FORMAT COMPRESSED, ...);</code></p> </li> <li> <p>when altering a table and modifying a column to the compressed format: <code>ALTER TABLE ... MODIFY [COLUMN] ... COLUMN_FORMAT COMPRESSED</code>, or <code>ALTER TABLE ... CHANGE [COLUMN] ... COLUMN_FORMAT COMPRESSED</code>.</p> </li> </ul> <p>Unlike Oracle MySQL, compression is applicable to generated stored columns. Use this syntax extension as follows:</p> <pre><code>mysql&gt; CREATE TABLE t1(\nid INT,\na BLOB,\nb JSON COLUMN_FORMAT COMPRESSED,\ng BLOB GENERATED ALWAYS AS (a) STORED COLUMN_FORMAT COMPRESSED WITH COMPRESSION_DICTIONARY numbers\n) ENGINE=InnoDB;\n</code></pre> <p>To decompress a column, specify a value other than <code>COMPRESSED</code> to <code>COLUMN_FORMAT</code>: <code>FIXED</code>, <code>DYNAMIC</code>, or <code>DEFAULT</code>. If there is a column compression/decompression request in an <code>ALTER TABLE</code>, it is forced to the <code>COPY</code> algorithm.</p> <p>Two new variables: innodb_compressed_columns_zip_level and innodb_compressed_columns_threshold have been implemented.</p>"},{"location":"flexibility/compressed_columns.html#compression-dictionary-support","title":"Compression dictionary support","text":"<p>To achieve a better compression ratio on relatively small individual data items, it is possible to pre-define a compression dictionary, which is a set of strings for each compressed column.</p> <p>Compression dictionaries can be represented as a list of words in the form of a string (comma or any other character can be used as a delimiter although not required). In other words, <code>a,bb,ccc</code>, <code>a bb ccc</code> and <code>abbccc</code> will have the same effect. However, the latter is more space-efficient. Quote symbol quoting is handled by regular SQL quoting. Maximum supported dictionary length is 32506 bytes (<code>zlib</code> limitation).</p> <p>The compression dictionary is stored in a new system InnoDB table. As this table is of the data dictionary kind, concurrent reads are allowed, but writes are serialized, and reads are blocked by writes. Table read through old read views are unsupported, similarly to InnoDB internal DDL transactions.</p>"},{"location":"flexibility/compressed_columns.html#example","title":"Example","text":"<p>In order to use the compression dictionary you need to create it. This can be done by running:</p> <pre><code>mysql&gt; SET @dictionary_data = 'one' 'two' 'three' 'four';\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <pre><code>mysql&gt; CREATE COMPRESSION_DICTIONARY numbers (@dictionary_data);\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <p>To create a table that has both compression and compressed dictionary support you should run:</p> <pre><code>mysql&gt; CREATE TABLE t1(\nid INT,\na BLOB COLUMN_FORMAT COMPRESSED,\nb BLOB COLUMN_FORMAT COMPRESSED WITH COMPRESSION_DICTIONARY numbers\n) ENGINE=InnoDB;\n</code></pre> <p>The following example shows how to insert a sample of JSON data into the table:</p> <pre><code>SET @json_value =\n'[\\n'\n' {\\n'\n' \"one\" = 0,\\n'\n' \"two\" = 0,\\n'\n' \"three\" = 0,\\n'\n' \"four\" = 0\\n'\n' },\\n'\n' {\\n'\n' \"one\" = 0,\\n'\n' \"two\" = 0,\\n'\n' \"three\" = 0,\\n'\n' \"four\" = 0\\n'\n' },\\n'\n' {\\n'\n' \"one\" = 0,\\n'\n' \"two\" = 0,\\n'\n' \"three\" = 0,\\n'\n' \"four\" = 0\\n'\n' },\\n'\n' {\\n'\n' \"one\" = 0,\\n'\n' \"two\" = 0,\\n'\n' \"three\" = 0,\\n'\n' \"four\" = 0\\n'\n' }\\n'\n']\\n'\n;\n</code></pre> <pre><code>mysql&gt; INSERT INTO t1 VALUES(0, @json_value, @json_value);\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Query OK, 1 row affected (0.01 sec)\n</code></pre>"},{"location":"flexibility/compressed_columns.html#information_schema-tables","title":"INFORMATION_SCHEMA Tables","text":"<p>This feature implemented two new <code>INFORMATION_SCHEMA</code> tables.</p>"},{"location":"flexibility/compressed_columns.html#information_schemaxtradb_zip_dict","title":"<code>INFORMATION_SCHEMA.XTRADB_ZIP_DICT</code>","text":"Column Name Description \u2018BIGINT(21)_UNSIGNED id\u2019 \u2018dictionary ID\u2019 \u2018VARCHAR(64) name\u2019 \u2018dictionary name\u2019 \u2018BLOB zip_dict\u2019 \u2018compression dictionary string\u2019 <p>This table provides a view over the internal compression dictionary table. <code>SUPER</code> privilege is required to query it.</p>"},{"location":"flexibility/compressed_columns.html#information_schemaxtradb_zip_dict_cols","title":"<code>INFORMATION_SCHEMA.XTRADB_ZIP_DICT_COLS</code>","text":"Column Name Description \u2018BIGINT(21)_UNSIGNED table_id\u2019 \u2018table ID from <code>INFORMATION_SCHEMA.INNODB_SYS_TABLES</code>\u2019 \u2018BIGINT(21)_UNSIGNED column_pos\u2019 \u2018column position (starts from <code>0</code> as in <code>INFORMATION_SCHEMA.INNODB_SYS_COLUMNS</code>)\u2019 \u2018BIGINT(21)_UNSIGNED dict_id\u2019 \u2018dictionary ID\u2019 <p>This table provides a view over the internal table that stores the mapping between the compression dictionaries and the columns using them. <code>SUPER</code> privilege is require to query it.</p>"},{"location":"flexibility/compressed_columns.html#limitations","title":"Limitations","text":"<p>Compressed columns cannot be used in indices (neither on their own nor as parts of composite keys).</p> <p>Note</p> <p><code>CREATE TABLE t2 AS SELECT \\* FROM t1</code> will create a new table with a compressed column, whereas <code>CREATE TABLE t2 AS SELECT CONCAT(a,'') AS a FROM t1</code> will not create compressed columns.</p> <p>At the same time, after executing <code>CREATE TABLE t2 LIKE t1</code> statement, <code>t2.a</code> will have <code>COMPRESSED</code> attribute.</p> <p><code>ALTER TABLE ... DISCARD/IMPORT TABLESPACE</code> is not supported for tables with compressed columns. To export and import tablespaces with compressed columns, you need to uncompress them first with: <code>ALTER TABLE ... MODIFY ... COLUMN_FORMAT DEFAULT</code>.</p>"},{"location":"flexibility/compressed_columns.html#mysqldump-command-line-parameters","title":"mysqldump command line parameters","text":"<p>By default, with no additional options, <code>mysqldump</code> will generate a MySQL compatible SQL output.</p> <p>All <code>/\\*!50633 COLUMN_FORMAT COMPRESSED \\*/</code> and <code>/\\*!50633 COLUMN_FORMAT COMPRESSED WITH COMPRESSION_DICTIONARY &lt;dictionary&gt; \\*/</code> won\u2019t be in the dump.</p> <p>When a new option <code>enable-compressed-columns</code> is specified, all <code>/\\*!50633 COLUMN_FORMAT COMPRESSED \\*/</code> will be left intact and all <code>/\\*!50633 COLUMN_FORMAT COMPRESSED WITH COMPRESSION_DICTIONARY &lt;dictionary&gt; \\*/</code> will be transformed into <code>/\\*!50633 COLUMN_FORMAT COMPRESSED \\*/</code>. In this mode the dump will contain the necessary SQL statements to create compressed columns, but without dictionaries.</p> <p>When a new <code>enable-compressed-columns-with-dictionaries</code> option is specified, dump will contain all compressed column attributes and compression dictionary.</p> <p>Moreover, the following dictionary creation fragments will be added before <code>CREATE TABLE</code> statements which are going to use these dictionaries for the first time.</p> <pre><code>/*!50633 DROP COMPRESSION_DICTIONARY IF EXISTS &lt;dictionary&gt;; */\n/*!50633 CREATE COMPRESSION_DICTIONARY &lt;dictionary&gt;(...); */\n</code></pre> <p>Two new options <code>add-drop-compression-dictionary</code> and <code>skip-add-drop-compression-dictionary</code> will control if <code>/\\*!50633 DROP COMPRESSION_DICTIONARY IF EXISTS &lt;dictionary&gt; \\*/</code> part from previous paragraph will be skipped or not. By default, <code>add-drop-compression-dictionary</code> mode will be used.</p> <p>When both <code>enable-compressed-columns-with-dictionaries</code> and <code>--tab=&lt;dir&gt;</code> (separate file for each table) options are specified, necessary compression dictionaries will be created in each output file using the following fragment (regardless of the values of <code>add-drop-compression-dictionary</code> and <code>skip-add-drop-compression-dictionary</code> options).</p> <pre><code>/*!50633 CREATE COMPRESSION_DICTIONARY IF NOT EXISTS &lt;dictionary&gt;(...); */\n</code></pre>"},{"location":"flexibility/compressed_columns.html#downgrade-scenario","title":"Downgrade scenario","text":"<p>If it is necessary to perform Percona Server for MySQL downgrade from a version Percona Server for MySQL 5.7.17-11 (or newer) to a version older than Percona Server for MySQL 5.7.17-11 and if user databases have one or more table with compressed columns, there are two options to do this safely:</p> <ul> <li> <p>Use <code>mysqldump</code> in compatible mode (no compressed columns extensions must be specified).</p> </li> <li> <p>Manually remove the <code>COMPRESSED</code> attribute from all columns which have it via <code>ALTER TABLE ... MODIFY ... COLUMN_FORMAT DEFAULT</code> before updating  server binaries. In this case, the downgraded server can start safely with old data files.</p> </li> </ul>"},{"location":"flexibility/compressed_columns.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.17-11: Feature implemented in Percona Server for MySQL 5.7</li> </ul>"},{"location":"flexibility/compressed_columns.html#system-variables","title":"System Variables","text":""},{"location":"flexibility/compressed_columns.html#innodb_compressed_columns_zip_level","title":"<code>innodb_compressed_columns_zip_level</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 6 Range <code>0</code>-<code>9</code> <p>This variable is used to specify the compression level used for compressed columns. Specifying <code>0</code> will use no compression, <code>1</code> the fastest and <code>9</code> the best compression. Default value is <code>6</code>.</p>"},{"location":"flexibility/compressed_columns.html#innodb_compressed_columns_threshold","title":"<code>innodb_compressed_columns_threshold</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 96 Range <code>1</code> - <code>2^64-1</code> (or <code>2^32-1</code> for 32-bit release) <p>By default a value being inserted will be compressed if its length exceeds innodb_compressed_columns_threshold bytes. Otherwise, it will be stored in raw (uncompressed) form.</p> <p>Please also notice that because of the nature of some data, its compressed representation can be longer than the original value. In this case it does not make sense to store such values in compressed form as Percona Server for MySQL would have to waste both memory space and CPU resources for unnecessary decompression. Therefore, even if the length of such non-compressible values exceeds innodb_compressed_columns_threshold, they will be stored in an uncompressed form (however, an attempt to compress them will still be made).</p> <p>This parameter can be tuned in order to skip unnecessary attempts of data compression for values that are known in advance by the user to have bad compression ratio of their first N bytes.</p>"},{"location":"flexibility/compressed_columns.html#other-reading","title":"Other reading","text":"<ul> <li>How to find a good/optimal dictionary for zlib \u2018setDictionary\u2019 when processing a given set of data?</li> </ul>"},{"location":"flexibility/csv_engine_mode.html","title":"CSV engine mode for standard-compliant quote and comma parsing","text":"<p>MySQL CSV Storage Engine is non-standard with respect to embedded <code>\"</code> and <code>,</code> character parsing. Fixing this issue unconditionally would break MySQL CSV format compatibility for any pre-existing user tables and for data exchange with other MySQL instances, but it would improve compatibility with other CSV producing/consuming tools.</p> <p>To keep both MySQL and other tool compatibility, a new dynamic, global/session server variable csv_mode has been implemented. This variable allows an empty value (the default), and <code>IETF_QUOTES</code>.</p> <p>If <code>IETF_QUOTES</code> is set, then embedded commas are accepted in quoted fields as-is, and a quote character is quoted by doubling it. In legacy mode embedded commas terminate the field, and quotes are quoted with a backslash.</p>"},{"location":"flexibility/csv_engine_mode.html#example","title":"Example","text":"<p>Table:</p> <pre><code>&gt; CREATE TABLE albums (\n`artist` text NOT NULL,\n`album` text NOT NULL\n) ENGINE=CSV DEFAULT CHARSET=utf8\n;\n</code></pre> <p>Following example shows the difference in parsing for default and <code>IETF_QUOTES</code> csv_quotes.</p> <pre><code>&gt; INSERT INTO albums VALUES (\"Great Artist\", \"Old Album\"),\n(\"Great Artist\", \"Old Album \\\"Limited Edition\\\"\");\n</code></pre> <p>If the variable csv_mode is set to empty value (default) parsed data will look like:</p> <pre><code>\"Great Artist\",\"Old Album\"\n\"Great Artist\",\"\\\"Limited Edition\\\",Old Album\"\n</code></pre> <p>If the variable csv_mode is set to <code>IETF_QUOTES</code> parsed data will look like as described in CSV rules:</p> <pre><code>\"Great Artist\",\"Old Album\"\n\"Great Artist\",\"\"\"Limited Edition\"\",Old Album\"\n</code></pre> <p>Parsing the CSV file which has the proper quotes (shown in the previous example) can show different results:</p> <p>With csv_mode set to empty value, parsed data will look like:</p> <p><pre><code>&gt; SELECT * FROM albums;\n</code></pre> The output could be similar to the following:</p> <pre><code>+--------------+--------------------+\n| artist       | album              |\n+--------------+--------------------+\n| Great Artist | Old Album          |\n| Great Artist | \"\"Limited Edition\" |\n+--------------+--------------------+\n2 rows in set (0.02 sec)\n</code></pre> <p>With csv_mode set to <code>IETF_QUOTES</code> parsed data will look like:</p> <p><pre><code>mysql&gt; SET csv_mode = 'IETF_QUOTES';\n</code></pre> The output could be similar to the following:</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <p><pre><code>&gt; SELECT * FROM albums;\n</code></pre> The output could be similar to the following:</p> <pre><code>+--------------+-----------------------------+\n| artist       | album                       |\n+--------------+-----------------------------+\n| Great Artist | Old Album                   |\n| Great Artist | \"Limited Edition\",Old Album |\n+--------------+-----------------------------+\n</code></pre>"},{"location":"flexibility/csv_engine_mode.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"flexibility/csv_engine_mode.html#system-variables","title":"System Variables","text":""},{"location":"flexibility/csv_engine_mode.html#csv_mode","title":"csv_mode","text":"Option Description Command-line Yes Config file Yes Scope Global, Session Dynamic Yes Data type SET Default <code>(empty string)</code> Range <code>(empty string)</code>, <code>IETF_QUOTES</code> <p>Setting this variable is to <code>IETF_QUOTES</code> will enable the standard-compliant quote parsing: commas are accepted in quoted fields as-is, and quoting of <code>\"</code> is changed from <code>\\\\\"</code> to <code>\"\"</code>. If the variable is set to empty value (the default), then the old parsing behavior is kept.</p>"},{"location":"flexibility/csv_engine_mode.html#related-reading","title":"Related Reading","text":"<ul> <li>MySQL bug #71091</li> </ul>"},{"location":"flexibility/extended_mysqlbinlog.html","title":"Extended mysqlbinlog","text":"<p>Percona Server for MySQL has implemented protocol compression support for the mysqlbinlog command.</p> <p>You can request protocol compression when connecting to a remote server to transfer binary log files. The protocol compression helps reduce the bandwidth use and improves the transfer speed.</p> <p>In the mysqlbinlog utility add either the <code>--compress</code> or <code>-C</code> flag to the command-line options.</p> <pre><code>mysqlbinlog [--compress|-C] --remote-server\n</code></pre>"},{"location":"flexibility/extended_mysqlbinlog.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"flexibility/extended_mysqldump.html","title":"Extended mysqldump","text":""},{"location":"flexibility/extended_mysqldump.html#backup-locks-support","title":"Backup Locks support","text":"<p>In Percona Server for MySQL Percona Server for MySQL 5.7.10-1 <code>mysqldump</code> has been extended with a new option, <code>lock-for-backup</code> (disabled by default). When used together with the <code>--single-transaction</code> option, the option makes <code>mysqldump</code> issue <code>LOCK TABLES FOR BACKUP</code> before starting the dump operation to prevent unsafe statements that would normally result in an inconsistent backup.</p> <p>More information can be found on the Backup Locks feature documentation.</p>"},{"location":"flexibility/extended_mysqldump.html#compressed-columns-support","title":"Compressed Columns support","text":"<p>In Percona Server for MySQL Percona Server for MySQL 5.7.17-11 mysqldump has been extended to support Compressed columns with dictionaries feature. More information about the new options can be found on the Compressed columns with dictionaries feature page.</p>"},{"location":"flexibility/extended_mysqldump.html#taking-backup-by-descending-primary-key-order","title":"Taking backup by descending primary key order","text":"<p>In Percona Server for MySQL 5.7.17-12 new <code>--order-by-primary-desc</code> has been implemented. This feature tells <code>mysqldump</code> to take the backup by descending primary key order (<code>PRIMARY KEY DESC</code>) which can be useful if storage engine is using reverse order column for a primary key.</p>"},{"location":"flexibility/extended_mysqldump.html#rocksdb-support","title":"RocksDB support","text":"<p>mysqldump will now detect when MyRocks is installed and available by seeing if there is a session variable named rocksdb_skip_fill_cache and setting it to <code>1</code> if it exists.</p> <p>mysqldump will now automatically enable session variable rocksdb_bulk_load if it is supported by target server.</p>"},{"location":"flexibility/extended_mysqldump.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li> <p>Percona Server for MySQL 5.7.10-1: mysqldump has been extended with Backup Locks support options</p> </li> <li> <p>Percona Server for MySQL 5.7.17-11: mysqldump has been extended with Compressed columns with dictionaries support options</p> </li> <li> <p>Percona Server for MySQL 5.7.17-12: mysqldump option <code>--order-by-primary-desc</code> introduced</p> </li> </ul>"},{"location":"flexibility/extended_select_into_outfile.html","title":"Extended SELECT INTO OUTFILE/DUMPFILE","text":"<p>Percona Server for MySQL has extended the <code>SELECT INTO ... OUTFILE</code> and <code>SELECT INTO DUMPFILE</code> commands to add the support for UNIX sockets and named pipes. Before this was implemented the database would return an error for such files.</p> <p>This feature allows using <code>LOAD DATA LOCAL INFILE</code> in combination with <code>SELECT INTO OUTFILE</code> to quickly load multiple partitions across the network or in other setups, without having to use an intermediate file which wastes space and I/O.</p>"},{"location":"flexibility/extended_select_into_outfile.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"flexibility/extended_select_into_outfile.html#other-reading","title":"Other Reading","text":"<ul> <li>MySQL bug: #44835</li> </ul>"},{"location":"flexibility/improved_memory_engine.html","title":"Improved MEMORY Storage Engine","text":"<p>As of <code>MySQL</code> 5.5.15, a Fixed Row Format (<code>FRF</code>) is still being used in the <code>MEMORY</code> storage engine. The fixed row format imposes restrictions on the type of columns as it assigns on advance a limited amount of memory per row. This renders a <code>VARCHAR</code> field in a <code>CHAR</code> field in practice and makes impossible to have a <code>TEXT</code> or <code>BLOB</code> field with that engine implementation.</p> <p>To overcome this limitation, the Improved MEMORY Storage Engine is introduced in this release for supporting true <code>VARCHAR</code>, <code>VARBINARY</code>, <code>TEXT</code> and <code>BLOB</code> fields in <code>MEMORY</code> tables.</p> <p>This implementation is based on the Dynamic Row Format (<code>DFR</code>) introduced by the mysql-heap-dynamic-rows patch.</p> <p><code>DFR</code> is used to store column values in a variable-length form, thus helping to decrease memory footprint of those columns and making possible <code>BLOB</code> and <code>TEXT</code> fields and real <code>VARCHAR</code> and <code>VARBINARY</code>.</p> <p>Unlike the fixed implementation, each column value in <code>DRF</code> only uses as much space as required. This is, for variable-length values, up to 4 bytes is used to store the actual value length, and then only the necessary number of blocks is used to store the value.</p> <p>Rows in <code>DFR</code> are represented internally by multiple memory blocks, which means that a single row can consist of multiple blocks organized into one set. Each row occupies at least one block, there can not be multiple rows within a single block. Block size can be configured when creating a table (see below).</p> <p>This <code>DFR</code> implementation has two caveats regarding to ordering and indexes.</p>"},{"location":"flexibility/improved_memory_engine.html#caveats","title":"Caveats","text":""},{"location":"flexibility/improved_memory_engine.html#ordering-of-rows","title":"Ordering of Rows","text":"<p>In the absence of <code>ORDER BY</code>, records may be returned in a different order than the previous <code>MEMORY</code> implementation.</p> <p>This is not a bug. Any application relying on a specific order without an <code>ORDER BY</code> clause may deliver unexpected results. A specific order without <code>ORDER BY</code> is a side effect of a storage engine and query optimizer implementation which may and will change between minor MySQL releases.</p>"},{"location":"flexibility/improved_memory_engine.html#indexing","title":"Indexing","text":"<p>It is currently impossible to use indexes on <code>BLOB</code> columns due to some limitations of the Dynamic Row Format. Trying to create such an index will fail with the following error:</p> <pre><code>BLOB column '&lt;name&gt;' can't be used in key specification with the used table type.\n</code></pre>"},{"location":"flexibility/improved_memory_engine.html#restrictions","title":"Restrictions","text":"<p>For performance reasons, a mixed solution is implemented: the fixed format is used at the beginning of the row, while the dynamic one is used for the rest of it.</p> <p>The size of the fixed-format portion of the record is chosen automatically on <code>CREATE TABLE</code> and cannot be changed later. This, in particular, means that no indexes can be created later with <code>CREATE INDEX</code> or <code>ALTER TABLE</code> when the dynamic row format is used.</p> <p>All values for columns used in indexes are stored in fixed format at the first block of the row, then the following columns are handled with <code>DRF</code>.</p> <p>This sets two restrictions to tables:</p> <ul> <li> <p>the order of the fields and therefore,</p> </li> <li> <p>the minimum size of the block used in the table.</p> </li> </ul>"},{"location":"flexibility/improved_memory_engine.html#ordering-of-columns","title":"Ordering of Columns","text":"<p>The columns used in fixed format must be defined before the dynamic ones in the <code>CREATE TABLE</code> statement. If this requirement is not met, the engine will not be able to add blocks to the set for these fields and they will be treated as fixed.</p>"},{"location":"flexibility/improved_memory_engine.html#minimum-block-size","title":"Minimum Block Size","text":"<p>The block size has to be big enough to store all fixed-length information in the first block. If not, the <code>CREATE TABLE</code> or <code>ALTER TABLE</code> statements will fail (see below).</p>"},{"location":"flexibility/improved_memory_engine.html#limitations","title":"Limitations","text":"<p>MyISAM tables are still used for query optimizer internal temporary tables where the <code>MEMORY</code> tables could be used now instead: for temporary tables containing large <code>VARCHAR\\</code>`s, ``BLOB<code>, and</code>TEXT` columns.</p>"},{"location":"flexibility/improved_memory_engine.html#setting-row-format","title":"Setting Row Format","text":"<p>Taking the restrictions into account, the Improved MEMORY Storage Engine will choose <code>DRF</code> over <code>FRF</code> at the moment of creating the table according to following criteria:</p> <ul> <li> <p>There is an implicit request of the user in the column types OR</p> </li> <li> <p>There is an explicit request of the user AND the overhead incurred by <code>DFR</code> is beneficial.</p> </li> </ul>"},{"location":"flexibility/improved_memory_engine.html#implicit-request","title":"Implicit Request","text":"<p>The implicit request by the user is taken when there is at least one <code>BLOB</code> or <code>TEXT</code> column in the table definition. If there are none of these columns and no relevant option is given, the engine will choose <code>FRF</code>.</p> <p>For example, this will yield the use of the dynamic format:</p> <pre><code>mysql&gt; CREATE TABLE t1 (f1 VARCHAR(32), f2 TEXT, PRIMARY KEY (f1)) ENGINE=HEAP;\n</code></pre> <p>While this will not:</p> <pre><code>mysql&gt; CREATE TABLE t1 (f1 VARCHAR(16), f2 VARCHAR(16), PRIMARY KEY (f1)) ENGINE=HEAP;\n</code></pre>"},{"location":"flexibility/improved_memory_engine.html#explicit-request","title":"Explicit Request","text":"<p>The explicit request is set with one of the following options in the <code>CREATE TABLE</code> statement:</p> <ul> <li> <p><code>KEY_BLOCK_SIZE = &lt;value&gt;</code></p> <ul> <li>Requests the DFR with the specified block size (in bytes)</li> </ul> </li> <li> <p><code>ROW_FORMAT = DYNAMIC</code></p> <ul> <li>Requests the dynamic format with the default block size (256 bytes)</li> </ul> </li> </ul> <p>Despite its name, the <code>KEY_BLOCK_SIZE</code> option refers to a block size used to store data rather then indexes. The reason for this is that an existing <code>CREATE TABLE</code> option is reused to avoid introducing new ones.</p> <p>The Improved MEMORY Engine checks whether the specified block size is large enough to keep all key column values. If it is too small, table creation will abort with an error.</p> <p>After <code>DRF</code> is requested explicitly and there are no <code>BLOB</code> or <code>TEXT</code> columns in the table definition, the Improved MEMORY Engine will check if using the dynamic format provides any space saving benefits as compared to the fixed one:</p> <ul> <li> <p>if the fixed row length is less than the dynamic block size (plus the dynamic row overhead - platform dependent) OR</p> </li> <li> <p>there isn\u2019t any variable-length columns in the table or <code>VARCHAR</code> fields are declared with length 31 or less, the engine will revert to the fixed format as it is more space efficient in such case. The row format being used by the engine can be checked using <code>SHOW TABLE STATUS</code>.</p> </li> </ul>"},{"location":"flexibility/improved_memory_engine.html#examples","title":"Examples","text":"<p>On a 32-bit platform:</p> <pre><code>mysql&gt; CREATE TABLE t1 (f1 VARCHAR(32), f2 VARCHAR(32), f3 VARCHAR(32), f4 VARCHAR(32),\nPRIMARY KEY (f1)) KEY_BLOCK_SIZE=124 ENGINE=HEAP ROW_FORMAT=DYNAMIC;\n\nmysql&gt; SHOW TABLE STATUS LIKE 't1';\n</code></pre> <p>The output should be simmilar to the following: </p> <pre><code>Name  Engine  Version Row_format      Rows    Avg_row_length  Data_length     Max_data_length Index_length    Data_free       Auto_increment  Create_time     Update_time     Check_time      Collation       Checksum        Create_options  Comment\nt1    MEMORY  10      Dynamic 0       X       0       X       0       0       NULL    NULL    NULL    NULL    latin1_swedish_ci       NULL    row_format=DYNAMIC KEY_BLOCK_SIZE=124\n</code></pre> <p>On a 64-bit platform:</p> <pre><code>mysql&gt; CREATE TABLE t1 (f1 VARCHAR(32), f2 VARCHAR(32), f3 VARCHAR(32), f4 VARCHAR(32),\nPRIMARY KEY (f1)) KEY_BLOCK_SIZE=124 ENGINE=HEAP ROW_FORMAT=DYNAMIC;\n\nmysql&gt; SHOW TABLE STATUS LIKE 't1';\n</code></pre> <p>The output should be simmilar to the following: </p> <pre><code>Name  Engine  Version Row_format      Rows    Avg_row_length  Data_length     Max_data_length Index_length    Data_free       Auto_increment  Create_time     Update_time     Check_time      Collation       Checksum        Create_options  Comment\nt1    MEMORY  10      Fixed   0       X       0       X       0       0       NULL    NULL    NULL    NULL    latin1_swedish_ci       NULL    row_format=DYNAMIC KEY_BLOCK_SIZE=124\n</code></pre>"},{"location":"flexibility/improved_memory_engine.html#implementation-details","title":"Implementation Details","text":"<p>MySQL MEMORY tables keep data in arrays of fixed-size chunks. These chunks are organized into two groups of <code>HP_BLOCK</code> structures:</p> <ul> <li> <p><code>group1</code> contains indexes, with one <code>HP_BLOCK</code> per key (part of <code>HP_KEYDEF</code>),</p> </li> <li> <p><code>group2</code> contains record data, with a single <code>HP_BLOCK</code> for all records.</p> </li> </ul> <p>While columns used in indexes are usually small, other columns in the table may need to accommodate larger data. Typically, larger data is placed into <code>VARCHAR</code> or <code>BLOB</code> columns.</p> <p>The Improved MEMORY Engine implements the concept of dataspace, <code>HP_DATASPACE</code>, which incorporates the <code>HP_BLOCK</code> structures for the record data, adding more information for managing variable-sized records.</p> <p>Variable-size records are stored in multiple \u201cchunks\u201d, which means that a single record of data (a database \u201crow\u201d) can consist of multiple chunks organized into one \u201cset\u201d, contained in <code>HP_BLOCK</code> structures.</p> <p>In variable-size format, one record is represented as one or many chunks depending on the actual data, while in fixed-size mode, one record is always represented as one chunk. The index structures would always point to the first chunk in the chunkset.</p> <p>Variable-size records are necessary only in the presence of variable-size columns. The Improved Memory Engine will be looking for <code>BLOB</code> or <code>VARCHAR</code> columns with a declared length of 32 or more. If no such columns are found, the table will be switched to the fixed-size format. You should always put such columns at the end of the table definition in order to use the variable-size format.</p> <p>Whenever data is being inserted or updated in the table, the Improved Memory Engine will calculate how many chunks are necessary.</p> <p>For <code>INSERT</code> operations, the engine only allocates new chunksets in the recordspace. For <code>UPDATE</code> operations it will modify the length of the existing chunkset if necessary, unlinking unnecessary chunks at the end, or allocating and adding more if a larger length is needed.</p> <p>When writing data to chunks or copying data back to a record, fixed-size columns are copied in their full format, while <code>VARCHAR</code> and <code>BLOB</code> columns are copied based on their actual length, skipping any <code>NULL</code> values.</p> <p>When allocating a new chunkset of N chunks, the engine will try to allocate chunks one-by-one, linking them as they become allocated. For allocating a single chunk, it will attempt to reuse a deleted (freed) chunk. If no free chunks are available, it will try to allocate a new area inside a <code>HP_BLOCK</code>.</p> <p>When freeing chunks, the engine will place them at the front of a free list in the dataspace, each one containing a reference to the previously freed chunk.</p> <p>The allocation and contents of the actual chunks varies between fixed and variable-size modes:</p> <ul> <li> <p>Format of a fixed-size chunk:</p> <ul> <li> <p><code>uchar[]</code></p> <ul> <li>With <code>sizeof=chunk_dataspace_length</code>, but at least <code>sizeof(uchar\\*)</code> bytes. It keeps actual data or pointer to the next deleted chunk, where <code>chunk_dataspace_length</code> equals to full record length</li> </ul> </li> <li> <p><code>uchar</code></p> <ul> <li>Status field (1 means \u201cin use\u201d, 0 means \u201cdeleted\u201d)</li> </ul> </li> </ul> </li> <li> <p>Format of a variable-size chunk:</p> </li> </ul> <pre><code>* `uchar[]`\n\n    * With `sizeof=chunk_dataspace_length`, but at least `sizeof(uchar\\*)` bytes. It keeps actual data or pointer to the next deleted chunk, where `chunk_dataspace_length` is set according to table\u2019s `key_block_size`\n</code></pre> <pre><code>* `uchar\\*`\n\n    * Pointer to the next chunk in this chunkset, or NULL for the last chunk\n\n* `uchar`\n\n    * Status field (1 means \u201cfirst\u201d, 0 means \u201cdeleted\u201d, 2 means \u201clinked\u201d)\n</code></pre> <p>Total chunk length is always aligned to the next <code>sizeof(uchar\\*)</code>.</p>"},{"location":"flexibility/improved_memory_engine.html#see-also","title":"See Also","text":"<ul> <li>Dynamic row format for MEMORY tables</li> </ul>"},{"location":"flexibility/innodb_fts_improvements.html","title":"InnoDB Full-Text Search improvements","text":""},{"location":"flexibility/innodb_fts_improvements.html#ignoring-stopword-list","title":"Ignoring Stopword list","text":"<p>By default all Full-Text Search indexes check the stopwords list, to see if any indexed elements contain one of the words on that list.</p> <p>Using this list for n-gram indexes isn\u2019t always suitable, as an example, any item that contains <code>a</code> or <code>i</code> will be ignored. Another word that can\u2019t be searched is <code>east</code>, this one will find no matches because <code>a</code> is on the FTS stopword list.</p> <p>To resolve this issue, in Percona Server for MySQL Percona Server 5.7.20-18 a new innodb_ft_ignore_stopwords variable has been implemented which controls whether InnoDB Full-Text Search should ignore stopword list.</p> <p>Although this variable is introduced to resolve n-gram issues, it affects all Full-Text Search indexes as well.</p> <p>Being a stopword doesn\u2019t just mean to be a one of the predefined words from the list. Tokens shorter than innodb_ft_min_token_size or longer than innodb_ft_max_token_size are also considered stopwords. Therefore, when innodb_ft_ignore_stopwords is set to <code>ON</code> even for non-ngram FTS, <code>innodb_ft_min_token_size</code> / <code>innodb_ft_max_token_size</code> will be ignored meaning that in this case very short and very long words will also be indexed.</p>"},{"location":"flexibility/innodb_fts_improvements.html#system-variables","title":"System Variables","text":""},{"location":"flexibility/innodb_fts_improvements.html#innodb_ft_ignore_stopwords","title":"<code>innodb_ft_ignore_stopwords</code>","text":"Option Description Command-line Yes Config file Yes Scope Global, Session Dynamic Yes Data type Boolean Default <code>OFF</code> <p>When enabled, this variable will instruct InnoDB Full Text Search parser to ignore the stopword list when building/updating an FTS index.</p>"},{"location":"flexibility/innodb_fts_improvements.html#punctuation-marks-in-full-text-search","title":"Punctuation Marks in Full-Text Search","text":"<p>By default, full text search is unable to find words with various punctuation characters in boolean search mode, although those characters are indexed with ngram parser. A new variable ft_query_extra_word_chars was introduced in Percona Server for MySQL Percona Server 5.7.21-20 to solve this issue.</p> <p>When it\u2019s enabled, all the non-whitespace symbols are considered to be word symbols by FTS query parser, except for the boolean search syntax symbols (which are specified by ft_boolean_syntax variable). The latter ones are also considered to be word symbols inside double quotes. This only applies for the query tokenizer, and the indexing tokenizer is not changed in any way. Because of this, the double quote symbol itself is never considered a word symbol, as no existing indexing tokenizer does so, thus searching for it would never return documents.</p>"},{"location":"flexibility/innodb_fts_improvements.html#system-variables_1","title":"System Variables","text":""},{"location":"flexibility/innodb_fts_improvements.html#ft_query_extra_word_chars","title":"<code>ft_query_extra_word_chars</code>","text":"Option Description Command-line Yes Config file Yes Scope Global, Session Dynamic Yes Data type Boolean Default <code>OFF</code> <p>When enabled, this variable will make all non-whitespace symbols (including punctuation marks) to be treated as word symbols in full-text search queries.</p>"},{"location":"flexibility/log_warnings_suppress.html","title":"Suppress Warning Messages","text":"<p>This feature is intended to provide a general mechanism (using <code>log_warnings_silence</code>) to disable certain warning messages to the log file. Currently, it is only implemented for disabling message #1592 warnings. This feature does not influence warnings delivered to a client. Please note that warning code needs to be a string:</p> <p><pre><code>mysql&gt; SET GLOBAL log_warnings_suppress = '1592';\n</code></pre> The output could be similar to the following:</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre>"},{"location":"flexibility/log_warnings_suppress.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li> <p>Percona Server for MySQL 5.7.10-1: Variable log_warnings_suppress ported from Percona Server for MySQL 5.6.</p> </li> <li> <p>Percona Server for MySQL 5.7.11-4: Feature has been removed from Percona Server for MySQL 5.7 because MySQL 5.7.11 has implemented a new system variable, log_statements_unsafe_for_binlog, which implements the same effect.</p> </li> </ul>"},{"location":"flexibility/log_warnings_suppress.html#system-variables","title":"System Variables","text":""},{"location":"flexibility/log_warnings_suppress.html#innodb_ft_ignore_stopwords","title":"<code>innodb_ft_ignore_stopwords</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type SET Default <code>(empty string)</code> Range <code>(empty string)</code>, <code>1592</code> <p>It is intended to provide a more general mechanism for disabling warnings than existed previously with variable suppress_log_warning_1592. When set to the empty string, no warnings are disabled. When set to <code>1592</code>, warning #1592 messages (unsafe statement for binary logging) are suppressed. In the future, the ability to optionally disable additional warnings may also be added.</p>"},{"location":"flexibility/log_warnings_suppress.html#related-reading","title":"Related Reading","text":"<ul> <li> <p>MySQL bug 42851</p> </li> <li> <p>MySQL InnoDB replication</p> </li> <li> <p>InnoDB Startup Options and System Variables</p> </li> <li> <p>InnoDB Error Handling</p> </li> </ul>"},{"location":"flexibility/max_binlog_files.html","title":"Restricting the number of binlog files","text":"<p>Maximum number of binlog files can now be restricted in Percona Server for MySQL with max_binlog_files. When variable max_binlog_files is set to non-zero value, the server will remove the oldest binlog file(s) whenever their number exceeds the value of the variable.</p> <p>This variable can be used with the existing max_binlog_size variable to limit the disk usage of the binlog files. If max_binlog_size is set to 1G and max_binlog_files to 20 this will limit the maximum size of the binlogs on disk to 20G. The actual size limit is not necessarily max_binlog_size * max_binlog_files. Server restart or <code>FLUSH LOGS</code> will make the server start a new log file and thus resulting in log files that are not fully written in these cases limit will be lower.</p>"},{"location":"flexibility/max_binlog_files.html#example","title":"Example","text":"<p>Number of the binlog files before setting this variable</p> <pre><code>$ ls -l mysql-bin.0* | wc -l\n</code></pre> <p>The output could be the following:</p> <pre><code>26\n</code></pre> <p>Variable max_binlog_files is set to 20:</p> <pre><code>max_binlog_files = 20\n</code></pre> <p>In order for new value to take effect <code>FLUSH LOGS</code> needs to be run. After that the number of binlog files is 20</p> <pre><code>$ ls -l mysql-bin.0* | wc -l\n</code></pre> <p>The output could be the following:</p> <pre><code>20\n</code></pre>"},{"location":"flexibility/max_binlog_files.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li> <p>Percona Server for MySQL 5.7.10-1: Variable max_binlog_files ported from Percona Server for MySQL 5.6.</p> </li> <li> <p>Percona Server 5.7.23-23: Variable max_binlog_files is deprecated and replaced with binlog_space_limit.</p> </li> </ul>"},{"location":"flexibility/max_binlog_files.html#system-variables","title":"System Variables","text":""},{"location":"flexibility/max_binlog_files.html#max_binlog_files","title":"max_binlog_files","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type ULONG Default 0 (unlimited) Range 0-102400"},{"location":"flexibility/max_binlog_files.html#binlog_space_limit","title":"binlog_space_limit","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type ULONG Default 0 (unlimited) Range 0-102400 <p>This option places an upper limit on the total size in bytes of all binary logs. A value of <code>0</code> means \u201cno limit\u201d. This is useful for a server host that has limited disk space.</p> <p>When the limit is reached, oldest binary logs are purged until the total size is under the limit or only active log is remaining.</p> <p>Note</p> <p>You should not set <code>--binlog-space-limit</code> to less or equal than the value of <code>--max-binlog-size</code> because after the max-binlog-size limit will be reached, logs will be rotated and immediately pruned by binlog-space-limit.</p>"},{"location":"flexibility/per_query_variable_statement.html","title":"Per-query variable statement","text":"<p>Percona Server for MySQL has implemented per-query variable statement support. This feature provides the ability to set variable values only for a certain query, after execution of which the previous values will be restored. Per-query variable values can be set up with the following command:</p> <pre><code>mysql&gt; SET STATEMENT &lt;variable=value&gt; FOR &lt;statement&gt;;\n</code></pre>"},{"location":"flexibility/per_query_variable_statement.html#examples","title":"Examples","text":"<p>If we want to increase the sort_buffer_size value just for one specific sort query we can do it like this:</p> <pre><code>mysql&gt; SET STATEMENT sort_buffer_size=100000 FOR SELECT name FROM name ORDER BY name;\n</code></pre> <p>This feature can also be used with max_execution_time to limit the execution time for a specific query:</p> <pre><code>mysql&gt; SET STATEMENT max_execution_time=1000 FOR SELECT name FROM name ORDER BY name;\n</code></pre> <p>We can provide more than one variable we want to set up:</p> <pre><code>mysql&gt; SET STATEMENT sort_buffer_size=100000, max_statement_time=1000 FOR SELECT name FROM name ORDER BY name;\n</code></pre>"},{"location":"flexibility/per_query_variable_statement.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"flexibility/per_query_variable_statement.html#other-reading","title":"Other Reading","text":"<ul> <li>WL#681: Per query variable settings</li> </ul>"},{"location":"flexibility/per_session_server-id.html","title":"Per-session server-id","text":"<p>Variable server_id is a global variable. In multi-source replication setups or for external replication, server_id can be useful as a session variable. In that case a session replaying a binary log from another server would set it to that server\u2019s id. That way binary log has the ultimate source server id attached to it no matter how many hosts it passes, and it would provide loop detection for multi-source replication.</p> <p>This was implemented by introducing the new pseudo_server_id variable. This variable, when set to non-zero value, will cause all binary log events in that session to have that server_id value. A new variable was introduced instead of converting server_id to have both global and session scope in order to preserve compatibility.</p> <p>You should use this option at your own risk because it is very easy to break replication when using pseudo_server_id. One special case is circular replication which definitely will be broken if you set pseudo_server_id to a value not assigned to any participating server (ie., setup is <code>1-&gt;2-&gt;3-&gt;4-&gt;1</code>, and pseudo_server_id is set to <code>5</code>). It is also possible to create a temporary table <code>foo</code>, then change pseudo_server_id and from now <code>foo</code> will not be visible by this session until pseudo_server_id gets restored.</p>"},{"location":"flexibility/per_session_server-id.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"flexibility/per_session_server-id.html#system-variables","title":"System Variables","text":""},{"location":"flexibility/per_session_server-id.html#pseudo_server_id","title":"<code>pseudo_server_id</code>","text":"Option Description Command-line Yes Config file No Scope Session Dynamic Yes Default 0 <p>When this variable is set to <code>0</code> (default), it will use the global server_id value. Note: this is different from the setting the global server_id to <code>0</code> which disables replication. Setting this variable to non-zero value will cause binary log events in that session to have it as server_id value. Setting this variable requires <code>SUPER</code> privileges.</p>"},{"location":"flexibility/per_session_server-id.html#other-reading","title":"Other Reading","text":"<ul> <li> <p>MDEV-500 -  Session variable for server_id</p> </li> <li> <p>Upstream bug #35125 -  allow the ability to set the server_id for a connection for logging to binary log</p> </li> </ul>"},{"location":"flexibility/proxy_protocol_support.html","title":"Support for PROXY protocol","text":"<p>The proxy protocol transports connection information in a safe way to an intermediate proxy server (for example, HAProxy) between a server and a client (i.e., mysql client, etc.). Since the proxy protocol is a way to spoof the client address, the proxy protocol is disabled by default. The protocol can be enabled on a per-host or a per-network basis for the trusted source addresses where trusted proxy servers are known to run.</p> <p>Unproxied connections are not allowed from these source addresses.</p> <p>Note</p> <p>You need to ensure proper firewall Access Control List (ACL) is in place when this feature is enabled.</p> <p>Proxying is supported for TCP over IPv4 and IPv6 connections only. UNIX socket connections can not be proxied and do not fall under the effect of proxy-protocol-networks=\u2019*\u2019.</p> <p>As a special exception, it is forbidden for the proxied IP address to be either <code>127.0.0.1</code> or <code>::1</code>.</p>"},{"location":"flexibility/proxy_protocol_support.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"flexibility/proxy_protocol_support.html#system-variables","title":"System Variables","text":""},{"location":"flexibility/proxy_protocol_support.html#proxy_protocol_networks","title":"<code>proxy_protocol_networks</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Default <code>(empty string)</code> <p>This variable is a global-only, read-only variable. The available values are:</p> <ul> <li> <p>Empty string, which is the default</p> </li> <li> <p>List of comma-separated IPv4 network and host addresses, or IPv6 network and host addresses. Network addresses are specified in CIDR notation, i.e. <code>192.168.0.0/24</code>.</p> </li> <li> <p>An <code>\\*</code> (asterisk) allows the proxy headers from any account. This setting is not recommended because this setting may compromise security.</p> </li> </ul> <p>To prevent source host spoofing, the setting of this variable must be as restrictive as possible to include only trusted proxy hosts.</p> <p>Note</p> <p>If the proxy_protocol_networks is set to a value that is not <code>\\*</code>, you must add <code>bind_address</code> with the MySQL server IP in my.cnf.</p> <p>If you set the proxy_protocol_networks to an IPv4-mapped address, the variable works without <code>bind_address</code>.</p>"},{"location":"flexibility/proxy_protocol_support.html#related-reading","title":"Related Reading","text":"<ul> <li>PROXY protocol specification</li> </ul>"},{"location":"flexibility/slowlog_rotation.html","title":"Slow Query Log Rotation and Expiration","text":"<p>Note</p> <p>This feature is currently considered BETA quality.</p> <p>Percona has implemented two new variables, max_slowlog_size and max_slowlog_files to provide users with ability to control the slow query log disk usage. These variables have the same behavior as upstream variable max_binlog_size and max_binlog_files variable used for controlling the binary log.</p> <p>Warning</p> <p>For this feature to work variable slow_query_log_file needs to be set up manually and without the <code>.log</code> sufix. The slow query log files will be named using slow_query_log_file as a stem, to which a dot and a sequence number will be appended.</p>"},{"location":"flexibility/slowlog_rotation.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"flexibility/slowlog_rotation.html#system-variables","title":"System Variables","text":""},{"location":"flexibility/slowlog_rotation.html#max_slowlog_size","title":"<code>max_slowlog_size</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type numeric Default 0 (unlimited) Range 4096 - 1073741824 <p>Slow query log will be rotated automatically when its size exceeds this value. The default is <code>0</code>, don\u2019t limit the size. When this feature is enabled slow query log file will be renamed to slow_query_log_file.000001.</p>"},{"location":"flexibility/slowlog_rotation.html#max_slowlog_files","title":"<code>max_slowlog_files</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type numeric Default 0 (unlimited) Range 0 - 102400 <p>Maximum number of slow query log files. Used with max_slowlog_size this can be used to limit the total amount of slow query log files. When this number is reached server will create a new slow query log file with increased sequence number. Log file with the lowest sequence number will be deleted.</p>"},{"location":"installation/apt_repo.html","title":"Installing Percona Server for MySQL 5.7 on Debian and Ubuntu","text":"<p>Note</p> <p>The following instructions install Percona Server for MySQL 5.7.  The instructions to install Percona Server for MySQL 8.0 are available at this location.</p> <p>Ready-to-use packages are available from the Percona Server for MySQL software repositories and the download page.</p> <p>Specific information on the supported platforms, products, and versions is described in Percona Software and Platform Lifecycle.</p>"},{"location":"installation/apt_repo.html#whats-in-each-deb-package","title":"What\u2019s in each DEB package?","text":"<p>The <code>percona-server-server-5.7</code> package contains the database server itself, the <code>mysqld</code> binary and associated files.</p> <p>The <code>percona-server-common-5.7</code> package contains files common to the server and client.</p> <p>The <code>percona-server-client-5.7</code> package contains the command line client.</p> <p>The <code>percona-server-5.7-dbg</code> package contains debug symbols for the server.</p> <p>The <code>percona-server-test-5.7</code> package contains the database test suite.</p> <p>The <code>percona-server-source-5.7</code> package contains the server source.</p> <p>The <code>libperconaserverclient20-dev</code> package contains header files needed to compile software to use the client library.</p> <p>The <code>libperconaserverclient20</code> package contains the client shared library. The <code>18.1</code> is a reference to the version of the shared library. The version is incremented when there is a ABI change that requires software using the client library to be recompiled or its source code modified.</p>"},{"location":"installation/apt_repo.html#installing-percona-server-for-mysql-from-percona-apt-repository","title":"Installing Percona Server for MySQL from Percona <code>apt</code> repository","text":"<ol> <li> <p>Update package repositories:</p> <pre><code>$ sudo apt update\n</code></pre> </li> <li> <p>Install <code>GnuPG</code>, the GNU Privacy Guard:</p> <pre><code>$ sudo apt install gnupg2\n</code></pre> </li> <li> <p>Fetch the repository packages from Percona web:</p> <pre><code>$ wget https://repo.percona.com/apt/percona-release_latest.$(lsb_release -sc)_all.deb\n</code></pre> </li> <li> <p>Install the downloaded package with dpkg. To do that, run the following commands as root or with sudo:</p> <pre><code>$ sudo dpkg -i percona-release_latest.$(lsb_release -sc)_all.deb\n</code></pre> <p>Once you install this package, the Percona repositories should be added. You can check the repository setup in the <code>/etc/apt/sources.list.d/percona-original-release.list</code> file.</p> </li> <li> <p>Remember to update the local cache:</p> <pre><code>$ sudo apt update\n</code></pre> <p>Once you install this package the Percona repositories should be added. You can check the repository setup in the <code>/etc/apt/sources.list.d/percona-release.list</code> file.</p> </li> <li> <p>After that you can install the server package:</p> <pre><code>$ sudo apt install percona-server-server-5.7\n</code></pre> <p>Note</p> <p>Percona Server for MySQL 5.7 comes with the TokuDB storage engine and MyRocks storage engine. These storage engines are installed as plugin.</p> </li> </ol> <p>For information on how to install and configure TokuDB, refer to the TokuDB Installation guide.</p> <p>For information on how to install and configure MyRocks, refer to the Percona MyRocks Installation Guide guide.</p> <p>The Percona Server for MySQL distribution contains several useful User Defined Functions (UDF) from Percona Toolkit. After the installation completes, run the following commands to create these functions:</p> <pre><code>$ mysql -e \"CREATE FUNCTION fnv1a_64 RETURNS INTEGER SONAME 'libfnv1a_udf.so'\"\n$ mysql -e \"CREATE FUNCTION fnv_64 RETURNS INTEGER SONAME 'libfnv_udf.so'\"\n$ mysql -e \"CREATE FUNCTION murmur_hash RETURNS INTEGER SONAME 'libmurmur_udf.so'\"\n</code></pre> <p>For more details on the UDFs, see Percona Toolkit UDFS.</p>"},{"location":"installation/apt_repo.html#percona-apt-testing-repository","title":"Percona <code>apt</code> Testing repository","text":"<p>Percona offers pre-release builds from the testing repository. To enable it, run percona-release with the <code>testing</code> argument. Run this command as root or by using the sudo command.</p> <pre><code>$ sudo percona-release enable original testing\n</code></pre>"},{"location":"installation/apt_repo.html#apt-pinning-the-packages","title":"Apt-Pinning the packages","text":"<p>In some cases, you might need to pin the selected packages to avoid upgrades from the distribution repositories. Create a new file <code>/etc/apt/preferences.d/00percona.pref</code> and add the following lines:</p> <pre><code>Package: *\nPin: release o=Percona Development Team\nPin-Priority: 1001\n</code></pre>"},{"location":"installation/apt_repo.html#installing-percona-server-for-mysql-using-downloaded-deb-packages","title":"Installing Percona Server for MySQL using downloaded deb packages","text":"<p>Download the packages of the desired series for your architecture from the download page. The easiest way is to download bundle which contains all the packages. Following example will download Percona Server for MySQL Percona Server for MySQL 5.7.10-3 release packages for Debian 8.0:</p> <pre><code>$ wget https://www.percona.com/downloads/Percona-Server-5.7/Percona-Server-5.7.10-3/binary/debian/jessie/x86_64/Percona-Server-5.7.10-3-r63dafaf-jessie-x86_64-bundle.tar\n</code></pre> <p>You should then unpack the bundle to get the packages:</p> <pre><code>$ tar xvf Percona-Server-5.7.10-3-r63dafaf-jessie-x86_64-bundle.tar\n</code></pre> <p>After you unpack the bundle you should see the following packages:</p> <pre><code>$ ls *.deb\n</code></pre> <p>The output could be this:</p> <pre><code>libperconaserverclient20-dev_5.7.10-3-1.jessie_amd64.deb\nlibperconaserverclient20_5.7.10-3-1.jessie_amd64.deb\npercona-server-5.7-dbg_5.7.10-3-1.jessie_amd64.deb\npercona-server-client-5.7_5.7.10-3-1.jessie_amd64.deb\npercona-server-common-5.7_5.7.10-3-1.jessie_amd64.deb\npercona-server-server-5.7_5.7.10-3-1.jessie_amd64.deb\npercona-server-source-5.7_5.7.10-3-1.jessie_amd64.deb\npercona-server-test-5.7_5.7.10-3-1.jessie_amd64.deb\npercona-server-tokudb-5.7_5.7.10-3-1.jessie_amd64.deb\n</code></pre> <p>Now you can install Percona Server for MySQL by running:</p> <pre><code>$ sudo dpkg -i *.deb\n</code></pre> <p>This will install all the packages from the bundle. Another option is to download/specify only the packages you need for running Percona Server for MySQL installation (<code>libperconaserverclient20_5.7.10-3-1.jessie_amd64.deb</code>, <code>percona-server-client-5.7_5.7.10-3-1.jessie_amd64.deb</code>, <code>percona-server-common-5.7_5.7.10-3-1.jessie_amd64.deb</code>, and <code>percona-server-server-5.7_5.7.10-3-1.jessie_amd64.deb</code>. Optionally you can install <code>percona-server-tokudb-5.7_5.7.10-3-1.jessie_amd64.deb</code> if you want TokuDB storage engine).</p> <p>Note</p> <p>Percona Server for MySQL 5.7 comes with the TokuDB storage engine. You can find more information on how to install and enable the TokuDB storage in the TokuDB Installation guide.</p> <p>Warning</p> <p>When installing packages manually like this, you\u2019ll need to make sure to resolve all the dependencies and install missing packages yourself. Following packages will need to be installed before you can manually install Percona Server: <code>mysql-common</code>, <code>libjemalloc1</code>, <code>libaio1</code> and <code>libmecab2</code></p>"},{"location":"installation/apt_repo.html#apparmor-settings","title":"AppArmor settings","text":"<p>AppArmor is a kernel-integrated system which controls how applications access the file system by creating application profiles. If the installation of MySQL adds an AppArmor profile, you can find the profile in the following locations:</p> <ul> <li> <p>/etc/apparmor.d/usr.sbin.mysqld</p> </li> <li> <p>/etc/apparmor.d/local/usr.sbin.mysqld</p> </li> </ul> <p>The <code>local</code> version contains only comments. Add any changes specific for the server to the <code>local</code> file.</p> <p>The <code>usr.sbin.mysqld</code> file has the following settings:</p> <pre><code>#include &lt;tunables/global&gt;\n\n/usr/sbin/mysqld {\n  ...\n  # Allow data dir access\n  /var/lib/mysql/ r,\n  /var/lib/mysql/** rwk,\n\n  # Allow data files dir access\n    /var/lib/mysql-files/ r,\n    /var/lib/mysql-files/** rwk,\n\n  # Allow keyring dir access\n    /var/lib/mysql-keyring/ r,\n    /var/lib/mysql-keyring/** rwk,\n\n  # Allow log file access\n    /var/log/mysql/ r,\n    /var/log/mysql/** rw,\n  ...\n}\n</code></pre> <p>The settings govern how the files are accessed. For example, the data file directory access gives read \u00ae access to a directory and read, write, and lock access (rwk) to all directories and files underneath <code>/mysql/</code>.</p> <p>You should download the apparmor-utils package when you are working with existing AppArmor profiles. The utilities allow you to edit a profile without stopping AppArmor or removing the profile.</p> <p>Before you edit a profile, change the profile to <code>complain</code> mode:</p> <pre><code>$ aa-complain /usr/sbin/mysqld\n</code></pre> <p>The output could be the following:</p> <pre><code>setting /usr/sbin/mysqld to complain mode\n</code></pre> <p>In complain mode, you can edit the profile to add settings because you have relocated the data directory: <code>/&lt;volume&gt;/dev/percona/data</code>:</p> <pre><code>    /&lt;volume&gt;/percona/data/ r,\n    /&lt;volume&gt;/percona/data/** rwk,\n\n\nYou may need to reload AppArmor or reload the specific AppArmor profile to apply the changes.\n</code></pre> <p>You can also modify the <code>/etc/apparmor.d/tunables/alias</code> file as follows:</p> <pre><code>$ alias /var/lib/mysql -/volume/percona/data/\n</code></pre> <p>To reload one profile, run the following command:</p> <pre><code>$ sudo apparmor_parser -r /etc/apparmor.d/usr.sbin.mysqld\n</code></pre> <p>Restart AppArmor with the following command:</p> <pre><code>$ sudo systemctl restart apparmor\n</code></pre> <p>You can also disable AppArmor, but this action is not recommended. For earlier Ubuntu systems, prior to 16.04, use the following command:</p> <pre><code>$ sudo systemctl stop apparmor\n$ sudo update-rc.d -f apparmor remove\n</code></pre> <p>For later Ubuntu systems, use the following:</p> <pre><code>$ sudo sudo systemctl stop apparmor\n$ sudo systemctl disable apparmor\n</code></pre> <p>The following table lists the default locations for files:</p> Files Location mysqld server /usr/sbin Configuration /etc/mysql/my.cnf Data directory /var/lib/mysql Logs /var/log/mysql <p>Note</p> <p>Debian and Ubuntu installation does not automatically create a special <code>debian-sys-maint</code> user which can be used by the control scripts to control the Percona Server for MySQL <code>mysqld</code> and <code>mysqld_safe</code> services like it was the case with previous Percona Server for MySQL versions. If you still require this user you must create the user manually.</p>"},{"location":"installation/apt_repo.html#running-percona-server-for-mysql","title":"Running Percona Server for MySQL","text":"<p>The following procedure runs the Percona Server for MySQL:</p> <ol> <li> <p>Starting the service</p> <p>Percona Server for MySQL starts automatically after installation unless the server encounters errors during the installation process. You can also manually start it by running the following command:</p> <pre><code>$ sudo service mysql start\n</code></pre> </li> <li> <p>Confirming the service is running</p> <p>You can verify the service status by running the following command:</p> <pre><code>$ service mysql status\n</code></pre> </li> <li> <p>Stopping the service</p> <p>You can stop the service by running the following command:</p> <pre><code>S sudo service mysql stop\n</code></pre> </li> <li> <p>Restarting the service</p> <p>You can restart the service by running the following command:</p> <pre><code>S sudo service mysql restart\n</code></pre> </li> </ol> <p>Note</p> <p>Debian 8.0 (jessie) and Ubuntu 16.04(Xenial) come with systemd as the default system and service manager so you can invoke all the above commands with <code>sytemctl</code> instead of <code>service</code>. Currently, both are supported.</p>"},{"location":"installation/apt_repo.html#uninstalling-percona-server-for-mysql","title":"Uninstalling Percona Server for MySQL","text":"<p>To uninstall Percona Server for MySQL, you must remove all of the installed packages.</p> <p>You have the following options:</p> <ul> <li> <p>Removing packages with apt remove leaves the configuration and data files.</p> </li> <li> <p>Removing the packages with apt purge removes all the packages with configuration files and data files (all the databases).</p> </li> </ul> <p>Depending on your needs, you can choose which command better suits you.</p> <ol> <li> <p>Stop the Percona Server for MySQL service</p> <pre><code>$ sudo service mysql stop\n</code></pre> </li> <li> <p>Remove the packages</p> <ul> <li>Remove the packages. This option does not delete the configuration or data files. If you do not require these files, you must delete each file manually.</li> </ul> <pre><code>$ sudo apt remove 'percona-server*'\n</code></pre> <ul> <li>Purge the packages. This option deletes packages, configuration, and data files. The option does not delete any configuration or data files stored in your home directory. You may need to delete some files manually.</li> </ul> <pre><code>$ sudo apt purge 'percona-server*'\n$ sudo apt autoremove -y\n$ sudo apt autoclean\n$ sudo rm -rf /etc/mysql\n</code></pre> </li> </ol> <p>Note</p> <p>In a regular expression, the <code>\\*</code> (asterisk) matches zero or more of the preceding item. The single quotes prevent the shell from misinterpreting the asterisk as a shell command.</p> <p>If you do not plan to upgrade, run the following commands to remove the data directory location:</p> <pre><code>$ rm -rf /var/lib/mysql\n$ rm -rf /var/log/mysql\n$ sudo apt purge percona-server*\n</code></pre>"},{"location":"installation/binary-tarball.html","title":"Installing Percona Server for MySQL 5.7 from a Binary Tarball","text":"<p>Note</p> <p>The following instructions install Percona Server for MySQL 5.7.  The instructions to install Percona Server for MySQL 8.0 are available at this location.</p> <p>In Percona Server for MySQL 5.7.31-34 and later, the multiple binary  tarballs are replaced with the following:</p> Type Name Description Full Percona-Server--Linux.x86_64.glibc2.12.tar.gz Contains binaries, libraries, test files, and debug symbols Minimal Percona-Server--Linux.x86_64.glibc2.12-minimal.tar.gz Contains binaries, and libraries but does not include test files, or debug symbols. <p>Select the Percona Server for MySQL 5.7 version number and type of tarball for your installation. Both binary tarballs support all distributions.</p> <p>In Percona Server for MySQL before 5.7.31-34, multiple tarballs are provided based on the OpenSSL library available in the distribution:</p> <ul> <li> <p>ssl100 - for Debian prior to 9 and Ubuntu prior to 14.04 versions (<code>libssl.so.1.0.0 =&gt; /usr/lib/x86_64-linux-gnu/libssl.so.1.0.0</code>);</p> </li> <li> <p>ssl101 - for CentOS 6 and CentOS 7 (<code>libssl.so.10 =&gt; /usr/lib64/libssl.so.10</code>);</p> </li> <li> <p>ssl102 - for Debian 9 and Ubuntu versions starting from 14.04 (<code>libssl.so.1.1 =&gt; /usr/lib/libssl.so.1.1</code>);</p> </li> <li> <p>ssl1:111 - for CentOS 8 and RedHat 8 (<code>libssl.so.1.1 =&gt; /usr/lib64/libssl.so.1.1.1b</code>);    </p> </li> </ul> <p>You can download the binary tarballs from the <code>Linux - Generic</code>  section on the download page.</p> <p>Fetch the correct binary tarball. The example fetches Percona Server  for MySQL 5.7.38-41 for Debian 10:</p> <pre><code>$ wget https://downloads.percona.com/downloads/Percona-Server-5.7/Percona-Server-5.7.38-41/binary/debian/buster/x86_64/Percona-Server-5.7.38-41-rda46e5474f9-buster-x86_64-bundle.tar\n</code></pre>"},{"location":"installation/compile.html","title":"Compiling Percona Server for MySQL 5.7 from Source","text":"<p>Note</p> <p>The following instructions compile Percona Server for MySQL 5.7.  The instructions on how to compile Percona Server for MySQL 8.0 are available at this location.</p> <p>After either fetching the source repository or extracting a source tarball (from Percona or one you generated yourself), you must configure and build Percona Server. Do the following:</p> <ol> <li> <p>Run <code>cmake</code> to configure the build. Specify the build options like you would for a MySQL build. You may require other options on your sever. </p> <p>This example configures Percona Server for MySQL with similar options to what Percona uses to produce the binaries:</p> <pre><code>$ cmake . -DCMAKE_BUILD_TYPE=RelWithDebInfo -DBUILD_CONFIG=mysql_release -DFEATURE_SET=community -DWITH_EMBEDDED_SERVER=OFF\n</code></pre> </li> <li> <p>compile using <code>make</code>:</p> <pre><code>$ make\n</code></pre> </li> <li> <p>install the compiled file:</p> <p><pre><code>$ make install\n</code></pre> Percona Server 5.7 is installed on your system.</p> </li> </ol>"},{"location":"installation/docker.html","title":"Running Percona Server for MySQL 5.7 in a Docker Container","text":"<p>Note</p> <p>The following instructions run Percona Server for MySQL 5.7 in a Docker container.  The instructions on how to run Percona Server for MySQL 8.0 in a Docker container are available at this location.</p> <p>Docker images of Percona Server are hosted publicly on Docker Hub at https://hub.docker.com/r/percona/percona-server/.</p> <p>For more information about using Docker, see the Docker Docs.</p> <p>Note</p> <p>Make sure that you are using the latest version of Docker. The ones provided via <code>apt</code> and <code>yum</code> may be outdated and cause errors.</p>"},{"location":"installation/docker.html#using-the-percona-server-images","title":"Using the Percona Server Images","text":"<p>The following procedure describes how to run and access Percona Server 5.7 using Docker.</p>"},{"location":"installation/docker.html#starting-a-percona-server-instance-in-a-container","title":"Starting a Percona Server Instance in a Container","text":"<p>Note</p> <p>By default, Docker pulls the image from Docker Hub if it is not available locally.</p> <p>To start a container named <code>ps</code> running the latest version in the Percona Server 5.7 series, with the root password set to <code>root</code>:</p> <pre><code>[root@docker-host] $ docker run -d \\\n  --name ps \\\n  -e MYSQL_ROOT_PASSWORD=root \\\n  percona/percona-server:5.7\n</code></pre> <p>Warning</p> <p><code>root</code> is not a secure password. The word is used in the example for illustrative purposes only. Do not use this example in production.</p> <p>Note</p> <p>The docker stop command sends a TERM signal. Docker waits 10 seconds and sends a KILL signal. A very large instance cannot dump the data from memory to disk in 10 seconds. If you plan to run a very large instance, add the following option to the docker run command.</p> <p><code>\u2013stop-timeout 600</code></p>"},{"location":"installation/docker.html#accessing-the-percona-server-container","title":"Accessing the Percona Server Container","text":"<p>To access the shell in the container:</p> <pre><code>[root@docker-host] $ docker exec -it ps /bin/bash\n</code></pre> <p>From the shell, you can view the error log:</p> <pre><code>[mysql@ps] $ more /var/log/mysql/error.log\n2017-08-29T04:20:22.190474Z 0 [Warning] 'NO_ZERO_DATE', 'NO_ZERO_IN_DATE' and 'ERROR_FOR_DIVISION_BY_ZERO' sql modes should be used with strict mode. They will be merged with strict mode in a future release.\n2017-08-29T04:20:22.190520Z 0 [Warning] 'NO_AUTO_CREATE_USER' sql mode was not set.\n...\n</code></pre> <p>You can also run the MySQL command-line client to access the database directly:</p> <pre><code>[mysql@ps] $ mysql -uroot -proot\n</code></pre> <p>The output may be similar to the following:</p> <pre><code>mysql: [Warning] Using a password on the command line interface can be insecure.\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 4\nServer version: 5.7.19-17 Percona Server (GPL), Release '17', Revision 'e19a6b7b73f'\n\nCopyright (c) 2009-2017 Percona LLC and/or its affiliates\nCopyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.\n\nOracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n\nmysql&gt;\n</code></pre>"},{"location":"installation/docker.html#accessing-percona-server-from-application-in-another-container","title":"Accessing Percona Server from Application in Another Container","text":"<p>The image exposes the standard MySQL port 3306, so container linking makes Percona Server instance available from other containers. To link a container running your application (in this case, from image named <code>app/image</code>) with the Percona Server container, run it with the following command:</p> <pre><code>[root@docker-host] $ docker run -d \\\n--name app \\\n--link ps \\\napp/image:latest\n</code></pre> <p>This application container will be able to access the Percona Server container via port 3306.</p>"},{"location":"installation/docker.html#environment-variables","title":"Environment Variables","text":"<p>When running a Docker container with Percona Server, you can adjust the configuration of the instance by passing one or more environment variables with the <code>docker run</code> command.</p> <p>Note</p> <p>These variables will not have any effect if you start the container with a data directory that already contains a database: any pre-existing database will always remain untouched on container startup.</p> <p>The variables are optional, except that you must specify at least one of the following:</p> <ul> <li> <p>MYSQL_ALLOW_EMPTY_PASSWORD: least secure, use only for testing.</p> </li> <li> <p>MYSQL_ROOT_PASSWORD: more secure, but setting the password on the command line is not recommended for sensitive production setups.</p> </li> <li> <p>MYSQL_RANDOM_ROOT_PASSWORD: most secure, recommended for production.</p> </li> </ul> <p>Note</p> <p>To further secure your instance, use the MYSQL_ONETIME_PASSWORD variable if you are running version 5.6 or later.</p>"},{"location":"installation/docker.html#storing-data","title":"Storing Data","text":"<p>There are two ways to store data used by applications that run in Docker containers:</p> <ul> <li> <p>Let Docker manage the storage of your data by writing the database files to disk on the host system using its own internal volume management.</p> </li> <li> <p>Create a data directory on the host system (outside the container on high performance storage) and mount it to a directory visible from inside the container. This places the database files in a known location on the host system, and makes it easy for tools and applications on the host system to access the files. The user should make sure that the directory exists, and that permissions and other security mechanisms on the host system are set up correctly.</p> </li> </ul> <p>For example, if you create a data directory on a suitable volume on your host system named <code>/local/datadir</code>, you run the container with the following command:</p> <pre><code>[root@docker-host] $ docker run -d \\\n--name ps \\\n-e MYSQL_ROOT_PASSWORD=root \\\n-v /local/datadir:/var/lib/mysql \\\npercona/percona-server:5.7\n</code></pre> <p>The <code>-v /local/datadir:/var/lib/mysql</code> option mounts the <code>/local/datadir</code> directory on the host to <code>/var/lib/mysql</code> in the container, which is the default data directory used by Percona Server.</p> <p>Note</p> <p>If you have the Percona Server container instance with a data directory that already contains data (the <code>mysql</code> subdirectory where all our system tables are stored), the MYSQL_ROOT_PASSWORD variable should be omitted from the <code>docker run</code> command.</p> <p>Note</p> <p>If you have SELinux enabled, assign the relevant policy type to the new data directory, so that the container will be allowed to access it:</p> <pre><code>[root@docker-host] $ chcon -Rt svirt_sandbox_file_t /local/datadir\n</code></pre>"},{"location":"installation/docker.html#port-forwarding","title":"Port Forwarding","text":"<p>Docker allows mapping ports on the container to ports on the host system using the <code>-p</code> option. If you run the container with this option, you can connect to the database by connecting your client to a port on the host machine. This can greatly simplify consolidating many instances to a single host.</p> <p>To map the standard MySQL port 3306 to port 6603 on the host:</p> <pre><code>[root@docker-host] $ docker run -d \\\n--name ps \\\n-e MYSQL_ROOT_PASSWORD=root \\\n-p 6603:3306 \\\npercona/percona-server:5.7\n</code></pre>"},{"location":"installation/docker.html#passing-options-to-percona-server","title":"Passing Options to Percona Server","text":"<p>You can pass options to Percona Server when running the container by appending them to the <code>docker run</code> command. For example, to start run Percona Server with UTF-8 as the default setting for character set and collation for all databases:</p> <pre><code>[root@docker-host] $ docker run -d \\\n--name ps \\\n-e MYSQL_ROOT_PASSWORD=root \\\npercona/percona-server:5.7 \\\n--character-set-server=utf8 \\\n--collation-server=utf8_general_ci\n</code></pre>"},{"location":"installation/git-source-tree.html","title":"Installing Percona Server for MySQL 5.7 from the Git Source Tree","text":"<p>Note</p> <p>The following instructions install Percona Server for MySQL 5.7 from the Git Source tree.  The instruction to install the Percona Server for MySQL 8.0 from a Git Source tree are available in this location.</p> <p>Percona uses the GitHub revision control system for development. To build the latest Percona Server for MySQL from the source tree you must have <code>git</code> installed on your system.</p> <p>You can now fetch the latest Percona Server for MySQL 5.7 sources.</p> <pre><code>$ git clone https://github.com/percona/percona-server.git\n$ cd percona-server\n$ git checkout 5.7\n$ git submodule init\n$ git submodule update\n</code></pre> <p>If you are going to be making changes to Percona Server for MySQL 5.7 and wanting to distribute the resulting work, generate a new source tarball, which  is process we follow for release:</p> <pre><code>$ cmake .\n$ make dist\n</code></pre> <p>Follow the instructions in Compiling Percona Server for MySQL from  Source.</p>"},{"location":"installation/post-installation.html","title":"Post-Installation steps for Percona Server for MySQL 5.7","text":"<p>After you have installed Percona Server for MySQL 5.7, you may need to do the following:</p> Task Description Initialize the data directory The source distribution or generic binary distribution installation does not automatically initialize the data directory Update the root password The CentOS/RedHat installations set up a temporary root password. Securing the Installation The mysql_secure_installation script improves the security of the installation. Checking the server status Verify the server returns information Configuring the Server to Start at Startup Common method to start the server automatically Testing the server Verify the server installation Populating the time zone tables The time zone tables are created but are not populated. Exclude Buffer Pool Pages from core files Reduce the size of the core files by excluding the buffer pool"},{"location":"installation/post-installation.html#initializing-the-data-directory","title":"Initializing the Data Directory","text":"<p>If you install the server using either the source distribution or generic binary distribution files, the data directory is not initialized, and you must run the initialization process after installation.</p> <p>Run mysqld with the \u2013initialize option or the initialize-insecure option.</p> <p>Executing mysqld with either option does the following:</p> <ul> <li> <p>Verifies the existence of the data directory</p> </li> <li> <p>Initializes the system tablespace and related structures</p> </li> <li> <p>Creates system tables including grant tables, time zone tables, and server-side help tables</p> </li> <li> <p>Creates <code>root@localhost</code></p> </li> </ul> <p>You should run the following steps with the <code>mysql</code> login.</p> <ol> <li> <p>Navigate to the MySQL directory. The example uses the default location.</p> <pre><code>$ cd /usr/local/mysql\n</code></pre> </li> <li> <p>Create a directory for the MySQL files. The secure_file_priv uses the directory path as a value.</p> <pre><code>$ mkdir mydata\n</code></pre> <p>The <code>mysql</code> user account should have the <code>drwxr-x---</code> permissions. Four sections define the permissions; file or directory, User, Group, and Others.</p> <p>The first character designates if the permissions are for a file or directory. The first character is <code>d</code> for a directory.</p> <p>The rest of the sections are specified in three-character sets.</p> Permission User Group Other Read Yes Yes No Write Yes No No Execute Yes Yes No </li> <li> <p>Run the command to initialize the data directory.</p> <pre><code>$ bin/mysqld --initialize\n</code></pre> </li> </ol> <p>The RedHat and derivative distributions set up a temporary password  when MySQL is installed. To  reset the password, you must start MySQL with the <code>--skip-grant-tables</code>  option and update the <code>user</code> table.</p> <p>The initial password is located using the following command:</p> <p><pre><code>$ grep 'temporary password' /var/log/mysqld.log\n</code></pre> Follow this procedure to reset the root password:</p> <ol> <li> <p>Stop MySQL.</p> <pre><code>$ sudo systemctl stop mysqld\n</code></pre> </li> <li> <p>Set <code>--skip-grant-tables</code> as an environment option. This method lets     you specify the option without modifying configuration files.</p> <pre><code>$ sudo systemctl set-environment MYSQLD_OPTS=\"--skip-grant-tables \n--skip-networking\"\n</code></pre> </li> <li> <p>Restart MySQL to make the option change effective.</p> <pre><code>$ sudo systemctl restart mysqld\n</code></pre> </li> <li> <p>Access MySQL as <code>root</code>.</p> <pre><code>$ mysql -u root\n</code></pre> </li> <li> <p>Change the root password.</p> <pre><code>mysql&gt; FLUSH PRIVILEGES;\nmysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY ('NewPassword');\nmysql&gt; FLUSH PRIVILEGES;\nmysql&gt; exit\n</code></pre> </li> <li> <p>Stop MySQL</p> <pre><code>$ sudo systemctl stop mysqld\n</code></pre> </li> <li> <p>Reset the environment options.</p> <pre><code>$ sudo systemctl unset-environment MYSQLD_OPTS\n</code></pre> </li> <li> <p>Start MySQL</p> <pre><code>$ sudo systemctl start mysqld\n</code></pre> </li> <li> <p>Log in to MySQL using the root password.</p> <pre><code>$ mysql -u root -p\n</code></pre> </li> </ol> <p>Note</p> <p>if you have trouble logging in after following the steps, repeat the procedure but, instead of using the <code>ALTER USER</code> statement, modify the <code>user</code> table.</p> <pre><code>mysql&gt; UPDATE mysql.user SET authentication_string=PASSWORD\n('NewPassword'), password_expired='N' WHERE User='root' AND Host='localhost';\nmysql&gt;FLUSH PRIVILEGES;\n</code></pre>"},{"location":"installation/post-installation.html#updating-the-root-password","title":"Updating the <code>root</code> password","text":""},{"location":"installation/post-installation.html#securing-the-installation","title":"Securing the Installation","text":"<p>The mysql_secure_installation script improves the security of the installation.</p> <p>Running the script does the following:</p> <ul> <li> <p>Changes the <code>root</code> password</p> </li> <li> <p>Disallows remote login for <code>root</code> accounts</p> </li> <li> <p>Removes anonymous users</p> </li> <li> <p>Removes the <code>test</code> database</p> </li> <li> <p>Reloads the privilege tables</p> </li> </ul> <p>The following statement runs the script:</p> <pre><code>$ mysql_secure_installation\n</code></pre>"},{"location":"installation/post-installation.html#checking-the-server-status","title":"Checking the server status","text":"<p>After a generic binary installation, the server starts. The following command checks the server status:</p> <pre><code>$ sudo service mysql status\n</code></pre> <p>Access the server with the following command:</p> <pre><code>$ mysql -u root -p\n</code></pre>"},{"location":"installation/post-installation.html#configuring-the-server-to-start-at-startup","title":"Configuring the Server to Start at Startup","text":"<p>You can manage the server with systemd. If you have installed the server from a generic binary distribution on an operating system that uses systemd, you can manually configure systemd support.</p> <p>The following commands start, check the status, and stop the server:</p> <pre><code>$ sudo systemctl start mysql\n$ sudo systemctl status mysql\n$ sudo systemctl stop mysql\n</code></pre> <p>Enabling the server to start at startup, run the following:</p> <pre><code>$ sudo systemctl enable mysql\n</code></pre>"},{"location":"installation/post-installation.html#testing-the-server","title":"Testing the Server","text":"<p>After you have initialized the data directory, and the server is started, you can run tests on the server.</p> <p>This section assumes you have used the default installation settings. If you have modified the installation, navigate to the installation location. You can also add the location by Setting the Environment Variables.</p> <p>You can use the mysqladmin client to access the server.</p> <p>If you have issues connecting to the server, you should use the <code>root</code> user and the root account password.</p> <p><pre><code>$ sudo mysqladmin -u root -p version\n</code></pre> The output should be similar to the following: <pre><code>Enter password:\n\nmysql Ver 8.0.19-10 for debian-linux-gnu on x86_64 (Percona Server (GPL), Release '10', Revision 'f446c04')\n...\nServer version      8.0.19-10\nProtocol version    10\nConnection          Localhost via UNIX socket\nUNIX socket         /var/run/mysqld/mysqld.sock\nUptime:             4 hours 58 min 10 section\n\nThreads:    2 Questions:    16 Slow queries: 0 Opens: 139 Flush tables: 3\nOpen tables: 59  Queries per second avg: 0.0000\n</code></pre></p> <p>Use mysqlshow to display database and table information.</p> <p><pre><code>$ sudo mysqlshow -u root -p\n</code></pre> The output should be similar to the following:</p> <pre><code>Enter password:\n\n+---------------------+\n|      Databases      |\n+=====================+\n| information_schema  |\n+---------------------+\n| mysql               |\n+---------------------+\n| performance_schema  |\n+---------------------+\n| sys                 |\n+---------------------+\n</code></pre>"},{"location":"installation/post-installation.html#populating-the-time-zone-tables","title":"Populating the time zone tables","text":"<p>The time zone system tables are the following:</p> <ul> <li> <p><code>time_zone</code></p> </li> <li> <p><code>time_zone_leap_second</code></p> </li> <li> <p><code>time_zone_name</code></p> </li> <li> <p><code>time_zone_transition</code></p> </li> <li> <p><code>time_zone_transition_type</code></p> </li> </ul> <p>If you install the server using either the source distribution or generic binary distribution files, the installation creates the time zone tables, but the tables are not populated.</p> <p>The mysql_tzinfo_to_sql program populates the tables from the <code>zoneinfo</code> directory data available in Linux.</p> <p>A common method to populate the tables is to add the zoneinfo directory path to <code>mysql_tzinfo_to_sql</code> and then send the output into <code>mysql</code>.</p> <p>The example assumes you are running the command with the <code>root</code> account. You must use an account with the privileges able to modify MySQL system tables.</p> <pre><code>$ mysql_tzinfo_to_sql /usr/share/zoneinfo | mysql -u root -p rootpassword\n</code></pre>"},{"location":"installation/post-installation.html#excluding-buffer-pool-pages-from-core-files","title":"Excluding Buffer Pool Pages from Core files","text":"<p>Implemented in Percona Server for MySQL 5.7.33-36, you can use the innodb_buffer_pool_in__core_file to reduce the size of the core file.</p> <p>Buffer pools can produce large core files because the buffer pool is located in main memory. If the main memory is dumped to a core file, the buffer pool increases the size of the dump.</p> <p>Having a large core file can have the following issues:</p> <ul> <li> <p>Requires more time to write</p> </li> <li> <p>Consume disk space</p> </li> <li> <p>Reading the file</p> </li> </ul> <p>To exclude the buffer pool, run the following command at startup or use a <code>SET</code> statement:</p> <pre><code>mysqld&gt; SET GLOBAL innodb_buffer_pool_in__core_file=OFF;\n</code></pre>"},{"location":"installation/source-tarball.html","title":"Installing Percona Server for MySQL 5.7 from a Source Tarball","text":"<p>Note</p> <p>The following instructions install Percona Server for MySQL 5.7 from a source tarball.  The instructions to install Percona Server for MySQL 8.0 from a source tarball are available at this location.</p> <p>Fetch and extract the source tarball from Percona Downloads. The  following example downloads and extracts Percona Server for MySQL 5.7. 38-41 on Ubuntu 22.04:</p> <pre><code>$ wget https://downloads.percona.com/downloads/Percona-Server-5.7/Percona-Server-5.7.38-41/binary/debian/focal/x86_64/Percona-Server-5.7.38-41-rda46e5474f9-focal-x86_64-bundle.tar\n</code></pre> <p>The output lists the downloaded file:</p> <pre><code>...\nSaving to: 'Percona-Server-5.7.38-41-rda46e5474f9-focal-x86_64-bundle.tar'\n...\n</code></pre> <p>Extract the tar file:</p> <p><pre><code>$ tar xvf Percona-Server-5.7.38-41-rda46e5474f9-focal-x86_64-bundle.tar\n</code></pre> The output lists the files:</p> <p><pre><code>libperconaserverclient20_5.7.38-41-1.focal_amd64.deb\nlibperconaserverclient20-dev_5.7.38-41-1.focal_amd64.deb\npercona-server-5.7-dbg_5.7.38-41-1.focal_amd64.deb\npercona-server-client-5.7_5.7.38-41-1.focal_amd64.deb\npercona-server-common-5.7_5.7.38-41-1.focal_amd64.deb\npercona-server-rocksdb-5.7_5.7.38-41-1.focal_amd64.deb\npercona-server-server-5.7_5.7.38-41-1.focal_amd64.deb\npercona-server-source-5.7_5.7.38-41-1.focal_amd64.deb\npercona-server-test-5.7_5.7.38-41-1.focal_amd64.deb\npercona-server-tokudb-5.7_5.7.38-41-1.focal_amd64.deb\n</code></pre> Follow the instructions in Compiling Percona Server for MySQL 5.7 from Source to complete the installation.</p>"},{"location":"installation/yum_repo.html","title":"Installing Percona Server for MySQL 5.7 on Red Hat Enterprise Linux and CentOS","text":"<p>Note</p> <p>The following instructions install Percona Server for MySQL 5.7 using the Yum repository.  The instructions to install Percona Server for MySQL 8.0 using the Yum repository are available at this location.</p> <p>Ready-to-use packages are available from the Percona Server for MySQL software repositories and the download page. The Percona yum repository supports popular RPM-based operating systems, including the Amazon Linux AMI.</p> <p>The easiest way to install the Percona Yum repository is to install an RPM that configures yum and installs the Percona GPG key.</p> <p>Specific information on the supported platforms, products, and versions are described in Percona Software and Platform Lifecycle.</p> <p>Note</p> <p>The RPM packages for Red Hat Enterprise Linux 7 (and compatible derivatives) do not support TLSv1.3, because TLSv1.3 requires OpenSSL 1.1.1, which is currently not available on this platform.</p>"},{"location":"installation/yum_repo.html#whats-in-each-rpm-package","title":"What\u2019s in each RPM package?","text":"<p>Each of the Percona Server for MySQL RPM packages have a particular purpose.</p> <ul> <li> <p>The <code>Percona-Server-server-57</code> package contains the server itself (the <code>mysqld</code> binary).</p> </li> <li> <p>The <code>Percona-Server-57-debuginfo</code> package contains debug symbols for the server.</p> </li> <li> <p>The <code>Percona-Server-client-57</code> package contains the command line client.</p> </li> <li> <p>The <code>Percona-Server-devel-57</code> package contains the header files needed to compile software using the client library.</p> </li> <li> <p>The <code>Percona-Server-shared-57</code> package includes the client shared library.</p> </li> <li> <p>The <code>Percona-Server-shared-compat</code> package includes shared libraries for software compiled against older versions of the client library. The following libraries are included in this package: <code>libmysqlclient.so.12</code>, <code>libmysqlclient.so.14</code>, <code>libmysqlclient.so.15</code>, <code>libmysqlclient.so.16</code>, and <code>libmysqlclient.so.18</code>. </p> <p>This package is not included in downloads for Red Hat Enterprise Linux 9 and derivatives.</p> </li> <li> <p>The <code>Percona-Server-test-57</code> package includes the test suite for Percona Server for MySQL.</p> </li> </ul>"},{"location":"installation/yum_repo.html#installing-from-the-percona-yum-repository","title":"Installing from the Percona YUM repository","text":"<p>Note</p> <p>RHEL 8 and other EL8 systems enable the MySQL module by default. This module hides the Percona-provided packages and the module must be disabled to make these packages visible. The following command disables the module:</p> <pre><code>$ sudo yum module disable mysql\n</code></pre> <ol> <li> <p>Install the Percona yum repository by running the following command as a <code>root</code> user or with <code>sudo</code>:</p> <pre><code>$ sudo yum install https://repo.percona.com/yum/percona-release-latest.noarch.rpm\n</code></pre> </li> <li> <p>Enable the Percona Server 5.7 repository:</p> <p><pre><code>$ sudo percona-release setup ps57\n</code></pre> The output should resemble the following:</p> <pre><code>* Disabling all Percona Repositories\n* Enabling the Percona Server 5.7 repository\n* Enabling the Percona XtraBackup 2.4 repository\n</code></pre> </li> <li> <p>Test the repository. Make sure packages are available from the repository by executing the <code>yum list</code> command. We filter the results by the version number:</p> <pre><code>$ yum list | grep 5.7.38-41.1\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>...\nPercona-Server-57-debuginfo.x86_64                     5.7.38-41.1.el8                                           percona-release-x86_64\nPercona-Server-57-debugsource.x86_64                   5.7.38-41.1.el8                                           percona-release-x86_64\nPercona-Server-client-57-debuginfo.x86_64              5.7.38-41.1.el8                                           percona-release-x86_64\nPercona-Server-rocksdb-57.x86_64                       5.7.38-41.1.el8                                           percona-release-x86_64\nPercona-Server-rocksdb-57-debuginfo.x86_64             5.7.38-41.1.el8                                           percona-release-x86_64\nPercona-Server-server-57-debuginfo.x86_64              5.7.38-41.1.el8                                           percona-release-x86_64\nPercona-Server-shared-57-debuginfo.x86_64              5.7.38-41.1.el8                                           percona-release-x86_64\nPercona-Server-shared-compat-57.x86_64                 5.7.38-41.1.el8                                           percona-release-x86_64\nPercona-Server-test-57-debuginfo.x86_64                5.7.38-41.1.el8                                           percona-release-x86_64\nPercona-Server-tokudb-57.x86_64                        5.7.38-41.1.el8                                           percona-release-x86_64\nPercona-Server-tokudb-57-debuginfo.x86_64              5.7.38-41.1.el8                                           percona-release-x86_64\n...\n</code></pre> </li> <li> <p>Install the packages. You can install Percona Server for MySQL by running the following command:</p> <pre><code>$ yum install Percona-Server-server-57\n</code></pre> </li> </ol> <p>Note</p> <p>Percona Server for MySQL 5.7 comes with the TokuDB storage engine. You can find more information on how to install and enable the TokuDB storage in the TokuDB Installation guide.</p>"},{"location":"installation/yum_repo.html#percona-yum-testing-repository","title":"Percona yum Testing repository","text":"<p>Percona offers pre-release builds from our testing repository. To subscribe to the testing repository, you\u2019ll need to enable the testing repository in <code>/etc/yum.repos.d/percona-release.repo</code>. To do so, set both <code>percona-testing-$basearch</code> and <code>percona-testing-noarch</code> to <code>enabled = 1</code> (Note that there are 3 sections in this file: release, testing and experimental - in this case it is the second section that requires updating). </p> <p>Note</p> <p>You must install the Percona repository first if this operation has not been done already. See installing from the Percona YUM repository</p>"},{"location":"installation/yum_repo.html#installing-percona-server-for-mysql-using-downloaded-rpm-packages","title":"Installing Percona Server for MySQL using downloaded rpm packages","text":"<ol> <li> <p>Download the packages of the desired series for your architecture from the download page. The easiest way is to download bundle which contains all the packages. The following example downloads the Percona Server for MySQL 5.7.31-34 release packages for CentOS 7:</p> <pre><code>$ wget https://www.percona.com/downloads/Percona-Server-5.\n7/Percona-Server-5.7.31-34/binary/redhat/7/x86_64/Percona-Server-5.7.31-34-r2e68637-el7-x86_64-bundle.tar\n</code></pre> </li> <li> <p>You should then unpack the bundle to get the packages:</p> <pre><code>$ tar xvf Percona-Server-5.7.31-34-r2e68637-el7-x86_64-bundle.tar\n</code></pre> <p>You should see the following packages:</p> <p><pre><code>$ ls *.rpm\n</code></pre> The output should be similar to the following:</p> <pre><code>Percona-Server-57-debuginfo-5.7.31-34.1.el7.x86_64.rpm\nPercona-Server-client-57-5.7.31-34.1.el7.x86_64.rpm\nPercona-Server-devel-57-5.7.31-34.1.el7.x86_64.rpm\nPercona-Server-rocksdb-57-5.7.31-34.1.el7.x86_64.rpm\nPercona-Server-server-57-5.7.31-34.1.el7.x86_64.rpm\nPercona-Server-shared-57-5.7.31-34.1.el7.x86_64.rpm\nPercona-Server-shared-compat-57-5.7.31-34.1.el7.x86_64.rpm\nPercona-Server-test-57-5.7.31-34.1.el7.x86_64.rpm\nPercona-Server-tokudb-57-5.7.31-34.1.el7.x86_64.rpm\n</code></pre> </li> <li> <p>Run the following command to Percona Server for     MySQL 5.7:</p> <pre><code>$ rpm -ivh Percona-Server-server-57-5.7.31-34.1.el7.x86_64.rpm \\\nPercona-Server-client-57-5.7.31-34.1.el7.x86_64.rpm \\\nPercona-Server-shared-57-5.7.31-34.1.el7.x86_64.rpm\n</code></pre> </li> </ol> <p>This command only installs the packages required to run the Percona Server for MySQL 5.7.</p> <p>Optionally, you can install either the TokuDB storage engine, adding <code>Percona-Server-tokudb-57-5.7.31-34.1.el7.x86_64.rpm</code>  or the MyRocks storage engine, adding <code>Percona-Server-rocksdb-57-5.7.31-34.1.el7.x86_64.rpm</code> to the install command.</p> <p>You can find more information on how to install and enable the TokuDB storage in the TokuDB Installation guide.</p> <p>You can find more information on how to install and enable the MyRocks storage engine in Percona MyRocks Installation.</p> <p>To install all the packages (for debugging, testing, etc.) run the  following command:</p> <pre><code>$ rpm -ivh *.rpm\n</code></pre> <p>Note</p> <p>When installing packages manually, you must resolve all dependencies and install any missing packages.</p> <p>The following table lists the default locations for files:</p> Files Location mysqld server /usr/bin Configuration /etc/my.cnf Data directory /var/lib/mysql Logs /var/log/mysqld.log <p>You can use the following command to locate the Data directory:</p> <p><pre><code>$ grep datadir /etc/my.cnf\n</code></pre> The output should resemble the following: <pre><code>datadir=/var/lib/mysql\n</code></pre></p>"},{"location":"installation/yum_repo.html#running-percona-server-for-mysql","title":"Running Percona Server for MySQL","text":"<p>Note</p> <p>RHEL 7 and CentOS 7 come with systemd as the default system and service manager so you can invoke all the above commands with <code>sytemctl</code> instead of <code>service</code>. Currently both are supported.</p> <ol> <li> <p>Start the service. Percona Server for MySQL does not start automatically on RHEL and CentOS after the installation. Start the server by running the following command:</p> <pre><code>$ service mysql start\n</code></pre> </li> <li> <p>Confirm that service is running by running the following command:</p> <pre><code>$ service mysql status\n</code></pre> </li> <li> <p>Stop the service by running the following command:</p> <pre><code>$ service mysql stop\n</code></pre> </li> <li> <p>Restart the service by running the following command:</p> <pre><code>$ service mysql restart\n</code></pre> </li> </ol> <p>Note</p> <p>The RHEL 8 distributions and derivatives have added system-wide cryptographic policies component. This component allows the configuration of cryptographic subsystems.</p>"},{"location":"installation/yum_repo.html#uninstalling-percona-server-for-mysql","title":"Uninstalling Percona Server for MySQL","text":"<p>To completely uninstall Percona Server for MySQL you must remove all the  installed packages and data files.</p> <ol> <li> <p>Stop the Percona Server for MySQL service</p> <pre><code>$ service mysql stop\n</code></pre> </li> <li> <p>Remove the packages</p> <pre><code>$ yum remove Percona-Server*\n</code></pre> </li> <li> <p>Remove the data and configuration files:</p> </li> </ol> <p>Warning</p> <p>This command removes all the packages and deletes all the data files (databases, tables, logs, etc.). Take a backup in case you need the data.</p> <pre><code>$ rm -rf /var/lib/mysql\n$ rm -f /etc/my.cnf\n</code></pre>"},{"location":"management/audit_log_plugin.html","title":"Audit Log Plugin","text":"<p>Percona Audit Log Plugin provides monitoring and logging of connection and query activity that were performed on specific server. Information about the activity will be stored in the XML log file where each event will have its <code>NAME</code> field, its own unique <code>RECORD_ID</code> field and a <code>TIMESTAMP</code> field. </p> <p>Audit logging documents the database usage. You can use the log for troubleshooting.</p>"},{"location":"management/audit_log_plugin.html#installation","title":"Installation","text":"<p>Audit Log plugin is installed with Percona Server for MySQL, but is not enabled by default. You can verify if the plugin is enabled by running the following commands:</p> <pre><code>mysql&gt; SELECT * FROM information_schema.PLUGINS WHERE PLUGIN_NAME LIKE '%audit%';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>Empty set (0.00 sec)\n</code></pre> <pre><code>mysql&gt; SHOW variables LIKE 'audit%';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>Empty set (0.01 sec)\n</code></pre> <pre><code>mysql&gt; SHOW variables LIKE 'plugin%';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>+---------------+------------------------+\n| Variable_name | Value                  |\n+---------------+------------------------+\n| plugin_dir    | /usr/lib/mysql/plugin/ |\n+---------------+------------------------+\n1 row in set (0.00 sec)\n</code></pre> <p>Note</p> <p>The location of the MySQL plugin directory depends on the operating system and may be different.</p> <p>The following command enables the plugin:</p> <pre><code>mysql&gt; INSTALL PLUGIN audit_log SONAME 'audit_log.so';\n</code></pre> <p>Run the following command to verify if the plugin was installed correctly:</p> <pre><code>mysql&gt; SELECT * FROM information_schema.PLUGINS WHERE PLUGIN_NAME LIKE '%audit%'\\G\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>*************************** 1. row ***************************\n          PLUGIN_NAME: audit_log\n        PLUGIN_VERSION: 0.2\n        PLUGIN_STATUS: ACTIVE\n          PLUGIN_TYPE: AUDIT\n  PLUGIN_TYPE_VERSION: 4.1\n        PLUGIN_LIBRARY: audit_log.so\nPLUGIN_LIBRARY_VERSION: 1.7\n        PLUGIN_AUTHOR: Percona LLC and/or its affiliates.\n    PLUGIN_DESCRIPTION: Audit log\n        PLUGIN_LICENSE: GPL\n          LOAD_OPTION: ON\n1 row in set (0.00 sec)\n</code></pre> <p>You can review the audit log variables with the following command:</p> <pre><code>mysql&gt; SHOW variables LIKE 'audit%';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>+-----------------------------+---------------+\n| Variable_name               | Value         |\n+-----------------------------+---------------+\n| audit_log_buffer_size       | 1048576       |\n| audit_log_exclude_accounts  |               |\n| audit_log_exclude_commands  |               |\n| audit_log_exclude_databases |               |\n| audit_log_file              | audit.log     |\n| audit_log_flush             | OFF           |\n| audit_log_format            | OLD           |\n| audit_log_handler           | FILE          |\n| audit_log_include_accounts  |               |\n| audit_log_include_commands  |               |\n| audit_log_include_databases |               |\n| audit_log_policy            | ALL           |\n| audit_log_rotate_on_size    | 0             |\n| audit_log_rotations         | 0             |\n| audit_log_strategy          | ASYNCHRONOUS  |\n| audit_log_syslog_facility   | LOG_USER      |\n| audit_log_syslog_ident      | percona-audit |\n| audit_log_syslog_priority   | LOG_INFO      |\n+-----------------------------+---------------+\n18 rows in set (0.00 sec)\n</code></pre> <p>Audit Log plugin produces the log of following events:</p> <ul> <li>Audit - Audit event indicates that audit logging started or finished. <code>NAME</code> field will be <code>Audit</code> when logging started and <code>NoAudit</code> when logging finished. Audit record also includes server version and command-line arguments.</li> </ul> <p>Example of the Audit event:</p> <pre><code>&lt;AUDIT_RECORD\n  NAME=\"Audit\"\n  RECORD=\"1_2021-06-30T11:56:53\"\n  TIMESTAMP=\"2021-06-30T11:56:53 UTC\"\n  MYSQL_VERSION=\"5.7.34-37\"\n  STARTUP_OPTIONS=\"--daemonize --pid-file=/var/run/mysqld/mysqld.pid\"\n  OS_VERSION=\"x86_64-debian-linux-gnu\"\n/&gt;\n</code></pre> <ul> <li>Connect/Disconnect - Connect record event will have <code>NAME</code> field <code>Connect</code> when user logged in or login failed, or <code>Quit</code> when connection is closed.</li> </ul> <p>Additional fields for this event are the following:</p> <ul> <li> <p><code>CONNECTION_ID</code></p> </li> <li> <p><code>STATUS</code></p> </li> <li> <p><code>USER</code></p> </li> <li> <p><code>PRIV_USER</code></p> </li> <li> <p><code>OS_LOGIN</code></p> </li> <li> <p><code>PROXY_USER</code></p> </li> <li> <p><code>HOST</code></p> </li> <li> <p><code>IP</code></p> </li> </ul> <p>The value for <code>STATUS</code> is <code>0</code> for successful logins and non-zero for failed logins.</p> <p>Example of the Disconnect event:</p> <pre><code>&lt;AUDIT_RECORD\n  NAME=\"Quit\"\n  RECORD=\"5_2021-06-29T19:33:03\"\n  TIMESTAMP=\"2021-06-29T19:34:38Z\"\n  CONNECTION_ID=\"14\"\n  STATUS=\"0\"\n  USER=\"root\"\n  PRIV_USER=\"root\"\n  OS_LOGIN=\"\"\n  PROXY_USER=\"\"\n  HOST=\"localhost\"\n  IP=\"\"\n  DB=\"\"\n/&gt;\n</code></pre> <ul> <li>Query - Additional fields for this event are: <code>COMMAND_CLASS</code> (values come from the <code>com_status_vars</code> array in the <code>sql/mysqld.cc\\`` file in a MySQL source distribution. Examples are</code>select<code>,</code>alter_table<code>,</code>create_table<code>, etc.),</code>CONNECTION_ID<code>,</code>STATUS<code>(indicates error when non-zero),</code>SQLTEXT<code>(text of SQL-statement),</code>USER<code>,</code>HOST<code>,</code>OS_USER<code>,</code>IP<code>. Possible values for the</code>NAME<code>name field for this event are</code>Query<code>,</code>Prepare<code>,</code>Execute<code>,</code>Change user`, etc..</li> </ul> <p>Note</p> <p>The <code>statement/sql/%</code>  populates the audit log command_class field, for example, the <code>SELECT name FROM performance_schema.setup_instruments WHERE name LIKE \"statement/sql/%\"</code> query.</p> <p>The <code>%statement/com%</code> entry populates the audit log command_class field as lowercase text, for example, the <code>SELECT name FROM performance_schema.setup_instruments WHERE name LIKE '%statement/com%'</code> query.  If you run a \u2018Ping\u2019 command, then the command_class field is \u2018ping\u2019, and for \u2018Init DB\u2019, the command_class field is \u2018init db\u2019.</p> <p>Example of the Query event:</p> <pre><code>&lt;AUDIT_RECORD\n  NAME=\"Query\"\n  RECORD=\"4_2021-06-29T19:33:03\"\n  TIMESTAMP=\"2021-06-29T19:33:34Z\"\n  COMMAND_CLASS=\"show_variables\"\n  CONNECTION_ID=\"14\"\n  STATUS=\"0\"\n  SQLTEXT=\"show variables like 'audit%'\"\n  USER=\"root[root] @ localhost []\"\n  HOST=\"localhost\"\n  OS_USER=\"\"\n  IP=\"\"\n  DB=\"\"\n/&gt;\n</code></pre>"},{"location":"management/audit_log_plugin.html#log-format","title":"Log Format","text":"<p>The audit log plugin supports the following log formats: <code>OLD</code>, <code>NEW</code>, <code>JSON</code>, and <code>CSV</code>. The <code>OLD\\</code>`format and the``NEW<code>format are based on XML. The</code>OLD<code>format defines each log record with XML attributes. The</code>NEW` format defines each log record with XML tags. The information logged is the same for all four formats. The audit_log_format variable controls the log format choice.</p> <p>An example of the <code>OLD</code> format:</p> <pre><code>&lt;AUDIT_RECORD\n  NAME=\"Query\"\n  RECORD=\"3_2021-06-30T11:56:53\"\n  TIMESTAMP=\"2021-06-30T11:57:14 UTC\"\n  COMMAND_CLASS=\"select\"\n  CONNECTION_ID=\"3\"\n  STATUS=\"0\"\n  SQLTEXT=\"select * from information_schema.PLUGINS where PLUGIN_NAME like '%audit%'\"\n  USER=\"root[root] @ localhost []\"\n  HOST=\"localhost\"\n  OS_USER=\"\"\n  IP=\"\"\n  DB=\"\"\n/&gt;\n</code></pre> <p>An example of the <code>NEW</code> format:</p> <pre><code>&lt;AUDIT_RECORD&gt;\n  &lt;NAME&gt;Query&lt;/NAME&gt;\n  &lt;RECORD&gt;16684_2021-06-30T16:07:41&lt;/RECORD&gt;\n  &lt;TIMESTAMP&gt;2021-06-30T16:08:06 UTC&lt;/TIMESTAMP&gt;\n  &lt;COMMAND_CLASS&gt;select&lt;/COMMAND_CLASS&gt;\n  &lt;CONNECTION_ID&gt;2&lt;/CONNECTION_ID&gt;\n  &lt;STATUS&gt;0&lt;/STATUS&gt;\n  &lt;SQLTEXT&gt;select id, holder from one&lt;/SQLTEXT&gt;\n  &lt;USER&gt;root[root] @ localhost []&lt;/USER&gt;\n  &lt;HOST&gt;localhost&lt;/HOST&gt;\n  &lt;OS_USER&gt;&lt;/OS_USER&gt;\n  &lt;IP&gt;&lt;/IP&gt;\n  &lt;DB&gt;&lt;/DB&gt;\n</code></pre> <p>An example of the <code>JSON</code> format:</p> <pre><code>{\"audit_record\":{\"name\":\"Query\",\"record\":\"13149_2021-06-30T15:03:11\",\"timestamp\":\"2021-06-30T15:07:58 UTC\",\"command_class\":\"show_databases\",\"connection_id\":\"2\",\"status\":0,\"sqltext\":\"show databases\",\"user\":\"root[root] @ localhost []\",\"host\":\"localhost\",\"os_user\":\"\",\"ip\":\"\",\"db\":\"\"}}\n</code></pre> <p>An example of the <code>CSV</code> format:</p> <pre><code>\"Query\",\"22567_2021-06-30T16:10:09\",\"2021-06-30T16:19:00 UTC\",\"select\",\"2\",0,\"select count(*) from one\",\"root[root] @ localhost []\",\"localhost\",\"\",\"\",\"\"\n</code></pre>"},{"location":"management/audit_log_plugin.html#streaming-the-audit-log-to-syslog","title":"Streaming the audit log to syslog","text":"<p>To stream the audit log to syslog you\u2019ll need to set audit_log_handler variable to <code>SYSLOG</code>. To control the syslog file handler, the following variables can be used: audit_log_syslog_ident, audit_log_syslog_facility, and audit_log_syslog_priority These variables have the same meaning as appropriate parameters described in the syslog(3) manual.</p> <p>Note</p> <p>The actions for the variables: audit_log_strategy, audit_log_buffer_size, audit_log_rotate_on_size, audit_log_rotations are captured only with <code>FILE</code> handler.</p>"},{"location":"management/audit_log_plugin.html#filtering-by-user","title":"Filtering by user","text":"<p>In Percona Server for MySQL 5.7.14-7 Percona Server for MySQL has implemented filtering by user. This was implemented by adding two new global variables: audit_log_include_accounts and audit_log_exclude_accounts to specify which user accounts should be included or excluded from audit logging.</p> <p>Warning</p> <p>Only one of these variables can contain a list of users to be either included or excluded, while the other needs to be <code>NULL</code>. If one of the variables is set to be not <code>NULL</code> (contains a list of users), the attempt to set another one will fail. Empty string means an empty list.</p> <p>Note</p> <p>Changes of audit_log_include_accounts and audit_log_exclude_accounts do not apply to existing server connections.</p>"},{"location":"management/audit_log_plugin.html#example","title":"Example","text":"<p>Following example shows adding users who will be monitored:</p> <p><pre><code>mysql&gt; SET GLOBAL audit_log_include_accounts = 'user1@localhost,root@localhost';\n</code></pre> The output should be similar to the following;</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <p>If you you try to add users to both include and exclude lists server will show you the following error:</p> <pre><code>mysql&gt; SET GLOBAL audit_log_exclude_accounts = 'user1@localhost,root@localhost';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>ERROR 1231 (42000): Variable 'audit_log_exclude_accounts' can't be set to the value of 'user1@localhost,root@localhost'\n</code></pre> <p>To switch from filtering by included user list to the excluded one or back, first set the currently active filtering variable to <code>NULL</code>:</p> <pre><code>mysql&gt; SET GLOBAL audit_log_include_accounts = NULL;\n</code></pre> <p>The output should be similar to the following;</p> <p><pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <pre><code>mysql&gt; SET GLOBAL audit_log_exclude_accounts = 'user1@localhost,root@localhost';\n</code></pre></p> <p>The output should be similar to the following;</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <pre><code>mysql&gt; SET GLOBAL audit_log_exclude_accounts = \"'user'@'host'\";\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <pre><code>mysql&gt; SET GLOBAL audit_log_exclude_accounts = '''user''@''host''';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <pre><code>mysql&gt; SET GLOBAL audit_log_exclude_accounts = '\\'user\\'@\\'host\\'';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <p>To see what users are currently in the on the list you can run:</p> <pre><code>mysql&gt; SELECT @@audit_log_exclude_accounts;\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>+------------------------------+\n| @@audit_log_exclude_accounts |\n+------------------------------+\n| 'user'@'host'                |\n+------------------------------+\n1 row in set (0.00 sec)\n</code></pre> <p>Account names from mysql.user table are the one that are logged in the audit log. For example when you create a user:</p> <pre><code>mysql&gt; CREATE USER 'user1'@'%' IDENTIFIED BY '111';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <p>This is what you\u2019ll see when <code>user1</code> connected from <code>localhost</code>:</p> <pre><code>&lt;AUDIT_RECORD\n  NAME=\"Connect\"\n  RECORD=\"2_2021-06-30T11:56:53\"\n  TIMESTAMP=\"2021-06-30T11:56:53 UTC\"\n  CONNECTION_ID=\"6\"\n  STATUS=\"0\"\n  USER=\"user1\" ;; this is a 'user' part of account in 5.7\n  PRIV_USER=\"user1\"\n  OS_LOGIN=\"\"\n  PROXY_USER=\"\"\n  HOST=\"localhost\" ;; this is a 'host' part of account in 5.7\n  IP=\"\"\n  DB=\"\"\n/&gt;\n</code></pre> <p>To exclude <code>user1</code> from logging in Percona Server for MySQL 5.7 you must set:</p> <pre><code>SET GLOBAL audit_log_exclude_accounts = 'user1@%';\n</code></pre> <p>The value can be <code>NULL</code> or comma separated list of accounts in form <code>user@host</code> or <code>'user'@'host'</code> (if user or host contains comma).</p>"},{"location":"management/audit_log_plugin.html#filtering-by-sql-command-type","title":"Filtering by SQL command type","text":"<p>In Percona Server for MySQL 5.7.14-7 Percona Server for MySQL has implemented filtering by SQL command type. This was implemented by adding two new global variables: audit_log_include_commands and audit_log_exclude_commands to specify which command types should be included or excluded from audit logging.</p> <p>Warning</p> <p>Only one of these variables can contain a list of command types to be either included or excluded, while the other needs to be <code>NULL</code>. If one of the variables is set to be not <code>NULL</code> (contains a list of command types), the attempt to set another one will fail. Empty string means an empty list.</p> <p>Note</p> <p>If both audit_log_exclude_commands and audit_log_include_commands are <code>NULL</code> all commands will be logged.</p>"},{"location":"management/audit_log_plugin.html#example_1","title":"Example","text":"<p>The available command types can be listed by running:</p> <pre><code>mysql&gt; SELECT name FROM performance_schema.setup_instruments WHERE name LIKE \"statement/sql/%\" ORDER BY name;\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>+------------------------------------------+\n| name                                     |\n+------------------------------------------+\n| statement/sql/alter_db                   |\n| statement/sql/alter_db_upgrade           |\n| statement/sql/alter_event                |\n| statement/sql/alter_function             |\n| statement/sql/alter_procedure            |\n| statement/sql/alter_server               |\n| statement/sql/alter_table                |\n| statement/sql/alter_tablespace           |\n| statement/sql/alter_user                 |\n| statement/sql/analyze                    |\n| statement/sql/assign_to_keycache         |\n| statement/sql/begin                      |\n| statement/sql/binlog                     |\n| statement/sql/call_procedure             |\n| statement/sql/change_db                  |\n| statement/sql/change_master              |\n...\n| statement/sql/xa_rollback                |\n| statement/sql/xa_start                   |\n+------------------------------------------+\n145 rows in set (0.00 sec)\n</code></pre> <p>You can add commands to the include filter by running:</p> <pre><code>mysql&gt; SET GLOBAL audit_log_include_commands= 'set_option,create_db';\n</code></pre> <p>When you create a database with the following command:</p> <pre><code>mysql&gt; CREATE DATABASE sample;\n</code></pre> <p>The action is captured in the audit log:</p> <pre><code>&lt;AUDIT_RECORD&gt;\n  &lt;NAME&gt;Query&lt;/NAME&gt;\n  &lt;RECORD&gt;24320_2021-06-30T17:44:46&lt;/RECORD&gt;\n  &lt;TIMESTAMP&gt;2021-06-30T17:45:16 UTC&lt;/TIMESTAMP&gt;\n  &lt;COMMAND_CLASS&gt;create_db&lt;/COMMAND_CLASS&gt;\n  &lt;CONNECTION_ID&gt;2&lt;/CONNECTION_ID&gt;\n  &lt;STATUS&gt;0&lt;/STATUS&gt;\n  &lt;SQLTEXT&gt;CREATE DATABASE sample&lt;/SQLTEXT&gt;\n  &lt;USER&gt;root[root] @ localhost []&lt;/USER&gt;\n  &lt;HOST&gt;localhost&lt;/HOST&gt;\n  &lt;OS_USER&gt;&lt;/OS_USER&gt;\n  &lt;IP&gt;&lt;/IP&gt;\n  &lt;DB&gt;&lt;/DB&gt;\n&lt;/AUDIT_RECORD&gt;\n</code></pre> <p>To switch command type filtering type from included type list to excluded one or back, first reset the currently-active list to <code>NULL</code>:</p> <pre><code>mysql&gt; SET GLOBAL audit_log_include_commands = NULL;\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <pre><code>mysql&gt; SET GLOBAL audit_log_exclude_commands= 'set_option,create_db';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <p>Note</p> <p>Invocation of stored procedures have command type <code>call_procedure</code>, and all the statements executed within the procedure have the same type <code>call_procedure</code> as well.</p>"},{"location":"management/audit_log_plugin.html#filtering-by-database","title":"Filtering by database","text":"<p>In Percona Server for MySQL 5.7.14-7 Percona Server for MySQL has implemented filtering by SQL database. This was implemented by adding two new global variables: audit_log_include_databases and audit_log_exclude_databases to specify which databases should be included or excluded from audit logging.</p> <p>Warning</p> <p>Only one of these variables can contain a list of databases to be either included or excluded, while the other needs to be <code>NULL</code>. If one of the variables is set to be not <code>NULL</code> (contains a list of databases), the attempt to set another one will fail. Empty string means an empty list.</p> <p>If query is accessing any of databases listed in <code>audit_log_include_databases</code>, the query will be logged. If query is accessing only databases listed in <code>audit_log_exclude_databases</code>, the query will not be logged. <code>CREATE TABLE</code> statements are logged unconditionally.</p> <p>Note</p> <p>Changes of audit_log_include_databases and audit_log_exclude_databases do not apply to existing server connections.</p>"},{"location":"management/audit_log_plugin.html#example_2","title":"Example","text":"<p>To add databases to be monitored you should run:</p> <pre><code>mysql&gt; SET GLOBAL audit_log_include_databases = 'test,mysql,db1';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <pre><code>mysql&gt; SET GLOBAL audit_log_include_databases= 'db1,db3';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <p>If you you try to add databases to both include and exclude lists server will show you the following error:</p> <pre><code>mysql&gt; SET GLOBAL audit_log_exclude_databases = 'test,mysql,db1';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>ERROR 1231 (42000): Variable 'audit_log_exclude_databases can't be set to the value of 'test,mysql,db1'\n</code></pre> <p>To switch from filtering by included database list to the excluded one or back, first set the currently active filtering variable to <code>NULL</code>:</p> <pre><code>mysql&gt; SET GLOBAL audit_log_include_databases = NULL;\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <pre><code>mysql&gt; SET GLOBAL audit_log_exclude_databases = 'test,mysql,db1';\n</code></pre> <p>The output should be similar to the following;</p> <pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre>"},{"location":"management/audit_log_plugin.html#system-variables","title":"System Variables","text":""},{"location":"management/audit_log_plugin.html#audit_log_strategy","title":"audit_log_strategy","text":"Option Description Command-line Yes Scope Global Dynamic No Data type String Default ASYNCHRONOUS Allowed values ASYNCHRONOUS, PERFORMANCE, SEMISYNCHRONOUS, SYNCHRONOUS <p>This variable is used to specify the audit log strategy, possible values are:</p> <ul> <li> <p><code>ASYNCHRONOUS</code> - (default) log using memory buffer, do not drop messages if buffer is full</p> </li> <li> <p><code>PERFORMANCE</code> - log using memory buffer, drop messages if buffer is full</p> </li> <li> <p><code>SEMISYNCHRONOUS</code> - log directly to file, do not flush and sync every event</p> </li> <li> <p><code>SYNCHRONOUS</code> - log directly to file, flush and sync every event</p> </li> </ul> <p>This variable has effect only when audit_log_handler is set to <code>FILE</code>.</p>"},{"location":"management/audit_log_plugin.html#audit_log_file","title":"audit_log_file","text":"Option Description Command-line Yes Scope Global Dynamic No Data type String Default audit.log <p>This variable is used to specify the filename that\u2019s going to store the audit log. It can contain the path relative to the datadir or absolute path.</p>"},{"location":"management/audit_log_plugin.html#audit_log_flush","title":"audit_log_flush","text":"Option Description Command-line Yes Scope Global Dynamic Yes Data type String Default OFF <p>When this variable is set to <code>ON</code> log file will be closed and reopened. This can be used for manual log rotation.</p>"},{"location":"management/audit_log_plugin.html#audit_log_buffer_size","title":"audit_log_buffer_size","text":"Option Description Command-line Yes Scope Global Dynamic No Data type Numeric Default 1 Mb <p>This variable can be used to specify the size of memory buffer used for logging, used when audit_log_strategy variable is set to <code>ASYNCHRONOUS</code> or <code>PERFORMANCE</code> values. This variable has effect only when audit_log_handler is set to <code>FILE</code>.</p>"},{"location":"management/audit_log_plugin.html#audit_log_exclude_accounts","title":"audit_log_exclude_accounts","text":"Option Description Command-line Yes Scope Global Dynamic Yes Data type String <p>The variable has been implemented in Percona Server for MySQL 5.7.14-7. This variable is used to specify the list of users for which Filtering by user is applied. The value can be <code>NULL</code> or comma separated list of accounts in form <code>user@host</code> or <code>'user'@'host'</code> (if user or host contains comma). If this variable is set, then audit_log_include_accounts must be unset, and vice versa.</p>"},{"location":"management/audit_log_plugin.html#audit_log_exclude_commands","title":"audit_log_exclude_commands","text":"Option Description Command-line Yes Scope Global Dynamic Yes Data type String <p>The variable has been implemented in Percona Server for MySQL 5.7.14-7. This variable is used to specify the list of commands for which Filtering by SQL command type is applied. The value can be <code>NULL</code> or comma separated list of commands. If this variable is set, then audit_log_include_commands must be unset, and vice versa.</p>"},{"location":"management/audit_log_plugin.html#audit_log_exclude_databases","title":"audit_log_exclude_databases","text":"Option Description Command-line Yes Scope Global Dynamic Yes Data type String <p>The variable has been implemented in Percona Server for MySQL 5.7.14-7. This variable is used to specify the list of commands for which Filtering by database is applied. The value can be <code>NULL</code> or comma separated list of commands. If this variable is set, then audit_log_include_databases must be unset, and vice versa.</p>"},{"location":"management/audit_log_plugin.html#audit_log_format","title":"audit_log_format","text":"Option Description Command-line Yes Scope Global Dynamic No Data type String Default OLD Allowed values OLD, NEW, CSV, JSON <p>Implemented in Percona Server for MySQL 5.7.14-7.</p>"},{"location":"management/audit_log_plugin.html#audit_log_include_accounts","title":"audit_log_include_accounts","text":"Option Description Command-line Yes Scope Global Dynamic Yes Data type String <p>The variable has been implemented in Percona Server for MySQL 5.7.14-7. This variable is used to specify the list of users for which Filtering by user is applied. The value can be <code>NULL</code> or comma separated list of accounts in form <code>user@host</code> or <code>'user'@'host'</code> (if user or host contains comma). If this variable is set, then audit_log_exclude_accounts must be unset, and vice versa.</p>"},{"location":"management/audit_log_plugin.html#audit_log_include_commands","title":"audit_log_include_commands","text":"Option Description Command-line Yes Scope Global Dynamic Yes Data type String <p>The variable has been implemented in Percona Server for MySQL 5.7.14-7. This variable is used to specify the list of commands for which Filtering by SQL command type is applied. The value can be <code>NULL</code> or comma separated list of commands. If this variable is set, then audit_log_exclude_commands must be unset, and vice versa.</p>"},{"location":"management/audit_log_plugin.html#audit_log_include_databases","title":"audit_log_include_databases","text":"Option Description Command-line Yes Scope Global Dynamic Yes Data type String <p>The variable has been implemented in Percona Server for MySQL 5.7.14-7. This variable is used to specify the list of commands for which Filtering by database is applied. The value can be <code>NULL</code> or comma separated list of commands. If this variable is set, then audit_log_exclude_databases must be unset, and vice versa.</p>"},{"location":"management/audit_log_plugin.html#audit_log_policy","title":"audit_log_policy","text":"Option Description Command-line Yes Scope Global Dynamic Yes Data type String Default ALL Allowed values ALL, LOGINS, QUERIES, NONE <p>This variable is used to specify which events should be logged. Possible values are:</p> <ul> <li> <p><code>ALL</code> - all events will be logged</p> </li> <li> <p><code>LOGINS</code> - only logins will be logged</p> </li> <li> <p><code>QUERIES</code> - only queries will be logged</p> </li> <li> <p><code>NONE</code> - no events will be logged</p> </li> </ul>"},{"location":"management/audit_log_plugin.html#audit_log_rotate_on_size","title":"audit_log_rotate_on_size","text":"Option Description Command-line Yes Scope Global Dynamic No Data type Numeric Default 0 (don\u2019t rotate the log file) <p>This variable is measured in bytes and specifies the maximum size of the audit log file. Upon reaching this size, the audit log will be rotated. The rotated log files are present in the same directory as the current log file. The sequence number is appended to the log file name upon rotation. For this variable to take effect, set the audit_log_handler variable to <code>FILE</code> and the audit_log_rotations variable to a value greater than zero.</p>"},{"location":"management/audit_log_plugin.html#audit_log_rotations","title":"audit_log_rotations","text":"Option Description Command-line Yes Scope Global Dynamic No Data type Numeric Default 0 <p>This variable is used to specify how many log files should be kept when audit_log_rotate_on_size variable is set to non-zero value. This variable has effect only when audit_log_handler is set to <code>FILE</code>.</p>"},{"location":"management/audit_log_plugin.html#audit_log_handler","title":"audit_log_handler","text":"Option Description Command-line Yes Scope Global Dynamic No Data type String Default FILE Allowed values FILE, SYSLOG <p>This variable is used to configure where the audit log will be written. If it is set to <code>FILE</code>, the log will be written into a file specified by audit_log_file variable. If it is set to <code>SYSLOG</code>, the audit log will be written to syslog.</p>"},{"location":"management/audit_log_plugin.html#audit_log_syslog_ident","title":"audit_log_syslog_ident","text":"Option Description Command-line Yes Scope Global Dynamic No Data type String Default percona-audit <p>This variable is used to specify the <code>ident</code> value for syslog. This variable has the same meaning as the appropriate parameter described in the syslog(3) manual.</p>"},{"location":"management/audit_log_plugin.html#audit_log_syslog_facility","title":"audit_log_syslog_facility","text":"Option Description Command-line Yes Scope Global Dynamic No Data type String Default LOG_USER <p>This variable is used to specify the <code>facility</code> value for syslog. This variable has the same meaning as the appropriate parameter described in the syslog(3) manual.</p>"},{"location":"management/audit_log_plugin.html#audit_log_syslog_priority","title":"audit_log_syslog_priority","text":"Option Description Command-line Yes Scope Global Dynamic No Data type String Default LOG_INFO <p>This variable is used to specify the <code>priority</code> value for syslog. This variable has the same meaning as the appropriate parameter described in the syslog(3) manual.</p>"},{"location":"management/audit_log_plugin.html#status-variables","title":"Status Variables","text":""},{"location":"management/audit_log_plugin.html#audit_log_buffer_size_overflow","title":"audit_log_buffer_size_overflow","text":"Option Description Scope Global Data type Numeric <p>The number of times an audit log entry was either dropped or written directly to the file due to its size being bigger than audit_log_buffer_size variable.</p>"},{"location":"management/audit_log_plugin.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li> <p>Percona Server for MySQL 5.7.10-1 Feature ported from Percona Server for MySQL 5.6</p> </li> <li> <p>Percona Server for MySQL 5.7.14-7 Percona Server for MySQL Audit Log Plugin now supports filtering by user, sql_command, and databases.</p> </li> <li> <p>Percona Server for MySQL 5.7.26-29 Audit_log_buffer_size_overflow variable implemented</p> </li> </ul>"},{"location":"management/backup_locks.html","title":"Backup Locks","text":"<p>Percona Server for MySQL has implemented this feature to be a lightweight alternative to <code>FLUSH TABLES WITH READ LOCK</code> for both physical and logical backups. Three new statements are now available: <code>LOCK TABLES FOR BACKUP</code>, <code>LOCK BINLOG FOR BACKUP</code> and <code>UNLOCK BINLOG</code>.</p>"},{"location":"management/backup_locks.html#lock-tables-for-backup","title":"<code>LOCK TABLES FOR BACKUP</code>","text":"<p><code>LOCK TABLES FOR BACKUP</code> uses a new MDL lock type to block updates to non-transactional tables and DDL statements for all tables. If there is an active <code>LOCK TABLES FOR BACKUP</code> lock then all DDL statements and all updates to MyISAM, CSV, MEMORY, ARCHIVE, TokuDB, and MyRocks tables will be blocked in the <code>Waiting for backup lock</code> status, visible in <code>PERFORMANCE_SCHEMA</code> or <code>PROCESSLIST</code>.</p> <p><code>LOCK TABLES FOR BACKUP</code> has no effect on <code>SELECT</code> queries for all mentioned storage engines. Against InnoDB, Blackhole and Federated tables, the <code>LOCK TABLES FOR BACKUP</code> is not applicable to the <code>INSERT</code>, <code>REPLACE</code>, <code>UPDATE</code>, <code>DELETE</code> statements: Blackhole tables obviously have no relevance to backups, and Federated tables are ignored by both logical and physical backup tools.</p> <p>Unlike <code>FLUSH TABLES WITH READ LOCK</code>, <code>LOCK TABLES FOR BACKUP</code> does not flush tables, i.e. storage engines are not forced to close tables and tables are not expelled from the table cache. As a result, <code>LOCK TABLES FOR BACKUP</code> only waits for conflicting statements to complete (i.e. DDL and updates to non-transactional tables). It never waits for SELECTs, or UPDATEs to InnoDB tables to complete, for example.</p> <p>If an \u201cunsafe\u201d statement is executed in the same connection that is holding a <code>LOCK TABLES FOR BACKUP</code> lock, it fails with the following error:</p> <pre><code>ERROR 1880 (HY000): Can't execute the query because you have a conflicting backup lock\n\nUNLOCK TABLES releases the lock acquired by LOCK TABLES FOR BACKUP.\n</code></pre>"},{"location":"management/backup_locks.html#lock-binlog-for-backup","title":"<code>LOCK BINLOG FOR BACKUP</code>","text":"<p><code>LOCK BINLOG FOR BACKUP</code> uses another new MDL lock type to block all operations that might change either binary log position or <code>Exec_Master_Log_Pos</code> or <code>Exec_Gtid_Set</code> (i.e. source binary log coordinates corresponding to the current SQL thread state on a replication replica) as reported by <code>SHOW MASTER</code>/<code>SLAVE STATUS</code>. More specifically, a commit will only be blocked if the binary log is enabled (both globally, and for connection with sql_log_bin), or if commit is performed by a replica thread and would advance <code>Exec_Master_Log_Pos</code> or <code>Executed_Gtid_Set</code>. Connections that are currently blocked on the global binlog lock can be identified by the <code>Waiting for binlog lock</code> status in <code>PROCESSLIST</code>.</p> <p><code>LOCK TABLES FOR BACKUP</code> flushes the current binary log coordinates to InnoDB. Thus, under active <code>LOCK TABLES FOR BACKUP</code>, the binary log coordinates in InnoDB are consistent with its redo log and any non-transactional updates (as the latter are blocked by <code>LOCK TABLES FOR BACKUP</code>). It is planned that this change will enable Percona XtraBackup to avoid issuing the more invasive <code>LOCK BINLOG FOR BACKUP</code> command under some circumstances.</p>"},{"location":"management/backup_locks.html#unlock-binlog","title":"<code>UNLOCK BINLOG</code>","text":"<p><code>UNLOCK BINLOG</code> releases the <code>LOCK BINLOG FOR BACKUP</code> lock, if acquired by the current connection. The intended use case for Percona XtraBackup is:</p> <pre><code>LOCK TABLES FOR BACKUP\n... copy .frm, MyISAM, CSV, etc. ...\nLOCK BINLOG FOR BACKUP\nUNLOCK TABLES\n... get binlog coordinates ...\n... wait for redo log copying to finish ...\nUNLOCK BINLOG\n</code></pre>"},{"location":"management/backup_locks.html#privileges","title":"Privileges","text":"<p>Both <code>LOCK TABLES FOR BACKUP</code> and <code>LOCK BINLOG FOR BACKUP</code> require the <code>RELOAD</code> privilege. The reason for that is to have the same requirements as <code>FLUSH TABLES WITH READ LOCK</code>.</p>"},{"location":"management/backup_locks.html#interaction-with-other-global-locks","title":"Interaction with other global locks","text":"<p>Both <code>LOCK TABLES FOR BACKUP</code> and <code>LOCK BINLOG FOR BACKUP</code> have no effect if the current connection already owns a <code>FLUSH TABLES WITH READ LOCK</code> lock, as it\u2019s a more restrictive lock. If <code>FLUSH TABLES WITH READ LOCK</code> is executed in a connection that has acquired <code>LOCK TABLES FOR BACKUP</code> or <code>LOCK BINLOG FOR BACKUP</code>, <code>FLUSH TABLES WITH READ LOCK</code> fails with an error.</p> <p>If the server is operating in the read-only mode (i.e. read_only set to <code>1</code>), statements that are unsafe for backups will be either blocked or fail with an error, depending on whether they are executed in the same connection that owns <code>LOCK TABLES FOR BACKUP</code> lock, or other connections.</p>"},{"location":"management/backup_locks.html#myisam-index-and-data-buffering","title":"MyISAM index and data buffering","text":"<p>MyISAM key buffering is normally write-through, i.e. by the time each update to a MyISAM table is completed, all index updates are written to disk. The only exception is delayed key writing feature which is disabled by default.</p> <p>When the global system variable delay_key_write is set to <code>ALL</code>, key buffers for all MyISAM tables are not flushed between updates, so a physical backup of those tables may result in broken MyISAM indexes. To prevent this, <code>LOCK TABLES FOR BACKUP</code> will fail with an error if <code>delay_key_write</code> is set to <code>ALL</code>. An attempt to set delay_key_write to <code>ALL</code> when there\u2019s an active backup lock will also fail with an error.</p> <p>Another option to involve delayed key writing is to create MyISAM tables with the DELAY_KEY_WRITE option and set the delay_key_write variable to <code>ON</code> (which is the default). In this case, <code>LOCK TABLES FOR BACKUP</code> will not be able to prevent stale index files from appearing in the backup. Users are encouraged to set delay_key_writes to <code>OFF</code> in the configuration file, <code>my.cnf</code>, or repair MyISAM indexes after restoring from a physical backup created with backup locks.</p> <p>MyISAM may also cache data for bulk inserts, e.g. when executing multi-row INSERTs or <code>LOAD DATA</code> statements. Those caches, however, are flushed between statements, so have no effect on physical backups as long as all statements updating MyISAM tables are blocked.</p>"},{"location":"management/backup_locks.html#mysqldump","title":"mysqldump","text":"<p><code>mysqldump</code> has also been extended with a new option, <code>lock-for-backup</code> (disabled by default). When used together with the <code>--single-transaction</code> option, the option makes <code>mysqldump</code> issue <code>LOCK TABLES FOR BACKUP</code> before starting the dump operation to prevent unsafe statements that would normally result in an inconsistent backup.</p> <p>When used without the <code>single-transaction</code> option, <code>lock-for-backup</code> is automatically converted to <code>lock-all-tables</code>.</p> <p>Option <code>lock-for-backup</code> is mutually exclusive with <code>lock-all-tables</code>, i.e. specifying both on the command line will lead to an error.</p> <p>If the backup locks feature is not supported by the target server, but <code>lock-for-backup</code> is specified on the command line, <code>mysqldump</code> aborts with an error.</p> <p>Percona XtraBackup provides the \u2013backup-locks option. If you disable this option, <code>Flush Table with Read Lock</code> is used on the backup stage.</p>"},{"location":"management/backup_locks.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"management/backup_locks.html#system-variables","title":"System Variables","text":""},{"location":"management/backup_locks.html#have_backup_locks","title":"<code>have_backup_locks</code>","text":"Option Description Command-line Yes Config file No Scope Global Dynamic No Data type Boolean Default YES <p>This is a server variable implemented to help other utilities decide what locking strategy can be implemented for a server. When available, the backup locks feature is supported by the server and the variable value is always <code>YES</code>.</p>"},{"location":"management/backup_locks.html#have_backup_safe_binlog_info","title":"<code>have_backup_safe_binlog_info</code>","text":"Option Description Command-line Yes Config file No Scope Global Dynamic No Data type Boolean Default YES <p>This is a server variable implemented to help other utilities decide if <code>LOCK BINLOG FOR BACKUP</code> can be avoided in some cases. When the necessary server-side functionality is available, this server system variable exists and its value is always <code>YES</code>.</p>"},{"location":"management/backup_locks.html#status-variables","title":"Status Variables","text":""},{"location":"management/backup_locks.html#com_lock_tables_for_backup","title":"<code>Com_lock_tables_for_backup</code>","text":"Option Description Scope Global/Session Data type Numeric"},{"location":"management/backup_locks.html#com_lock_binlog_for_backup","title":"<code>Com_lock_binlog_for_backup</code>","text":"Option Description Scope Global/Session Data type Numeric"},{"location":"management/backup_locks.html#com_unlock_binlog","title":"<code>Com_unlock_binlog</code>","text":"Option Description Scope Global/Session Data type Numeric <p>These status variables indicate the number of times the corresponding statements have been executed.</p>"},{"location":"management/backup_locks.html#client-command-line-parameter","title":"Client Command Line Parameter","text":""},{"location":"management/backup_locks.html#lock-for-backup","title":"<code>lock-for-backup</code>","text":"Option Description Command-line Yes Scope Global Dynamic No Data type String Default Off <p>When used together with the <code>--single-transaction</code> option, the option makes <code>mysqldump</code> issue <code>LOCK TABLES FOR BACKUP</code> before starting the dump operation to prevent unsafe statements that would normally result in an inconsistent backup.</p>"},{"location":"management/changed_page_tracking.html","title":"XtraDB changed page tracking","text":"<p>XtraDB now tracks the pages that have changes written to them according to the redo log. This information is written out in special changed page bitmap files.  This information can be used to speed up incremental backups using Percona XtraBackup by removing the need to scan whole data files to find the changed pages. Changed page tracking is done by a new XtraDB worker thread that reads and parses log records between checkpoints. The tracking is controlled by a new read-only server variable innodb_track_changed_pages.</p> <p>Bitmap filename format used for changed page tracking is <code>ib_modified_log_&lt;seq&gt;_&lt;startlsn&gt;.xdb</code>. The first number is the sequence number of the bitmap log file and the startlsn number is the starting LSN number of data tracked in that file. Example of the bitmap log files should look like this:</p> <pre><code>ib_modified_log_1_0.xdb\nib_modified_log_2_1603391.xdb\n</code></pre> <p>Sequence number can be used to easily check if all the required bitmap files are present. Start LSN number will be used in XtraBackup and <code>INFORMATION_SCHEMA</code> queries to determine which files have to be opened and read for the required LSN interval data. The bitmap file is rotated on each server restart and whenever the current file size reaches the predefined maximum. This maximum is controlled by a new innodb_max_bitmap_file_size variable.</p> <p>Old bitmap files may be safely removed after a corresponding incremental backup is taken. For that there are server User statements for handling the XtraDB changed page bitmaps. Removing the bitmap files from the filesystem directly is safe too, as long as care is taken not to delete data for not-yet-backuped LSN range.</p> <p>This feature will be used for implementing faster incremental backups that use this information to avoid full data scans in Percona XtraBackup.</p>"},{"location":"management/changed_page_tracking.html#user-statements-for-handling-the-xtradb-changed-page-bitmaps","title":"User statements for handling the XtraDB changed page bitmaps","text":"<p>New statements have been introduced for handling the changed page bitmap tracking. All of these statements require <code>SUPER</code> privilege.</p> <ul> <li> <p><code>FLUSH CHANGED_PAGE_BITMAPS</code> - this statement can be used for synchronous bitmap write for immediate catch-up with the log checkpoint. This is used by innobackupex to make sure that XtraBackup indeed has all the required data it needs.</p> </li> <li> <p><code>RESET CHANGED_PAGE_BITMAPS</code> - this statement will delete all the bitmap log files and restart the bitmap log file sequence.</p> </li> <li> <p><code>PURGE CHANGED_PAGE_BITMAPS BEFORE &lt;lsn&gt;</code> - this statement will delete all the change page bitmap files up to the specified log sequence number.</p> </li> </ul>"},{"location":"management/changed_page_tracking.html#additional-information-in-show-engine-innodb-status","title":"Additional information in SHOW ENGINE INNODB STATUS","text":"<p>When log tracking is enabled, the following additional fields are displayed in the LOG section of the <code>SHOW ENGINE INNODB STATUS</code> output:</p> <ul> <li> <p>\u201cLog tracked up to:\u201d displays the LSN up to which all the changes have been parsed and stored as a bitmap on disk by the log tracking thread</p> </li> <li> <p>\u201cMax tracked LSN age:\u201d displays the maximum limit on how far behind the log tracking thread may be.</p> </li> </ul>"},{"location":"management/changed_page_tracking.html#information_schema-tables","title":"INFORMATION_SCHEMA Tables","text":"<p>This table contains a list of modified pages from the bitmap file data.  As these files are generated by the log tracking thread parsing the log whenever the checkpoint is made, it is not real-time data.</p>"},{"location":"management/changed_page_tracking.html#information_schemainnodb_changed_pages","title":"<code>INFORMATION_SCHEMA.INNODB_CHANGED_PAGES</code>","text":"Column Name Description \u2018INT(11) space_id\u2019 \u2018space id of modified page\u2019 \u2018INT(11) page_id\u2019 \u2018id of modified page\u2019 \u2018BIGINT(21) start_lsn\u2019 \u2018start of the interval\u2019 \u2018BIGINT(21) end_lsn\u2019 \u2018end of the interval \u2018 <p>The <code>start_lsn</code> and the <code>end_lsn</code> columns denote between which two checkpoints this page was changed at least once. They are also equal to checkpoint LSNs.</p> <p>Number of records in this table can be limited by using the variable innodb_max_changed_pages.</p>"},{"location":"management/changed_page_tracking.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"management/changed_page_tracking.html#system-variables","title":"System Variables","text":""},{"location":"management/changed_page_tracking.html#innodb_max_changed_pages","title":"<code>innodb_max_changed_pages</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 1000000 Range 1 - 0 (unlimited) <p>This variable is used to limit the result row count for the queries from INFORMATION_SCHEMA.INNODB_CHANGED_PAGES table.</p>"},{"location":"management/changed_page_tracking.html#innodb_track_changed_pages","title":"<code>innodb_track_changed_pages</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type Boolean Default 0 - False Range 0-1 <p>This variable is used to enable/disable XtraDB changed page tracking feature.</p>"},{"location":"management/changed_page_tracking.html#innodb_max_bitmap_file_size","title":"<code>innodb_max_bitmap_file_size</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 104857600 (100 MB) Range 4096 (4KB) - 18446744073709551615 (16EB) <p>This variable is used to control maximum bitmap size after which the file will be rotated.</p>"},{"location":"management/enforce_engine.html","title":"Enforcing Storage Engine","text":"<p>Percona Server for MySQL has implemented variable which can be used for enforcing the use of a specific storage engine.</p> <p>When this variable is specified and a user tries to create a table using an explicit storage engine that is not the specified enforced engine, he will get either an error if the <code>NO_ENGINE_SUBSTITUTION</code> SQL mode is enabled or a warning if <code>NO_ENGINE_SUBSTITUTION</code> is disabled and the table will be created anyway using the enforced engine (this is consistent with the default MySQL way of creating the default storage engine if other engines aren\u2019t available unless <code>NO_ENGINE_SUBSTITUTION</code> is set).</p> <p>In case user tries to enable enforce_storage_engine with engine that isn\u2019t available, system will not start.</p> <p>Note</p> <p>If you\u2019re using enforce_storage_engine, you must either disable it before doing <code>mysql_upgrade</code> or perform <code>mysql_upgrade</code> with server started with <code>--skip-grants-tables</code>.</p>"},{"location":"management/enforce_engine.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"management/enforce_engine.html#system-variables","title":"System Variables","text":""},{"location":"management/enforce_engine.html#enforce_storage_engine","title":"enforce_storage_engine","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type String Default NULL <p>Note</p> <p>This variable is not case sensitive.</p>"},{"location":"management/enforce_engine.html#example","title":"Example","text":"<p>Adding following option to source/glossary.rst`my.cnf` will start the server with InnoDB as enforced storage engine.</p> <pre><code>enforce_storage_engine=InnoDB\n</code></pre>"},{"location":"management/expanded_program_option_modifiers.html","title":"Expanded Program Option Modifiers","text":"<p>MySQL has the concept of options modifiers which is a simple way to modify either the way that MySQL interprets an option or the way the option behaves. Option modifiers are used by simply pre-pending the name of the modifier and a dash \u201c-\u201d before the actual configuration option name. For example specifying \u2013maximum-query_cache_size=4M on the mysqld command line or specifying maximum-query_cache_size=4M in the <code>my.cnf</code> will prevent any client from setting the query_cache_size value larger than 4MB.</p> <p>Currently MySQL supports five existing option modifiers:</p> <pre><code>* disable [disable-&lt;option_name&gt;] disables or ignores option_name.\n\n* enable [enable-&lt;option_name&gt;] enables option_name.\n\n* loose [loose-&lt;option_name&gt;] - mysqld will not exit with an error if it does not recognize option_name, but instead it will issue only a warning.\n\n* maximum [maximum-&lt;option_name&gt;=&lt;value&gt;] indicates that a client can not set the value of option_name greater than the limit specified. If the client does attempt to set the value of option_name greater than the limit, the option_name will simply be set to the defined limit. This option modifier does not work for non-numeric system variables.\n\n* skip [skip-&lt;option_name&gt;] skips or ignores option_name.\n</code></pre> <p>In order to offer more control over option visibility, access and range limits, the following new option modifiers have been added by Percona Server for MySQL:</p> <pre><code>* minimum [minimum-&lt;option_name&gt;=&lt;value&gt;] indicates that clients can not set the value of option_name to less than the limit specified. If the client does attempt to set the value of option_name lesser than the limit, the option_name will simply be set to the defined limit. This option modifier does not work for non-numeric system variables.\n\n* hidden [hidden-&lt;option_name&gt;=&lt;TRUE/FALSE&gt;] indicates that clients can not see or modify the value of option_name.\n\n* readonly [readonly-&lt;option_name&gt;=&lt;TRUE/FALSE&gt;] indicates that clients can see the value of option_name but can not modify the value.\n</code></pre>"},{"location":"management/expanded_program_option_modifiers.html#combining-the-options","title":"Combining the options","text":"<p>Some of the option modifiers may be used together in the same option specification, example:</p> <pre><code>--skip-loose-&lt;option_name&gt;\n--loose-readonly-&lt;option_name&gt;=&lt;T/F&gt;\n--readonly-&lt;option_name&gt;=&lt;T/F&gt;\n--hidden-&lt;option_name&gt;=&lt;T/F&gt;\n</code></pre>"},{"location":"management/expanded_program_option_modifiers.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"management/expanded_program_option_modifiers.html#examples","title":"Examples","text":"<p>Adding the following option to the <code>my.cnf</code> will set the minimum limit on query_cache_size</p> <pre><code>minimum-query_cache_size = 4M\n</code></pre> <p>Trying to set up bigger value will work correctly, but if we try to set it up with smaller than the limit, defined minimum limit will be used and warning (1292) will be issued:</p> <p>Initial query_cache_size size:</p> <pre><code>mysql&gt; SHOW variables LIKE 'query_cache_size';\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>+------------------+---------+\n| Variable_name    | Value   |\n+------------------+---------+\n| query_cache_size | 8388608 |\n+------------------+---------+\n1 row in set (0.00 sec)\n</code></pre> <p>Setting up bigger value:</p> <pre><code>mysql&gt; SET global query_cache_size=16777216;\n</code></pre> <p>The output should be similar to the following:</p> <p><pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <pre><code>mysql&gt; SHOW variables LIKE 'query_cache_size';\n</code></pre></p> <p>The output should be similar to the following:</p> <pre><code>+------------------+----------+\n| Variable_name    | Value    |\n+------------------+----------+\n| query_cache_size | 16777216 |\n+------------------+----------+\n1 row in set (0.00 sec)\n</code></pre> <p>Setting up smaller value:</p> <pre><code>mysql&gt; SET global query_cache_size=1048576;\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Query OK, 0 rows affected, 1 warning (0.00 sec)\n</code></pre> <pre><code>mysql&gt; SHOW warnings;\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>+---------+------+-------------------------------------------------------+\n| Level   | Code | Message                                               |\n+---------+------+-------------------------------------------------------+\n| Warning | 1292 | Truncated incorrect query_cache_size value: '1048576' |\n+---------+------+-------------------------------------------------------+\n1 row in set (0.00 sec)\n</code></pre> <pre><code>mysql&gt; SHOW variables LIKE 'query_cache_size';\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>+------------------+---------+\n| Variable_name    | Value   |\n+------------------+---------+\n| query_cache_size | 4194304 |\n+------------------+---------+\n1 row in set (0.00 sec)\n</code></pre> <p>Adding following option to <code>my.cnf</code> will make query_cache_size hidden.</p> <pre><code>hidden-query_cache_size=1\n</code></pre> <pre><code>mysql&gt; SHOW variables LIKE 'query_cache%';\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>+------------------------------+---------+\n| Variable_name                | Value   |\n+------------------------------+---------+\n| query_cache_limit            | 1048576 |\n| query_cache_min_res_unit     | 4096    |\n| query_cache_strip_comments   | OFF     |\n| query_cache_type             | ON      |\n| query_cache_wlock_invalidate | OFF     |\n+------------------------------+---------+\n5 rows in set (0.00 sec)\n</code></pre> <p>Adding following option to <code>my.cnf</code> will make query_cache_size read-only</p> <pre><code>readonly-query_cache_size=1\n</code></pre> <p>Trying to change the variable value will result in error:</p> <pre><code>mysql&gt; SHOW variables LIKE 'query_cache%';\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>+------------------------------+---------+\n| Variable_name                | Value   |\n+------------------------------+---------+\n| query_cache_limit            | 1048576 |\n| query_cache_min_res_unit     | 4096    |\n| query_cache_size             | 8388608 |\n| query_cache_strip_comments   | OFF     |\n| query_cache_type             | ON      |\n| query_cache_wlock_invalidate | OFF     |\n+------------------------------+---------+\n6 rows in set (0.00 sec)\n</code></pre> <pre><code>mysql&gt; SET global query_cache_size=16777216;\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>ERROR 1238 (HY000): Variable 'query_cache_size' is a read only variable\n</code></pre>"},{"location":"management/extended_show_grants.html","title":"Extended SHOW GRANTS","text":"<p>In Oracle MySQL <code>SHOW GRANTS</code> displays only the privileges granted explicitly to the named account. Other privileges might be available to the account, but they are not displayed. For example, if an anonymous account exists, the named account might be able to use its privileges, but <code>SHOW GRANTS</code> will not display them. In Percona Server for MySQL <code>SHOW GRANTS</code> command was extended to display all the effectively available privileges to the account.</p>"},{"location":"management/extended_show_grants.html#example","title":"Example","text":"<p>If we create the following users:</p> <pre><code>mysql&gt; CREATE USER grantee@localhost IDENTIFIED BY 'grantee1';\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Query OK, 0 rows affected (0.50 sec)\n</code></pre> <pre><code>mysql&gt; CREATE USER grantee IDENTIFIED BY 'grantee2';\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Query OK, 0 rows affected (0.09 sec)\n</code></pre> <pre><code>mysql&gt; CREATE DATABASE db2;\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Query OK, 1 row affected (0.20 sec)\n</code></pre> <pre><code>mysql&gt; GRANT ALL PRIVILEGES ON db2.* TO grantee WITH GRANT OPTION;\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Query OK, 0 rows affected (0.12 sec)\n</code></pre> <ul> <li><code>SHOW GRANTS</code> output before the change:</li> </ul> <pre><code>mysql&gt; SHOW GRANTS;\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>+----------------------------------------------------------------------------------------------------------------+\n| Grants for grantee@localhost                                                                                   |\n+----------------------------------------------------------------------------------------------------------------+\n| GRANT USAGE ON *.* TO 'grantee'@'localhost' IDENTIFIED BY PASSWORD '*9823FF338D44DAF02422CF24DD1F879FB4F6B232' |\n+----------------------------------------------------------------------------------------------------------------+\n1 row in set (0.04 sec)\n</code></pre> <p>Although the grant for the <code>db2</code> database isn\u2019t shown, <code>grantee</code> user has enough privileges to create the table in that database:</p> <pre><code>user@trusty:~$ mysql -ugrantee -pgrantee1 -h localhost\n</code></pre> <pre><code>mysql&gt; CREATE TABLE db2.t1(a int);\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Query OK, 0 rows affected (1.21 sec)\n</code></pre> <ul> <li><code>SHOW GRANTS</code> output after the change shows all the privileges for the <code>grantee</code> user:</li> </ul> <pre><code>mysql&gt; SHOW GRANTS;\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>+----------------------------------------------------------------------------------------------------------------+\n| Grants for grantee@localhost                                                                                   |\n+----------------------------------------------------------------------------------------------------------------+\n| GRANT USAGE ON *.* TO 'grantee'@'localhost' IDENTIFIED BY PASSWORD '*9823FF338D44DAF02422CF24DD1F879FB4F6B232' |\n| GRANT ALL PRIVILEGES ON `db2`.* TO 'grantee'@'%' WITH GRANT OPTION                                             |\n+----------------------------------------------------------------------------------------------------------------+\n2 rows in set (0.00 sec)\n</code></pre>"},{"location":"management/extended_show_grants.html#version-specific-information","title":"Version-Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"management/extended_show_grants.html#other-reading","title":"Other reading","text":"<ul> <li>#53645 - <code>SHOW GRANTS</code> not displaying all the applicable grants</li> </ul>"},{"location":"management/innodb_expanded_fast_index_creation.html","title":"Expanded Fast Index Creation","text":"<p>Note</p> <p>This feature implementation is considered BETA quality.</p> <p>Percona has implemented several changes related to MySQL\u2019s fast index creation feature. Fast index creation was implemented in MySQL as a way to speed up the process of adding or dropping indexes on tables with many rows.</p> <p>This feature implements a session variable that enables extended fast index creation. Besides optimizing DDL directly, expand_fast_index_creation may also optimize index access for subsequent DML statements because using it results in much less fragmented indexes.</p>"},{"location":"management/innodb_expanded_fast_index_creation.html#mysqldump","title":"mysqldump","text":"<p>A new option, <code>--innodb-optimize-keys</code>, was implemented in mysqldump. It changes the way InnoDB tables are dumped, so that secondary and foreign keys are created after loading the data, thus taking advantage of fast index creation. More specifically:</p> <ul> <li> <p><code>KEY</code>, <code>UNIQUE KEY</code>, and <code>CONSTRAINT</code> clauses are omitted from <code>CREATE TABLE</code> statements corresponding to InnoDB tables.</p> </li> <li> <p>An additional <code>ALTER TABLE</code> is issued after dumping the data, in order to create the previously omitted keys.</p> </li> </ul>"},{"location":"management/innodb_expanded_fast_index_creation.html#alter-table","title":"<code>ALTER TABLE</code>","text":"<p>When <code>ALTER TABLE</code> requires a table copy, secondary keys are now dropped and recreated later, after copying the data. The following restrictions apply:</p> <ul> <li> <p>Only non-unique keys can be involved in this optimization.</p> </li> <li> <p>If the table contains foreign keys, or a foreign key is being added as a part of the current <code>ALTER TABLE</code> statement, the optimization is disabled for all keys.</p> </li> </ul>"},{"location":"management/innodb_expanded_fast_index_creation.html#optimize-table","title":"<code>OPTIMIZE TABLE</code>","text":"<p>Internally, <code>OPTIMIZE TABLE</code> is mapped to <code>ALTER TABLE ... ENGINE=innodb</code> for InnoDB tables. As a consequence, it now also benefits from fast index creation, with the same restrictions as for <code>ALTER TABLE</code>.</p>"},{"location":"management/innodb_expanded_fast_index_creation.html#caveats","title":"Caveats","text":"<p>InnoDB fast index creation uses temporary files in tmpdir for all indexes being created. So make sure you have enough tmpdir space when using expand_fast_index_creation. It is a session variable, so you can temporarily switch it off if you are short on tmpdir space and/or don\u2019t want this optimization to be used for a specific table.</p> <p>There\u2019s also a number of cases when this optimization is not applicable:</p> <pre><code>* `UNIQUE` indexes in `ALTER TABLE` are ignored to enforce uniqueness where necessary when copying the data to a temporary table;\n\n\n* `ALTER TABLE` and `OPTIMIZE TABLE` always process tables containing foreign keys as if expand_fast_index_creation is OFF to avoid dropping keys that are part of a FOREIGN KEY constraint;\n\n\n* **mysqldump --innodb-optimize-keys** ignores foreign keys because InnoDB requires a full table rebuild on foreign key changes. So adding them back with a separate `ALTER TABLE` after restoring the data from a dump would actually make the restore slower;\n\n\n* **mysqldump --innodb-optimize-keys** ignores indexes on `AUTO_INCREMENT` columns, because they must be indexed, so it is impossible to temporarily drop the corresponding index;\n\n\n* **mysqldump --innodb-optimize-keys** ignores the first UNIQUE index on non-nullable columns when the table has no `PRIMARY KEY` defined, because in this case InnoDB picks such an index as the clustered one.\n</code></pre>"},{"location":"management/innodb_expanded_fast_index_creation.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"management/innodb_expanded_fast_index_creation.html#system-variables","title":"System Variables","text":""},{"location":"management/innodb_expanded_fast_index_creation.html#expand_fast_index_creation","title":"expand_fast_index_creation","text":"Option Description Command-line Yes Config file No Scope Local/Global Dynamic Yes Data type Boolean Default ON/OFF"},{"location":"management/innodb_expanded_fast_index_creation.html#other-reading","title":"Other Reading","text":"<ul> <li> <p>Improved InnoDB fast index creation</p> </li> <li> <p>Thinking about running OPTIMIZE on your InnoDB Table? Stop!</p> </li> </ul>"},{"location":"management/innodb_kill_idle_trx.html","title":"Kill Idle Transactions","text":"<p>This feature limits the age of idle transactions, for all transactional storage engines. If a transaction is idle for more seconds than the threshold specified, it will be killed. This prevents users from blocking InnoDB purge by mistake.</p> <p>In Percona Server for MySQL Percona Server for MySQL 5.7.17-11 this feature has been re-implemented by setting a connection socket read timeout value instead of periodically scanning the internal InnoDB transaction list.</p>"},{"location":"management/innodb_kill_idle_trx.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li> <p>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6.</p> </li> <li> <p>Percona Server for MySQL 5.7.17-11: Feature re-implemented using socket timeouts.</p> </li> </ul>"},{"location":"management/innodb_kill_idle_trx.html#system-variables","title":"System Variables","text":""},{"location":"management/innodb_kill_idle_trx.html#innodb_kill_idle_transaction","title":"<code>innodb_kill_idle_transaction</code>","text":"Option Description Config file <code>YES</code> Scope <code>GLOBAL</code> Dynamic <code>YES</code> Data type <code>INTEGER</code> Default 0 (disabled) Units Seconds <p>Starting with Percona Server for MySQL 5.7.17-11 this variable is an alias of kill_idle_transaction. To enable this feature, set this variable to the desired seconds wait until the transaction is killed. </p> <p>NOTE: This variable has been deprecated and it will be removed in a future major release.</p>"},{"location":"management/innodb_kill_idle_trx.html#kill_idle_transaction","title":"<code>kill_idle_transaction</code>","text":"Option Description Config file <code>YES</code> Scope <code>GLOBAL</code> Dynamic <code>YES</code> Data type <code>INTEGER</code> Default 0 (disabled) Units Seconds <p>The variable has been implemented in Percona Server for MySQL 5.7.17-11. If non-zero, any idle transaction will be killed after being idle for this many seconds.</p>"},{"location":"management/ps-admin.html","title":"PS-Admin script","text":"<p>You can use the ps-admin script allows Enabling the TokuDB Storage Engine and Percona TokuBackup. If the TokuDB storage engine enables the transparent huge pages, the script adds the thp-setting=never option to my.cnf to disable transparent huge pages on runtime.</p> <p>An example of enabling the TokuDB plugin follows:</p> <pre><code>$ sudo ps-admin --enable-tokudb -u root -pPassw0rd\n</code></pre> <p>The following example enables the TokuBackup.</p> <pre><code>$ sudo ps-admin --enable-tokubackup\n</code></pre> <p>You are able to Enable MyRocks with ps-admin and disable and uninstall the MyRocks storage engine.</p> <p>An example of the enabling and disabling the MyRocksDB plugin follows:</p> <pre><code>$ sudo ps-admin --enable-rocksdb -u root -pPassw0rd\n</code></pre> <pre><code>$ sudo ps-admin --disable-rocksdb -u root -pPassw0rd\n</code></pre> <p>The ps-admin script can also enable or disable the following:</p> <ul> <li> <p>Audit Log plugin</p> </li> <li> <p>PAM Authentication Plugin</p> </li> <li> <p>Query Reponse Time plugin</p> </li> <li> <p>MYSQLX plugin</p> </li> </ul> <p>An example of enabling the PAM Authentication plugin follows:</p> <pre><code>$ sudo ps-admin --enable-pam -u root -pPassw0rd\n</code></pre>"},{"location":"management/start_transaction_with_consistent_snapshot.html","title":"Start transaction with consistent snapshot","text":"<p>Percona Server for MySQL has ported MariaDB enhancement for <code>START TRANSACTION WITH CONSISTENT SNAPSHOTS</code> feature to MySQL 5.6 group commit implementation. This enhancement makes binary log positions consistent with InnoDB transaction snapshots.</p> <p>This feature is quite useful to obtain logical backups with correct positions without running a <code>FLUSH TABLES WITH READ LOCK</code>. Binary log position can be obtained by two newly implemented status variables: Binlog_snapshot_file and Binlog_snapshot_position. After starting a transaction using the <code>START TRANSACTION WITH CONSISTENT SNAPSHOT</code>, these two variables will provide you with the binlog position corresponding to the state of the database of the consistent snapshot so taken, irrespectively of which other transactions have been committed since the snapshot was taken.</p>"},{"location":"management/start_transaction_with_consistent_snapshot.html#snapshot-cloning","title":"Snapshot Cloning","text":"<p>The Percona Server for MySQL implementation extends the <code>START TRANSACTION WITH CONSISTENT SNAPSHOT</code> syntax with the optional <code>FROM SESSION</code> clause:</p> <pre><code>START TRANSACTION WITH CONSISTENT SNAPSHOT FROM SESSION &lt;session_id&gt;;\n</code></pre> <p>When specified, all participating storage engines and binary log instead of creating a new snapshot of data (or binary log coordinates), create a copy of the snapshot which has been created by an active transaction in the specified session. <code>session_id</code> is the session identifier reported in the <code>Id</code> column of <code>SHOW PROCESSLIST</code>.</p> <p>Currently snapshot cloning is only supported by XtraDB and the binary log. As with the regular <code>START TRANSACTION WITH CONSISTENT SNAPSHOT</code>, snapshot clones can only be created with the <code>REPEATABLE READ</code> isolation level.</p> <p>For XtraDB, a transaction with a cloned snapshot will only see data visible or changed by the donor transaction. That is, the cloned transaction will see no changes committed by transactions that started after the donor transaction, not even changes made by itself. Note that in case of chained cloning the donor transaction is the first one in the chain. For example, if transaction A is cloned into transaction B, which is in turn cloned into transaction C, the latter will have read view from transaction A (i.e. the donor transaction). Therefore, it will see changes made by transaction A, but not by transaction B.</p>"},{"location":"management/start_transaction_with_consistent_snapshot.html#mysqldump","title":"mysqldump","text":"<p><code>mysqldump</code> has been updated to use new status variables automatically when they are supported by the server and both <code>--single-transaction</code> and <code>--master-data</code> are specified on the command line. Along with the <code>mysqldump</code> improvements introduced in Backup Locks there is now a way to generate <code>mysqldump</code> backups that are guaranteed to be consistent without using <code>FLUSH TABLES WITH READ LOCK</code> even if <code>--master-data</code> is requested.</p>"},{"location":"management/start_transaction_with_consistent_snapshot.html#system-variables","title":"System Variables","text":""},{"location":"management/start_transaction_with_consistent_snapshot.html#have_snapshot_cloning","title":"<code>have_snapshot_cloning</code>","text":"Option Description Command-line Yes Config file No Scope Global Dynamic No Data type Boolean <p>This server variable is implemented to help other utilities detect if the server supports the <code>FROM SESSION</code> extension. When available, the snapshot cloning feature and the syntax extension to <code>START TRANSACTION WITH CONSISTENT SNAPSHOT</code> are supported by the server, and the variable value is always <code>YES</code>.</p>"},{"location":"management/start_transaction_with_consistent_snapshot.html#status-variables","title":"Status Variables","text":""},{"location":"management/start_transaction_with_consistent_snapshot.html#binlog_snapshot_file","title":"<code>Binlog_snapshot_file</code>","text":"Option Description Scope Global Data type String"},{"location":"management/start_transaction_with_consistent_snapshot.html#binlog_snapshot_position","title":"<code>Binlog_snapshot_position</code>","text":"Option Description Scope Global Data type Numeric <p>These status variables are only available when the binary log is enabled globally.</p>"},{"location":"management/start_transaction_with_consistent_snapshot.html#other-reading","title":"Other Reading","text":"<ul> <li>MariaDB Enhancements for START TRANSACTION WITH CONSISTENT SNAPSHOT</li> </ul>"},{"location":"management/udf_percona_toolkit.html","title":"Percona Toolkit UDFs","text":"<p>Three Percona Toolkit UDFs that provide faster checksums are provided:</p> <ul> <li> <p><code>libfnv1a_udf</code></p> </li> <li> <p><code>libfnv_udf</code></p> </li> <li> <p><code>libmurmur_udf</code></p> </li> </ul>"},{"location":"management/udf_percona_toolkit.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"management/udf_percona_toolkit.html#other-information","title":"Other Information","text":"<ul> <li>Author / Origin: Baron Schwartz</li> </ul>"},{"location":"management/udf_percona_toolkit.html#installation","title":"Installation","text":"<p>These UDFs are part of the Percona Server for MySQL packages. To install one of the UDFs into the server, execute one of the following commands, depending on which UDF you want to install:</p> <pre><code>mysql -e \"CREATE FUNCTION fnv1a_64 RETURNS INTEGER SONAME 'libfnv1a_udf.so'\"\nmysql -e \"CREATE FUNCTION fnv_64 RETURNS INTEGER SONAME 'libfnv_udf.so'\"\nmysql -e \"CREATE FUNCTION murmur_hash RETURNS INTEGER SONAME 'libmurmur_udf.so'\"\n</code></pre> <p>Executing each of these commands will install its respective UDF into the server.</p>"},{"location":"management/udf_percona_toolkit.html#troubleshooting","title":"Troubleshooting","text":"<p>If you get the error:</p> <pre><code>ERROR 1126 (HY000): Can't open shared library 'fnv_udf.so' (errno: 22 fnv_udf.so: cannot open shared object file: No such file or directory)\n</code></pre> <p>Then you may need to copy the .so file to another location in your system. Try both <code>/lib</code> and <code>/usr/lib</code>. Look at your environment\u2019s <code>$LD_LIBRARY_PATH</code> variable for clues. If none is set, and neither <code>/lib</code> nor <code>/usr/lib</code> works, you may need to set <code>LD_LIBRARY_PATH</code> to <code>/lib</code> or <code>/usr/lib</code>.</p>"},{"location":"management/udf_percona_toolkit.html#other-reading","title":"Other Reading","text":"<ul> <li>Percona Toolkit documentation</li> </ul>"},{"location":"management/utility_user.html","title":"Utility user","text":"<p>Percona Server for MySQL has implemented ability to have a MySQL user who has system access to do administrative tasks but limited access to user schema. This feature is especially useful to those operating MySQL As A Service.</p> <p>This user has a mixed and special scope of abilities and protection:</p> <ul> <li> <p>Utility user will not appear in the mysql.user table and can not be modified by any other user, including root.</p> </li> <li> <p>Utility user will not appear in INFORMATION_SCHEMA.USER_STATISTICS, INFORMATION_SCHEMA.CLIENT_STATISTICS or THREAD_STATISTICS tables or in any performance_schema tables.</p> </li> <li> <p>Utility user\u2019s queries may appear in the general and slow logs.</p> </li> <li> <p>Utility user doesn\u2019t have the ability create, modify, delete or see any schemas or data not specified (except for information_schema).</p> </li> <li> <p>Utility user may modify all visible, non read-only system variables (see Expanded Program Option Modifiers functionality).</p> </li> <li> <p>Utility user may see, create, modify and delete other system users only if given access to the mysql schema.</p> </li> <li> <p>Regular users may be granted proxy rights to the utility user but any attempt to impersonate the utility user will fail. The utility user may not be granted proxy rights on any regular user. For example running: GRANT PROXY ON utility_user TO regular_user; will not fail, but any actual attempt to impersonate as the utility user will fail. Running: GRANT PROXY ON regular_user TO utility_user; will fail when utility_user is an exact match or is more specific than than the utility user specified.</p> </li> </ul> <p>When the server starts, it will note in the log output that the utility user exists and the schemas that it has access to.</p> <p>In order to have the ability for a special type of MySQL user, which will have a very limited and special amount of control over the system and can not be see or modified by any other user including the root user, three new options have been added.</p> <p>Option utility_user specifies the user which the system will create and recognize as the utility user. The host in the utility user specification follows conventions described in the MySQL manual, i.e. it allows wildcards and IP masks. Anonymous user names are not permitted to be used for the utility user name.</p> <p>This user must not be an exact match to any other user that exists in the mysql.user table. If the server detects that the user specified with this option exactly matches any user within the mysql.user table on start up, the server will report an error and shut down gracefully. If host name wildcards are used and a more specific user specification is identified on start up, the server will report a warning and continue.</p> <p>Example: <code>--utility_user</code> =frank@% and frank@localhost exists within the mysql.user table.</p> <p>If a client attempts to create a MySQL user that matches this user specification exactly or if host name wildcards are used for the utility user and the user being created has the same name and a more specific host, the creation attempt will fail with an error.</p> <p>Example: <code>--utility_user</code> =frank@% and CREATE USER \u2018frank@localhost\u2019;</p> <p>As a result of these requirements, it is strongly recommended that a very unique user name and reasonably specific host be used and that any script or tools test that they are running within the correct user by executing \u2018SELECT CURRENT_USER()\u2019 and comparing the result against the known utility user.</p> <p>Option utility_user_password specifies the password for the utility user and MUST be specified or the server will shut down gracefully with an error.</p> <p>Example: <code>--utility_user_password</code> =`Passw0rD`</p> <p>Option utility_user_schema_access specifies the name(s) of the schema(s) that the utility user will have access to read write and modify. If a particular schema named here does not exist on start up it will be ignored. If a schema by the name of any of those listed in this option is created after the server is started, the utility user will have full access to it.</p> <p>Example: <code>--utility_user_schema_access</code> =schema1,schema2,schema3</p> <p>Option utility_user_privileges allows a comma-separated list of extra access privileges to grant to the utility user.</p> <p>Example: <code>--utility-user-privileges</code> =\u201dCREATE,DROP,LOCK TABLES\u201d</p>"},{"location":"management/utility_user.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"management/utility_user.html#system-variables","title":"System Variables","text":""},{"location":"management/utility_user.html#utility_user","title":"<code>utility_user</code>","text":"Option Description Command-line Yes Config file utility_user=user@host Scope Global Dynamic No Data type String Default NULL <p>Specifies a MySQL user that will be added to the internal list of users and recognized as the utility user.</p>"},{"location":"management/utility_user.html#utility_user_password","title":"<code>utility_user_password</code>","text":"Option Description Command-line Yes Config file utility_user_password= Scope Global Dynamic No Data type String Default NULL <p>Specifies the password required for the utility user.</p>"},{"location":"management/utility_user.html#utility_user_schema_access","title":"<code>utility_user_schema_access</code>","text":"Option Description Command-line Yes Config file utility_user_schema_access=,, Scope Global Dynamic No Data type String Default NULL <p>Specifies the schemas that the utility user has access to in a comma delimited list.</p>"},{"location":"management/utility_user.html#utility_user_privileges","title":"<code>utility_user_privileges</code>","text":"Option Description Command-line Yes Config file utility_user_privileges=,, Scope Global Dynamic No Data type String Default NULL <p>This variable can be used to specify a comma-separated list of extra access privileges to grant to the utility user. Supported values for the privileges list are: <code>SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, SHUTDOWN, PROCESS, FILE, GRANT, REFERENCES, INDEX, ALTER, SHOW DATABASES, SUPER, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, CREATE USER, EVENT, TRIGGER, CREATE TABLESPACE</code></p>"},{"location":"myrocks/index.html","title":"Percona MyRocks Introduction","text":"<p>MyRocks is a storage engine for MySQL based on RocksDB, an embeddable, persistent key-value store. Percona MyRocks is an implementation for Percona Server.</p> <p>The RocksDB store is based on the log-structured merge-tree (or LSM tree). It is optimized for fast storage and combines outstanding space and write efficiency with acceptable read performance. As a result, MyRocks has the following advantages compared to other storage engines, if your workload uses fast storage, such as SSD:</p> <ul> <li> <p>Requires less storage space</p> </li> <li> <p>Provides more storage endurance</p> </li> <li> <p>Ensures better IO capacity</p> </li> <li> <p>Percona MyRocks Installation Guide</p> </li> <li> <p>MyRocks Limitations</p> </li> <li> <p>Differences between Percona MyRocks and Facebook MyRocks</p> </li> <li> <p>MyRocks Server Variables</p> </li> <li> <p>MyRocks Information Schema Tables</p> </li> </ul>"},{"location":"myrocks/data_loading.html","title":"Data Loading","text":"<p>By default, MyRocks configurations are optimized for short transactions, and not for data loading. MyRocks has a couple of special session variables to speed up data loading dramatically.</p>"},{"location":"myrocks/data_loading.html#sorted-bulk-loading","title":"Sorted bulk loading","text":"<p>If your data is guaranteed to be loaded in primary key order, then this method is recommended. This method works by dropping any secondary keys first, loading data into your table in primary key order, and then restoring the secondary keys via Fast Secondary Index Creation.</p>"},{"location":"myrocks/data_loading.html#creating-secondary-indexes","title":"Creating Secondary Indexes","text":"<p>When loading data into empty tables, it is highly recommended to drop all secondary indexes first, then loading data, and adding all secondary indexes after finishing loading data. MyRocks has a feature called <code>Fast Secondary Index Creation</code>. Fast Secondary Index Creation is automatically used when executing <code>CREATE INDEX</code> or <code>ALTER TABLE ... ADD INDEX</code>. With Fast Secondary Index Creation, the secondary index entries are directly written to bottommost RocksDB levels and bypassing compaction. This significantly reduces total write volume and CPU time for decompressing and compressing data on higher levels.</p>"},{"location":"myrocks/data_loading.html#loading-data","title":"Loading Data","text":"<p>As described above, loading data is highly recommended for tables with primary key only (no secondary keys), with all secondary indexes added after loading data.</p> <p>When loading data into MyRocks tables, there are two recommended session variables:</p> <pre><code>SET session sql_log_bin=0;\nSET session rocksdb_bulk_load=1;\n</code></pre> <p>When converting from large MyISAM/InnoDB tables, either by using the <code>ALTER</code> or <code>INSERT INTO SELECT</code> statements it\u2019s recommended that you create MyRocks tables as below (in case the table is sufficiently big it will cause the server to consume all the memory and then be terminated by the OOM killer):</p> <pre><code>SET session sql_log_bin=0;\nSET session rocksdb_bulk_load=1;\nALTER TABLE large_myisam_table ENGINE=RocksDB;\nSET session rocksdb_bulk_load=0;\n</code></pre> <p>Using sql_log_bin=0 avoids writing to binary logs.</p> <p>With rocksdb_bulk_load set to <code>1</code>, MyRocks enters special mode to write all inserts into bottommost RocksDB levels, and skips writing data into MemTable and the following compactions. This is very efficient way to load data.</p> <p>The rocksdb_bulk_load mode operates with a few conditions:</p> <ul> <li> <p>None of the data being bulk loaded can overlap with existing data in the table. The easiest way to ensure this is to always bulk load into an empty table, but the mode will allow loading some data into the table, doing other operations, and then returning and bulk loading addition data if there is no overlap between what is being loaded and what already exists.</p> </li> <li> <p>The data may not be visible until bulk load mode is ended (i.e. the rocksdb_bulk_load is set to zero again). The method that is used is building up SST files which will later be added as-is to the database. Until a particular SST has been added the data will not be visible to the rest of the system, thus issuing a <code>SELECT</code> on the table currently being bulk loaded will only show older data and will likely not show the most recently added rows. Ending the bulk load mode will cause the most recent SST file to be added. When bulk loading multiple tables, starting a new table will trigger the code to add the most recent SST file to the system \u2013 as a result, it is inadvisable to interleave <code>INSERT</code> statements to two or more tables during bulk load mode.</p> </li> </ul> <p>By default, the rocksdb_bulk_load mode expects all data be inserted in primary key order (or reversed order). If the data is in the reverse order (i.e. the data is descending on a normally ordered primary key or is ascending on a reverse ordered primary key), the rows are cached in chunks to switch the order to match the expected order.</p> <p>Inserting one or more rows out of order will result in an error and may result in some of the data being inserted in the table and some not. To resolve the problem, one can either fix the data order of the insert, truncate the table, and restart.</p>"},{"location":"myrocks/data_loading.html#unsorted-bulk-loading","title":"Unsorted bulk loading","text":"<p>If your data is not ordered in primary key order, then this method is recommended. With this method, secondary keys do not need to be dropped and restored. However, writing to the primary key no longer goes directly to SST files, and are written to temporary files for sorted first, so there is extra cost to this method.</p> <p>To allow for loading unsorted data:</p> <pre><code>SET session sql_log_bin=0;\nSET session rocksdb_bulk_load_allow_unsorted=1;\nSET session rocksdb_bulk_load=1;\n...\nSET session rocksdb_bulk_load=0;\nSET session rocksdb_bulk_load_allow_unsorted=0;\n</code></pre> <p>Note that rocksdb_bulk_load_allow_unsorted can only be changed when rocksdb_bulk_load is disabled (set to <code>0</code>). In this case, all input data will go through an intermediate step that writes the rows to temporary SST files, sorts them rows in the primary key order, and then writes to final SST files in the correct order.</p>"},{"location":"myrocks/data_loading.html#other-approaches","title":"Other Approaches","text":"<p>If rocksdb_commit_in_the_middle is enabled, MyRocks implicitly commits every rocksdb_bulk_load_size records (default is <code>1,000</code>) in the middle of your transaction. If your data loading fails in the middle of the statement (<code>LOAD DATA</code> or bulk <code>INSERT</code>), rows are not entirely rolled back, but some of rows are stored in the table. To restart data loading, you\u2019ll need to truncate the table and loading data again.</p> <p>Warning</p> <p>If you are loading large data without enabling rocksdb_bulk_load or rocksdb_commit_in_the_middle, please make sure transaction size is small enough. All modifications of the ongoing transactions are kept in memory.</p>"},{"location":"myrocks/data_loading.html#other-reading","title":"Other Reading","text":"<ul> <li> <p>Data Loading - this document has been used as a source for writing this documentation</p> </li> <li> <p>ALTER TABLE \u2026 ENGINE=ROCKSDB uses too much memory</p> </li> </ul>"},{"location":"myrocks/differences.html","title":"Differences between Percona MyRocks and Facebook MyRocks","text":"<p>The original MyRocks was developed by Facebook and works with their implementation of MySQL. Percona MyRocks is a branch of MyRocks for Percona Server and includes the following differences from the original implementation:</p> <ul> <li>The behavior of the <code>START TRANSACTION WITH CONSISTENT SNAPSHOT</code> statement depends on the transaction isolation level.</li> </ul> Storage Engine Transaction isolation level <code>READ COMMITTED</code> <code>REPEATABLE READ</code> InnoDB Success Success Facebook MyRocks Fail Success (MyRocks engine only; read-only, as all MyRocks engine snapshots) Percona MyRocks Fail with any DML which would violate the read-only snapshot constraint Success (read-only snapshots independent of the engines in use) <ul> <li>Percona MyRocks includes the <code>lz4</code> and <code>zstd</code> statically linked libraries.</li> </ul>"},{"location":"myrocks/differences.html#compression","title":"Compression","text":"<p>The supported compression algorithms are the following:</p> Compression Algorithm Percona MyRocks Facebook MyRocks Zlib Yes Yes LZ4 Yes Yes ZStd Yes Yes None Yes Yes Snappy No Yes Bzip No Yes"},{"location":"myrocks/gap_locks_detection.html","title":"Gap locks detection","text":"<p>Percona Server for MySQL has implemented Gap locks detection in Percona Server for MySQL 5.7.18-14 based on a Facebook MySQL patch.</p> <p>If a transactional storage engine does not support gap locks (for example MyRocks) and a gap lock is being attempted while the transaction isolation level is either <code>REPEATABLE READ</code> or <code>SERIALIZABLE</code>, the following SQL error will be returned to the client and no actual gap lock will be taken on the effected rows.</p> <pre><code>ERROR HY000: Using Gap Lock without full unique key in multi-table or multi-statement transactions is not allowed. You need to either rewrite queries to use all unique key columns in WHERE equal conditions, or rewrite to single-table, single-statement transaction.\n</code></pre>"},{"location":"myrocks/information-schema-tables.html","title":"MyRocks Information Schema Tables","text":"<p>When you install the MyRocks plugin for MySQL, the Information Schema is extended to include the following tables:</p>"},{"location":"myrocks/information-schema-tables.html#rocksdb_global_info","title":"ROCKSDB_GLOBAL_INFO","text":""},{"location":"myrocks/information-schema-tables.html#columns","title":"Columns","text":"Column Name Type TYPE varchar(513) NAME varchar(513) VALUE varchar(513)"},{"location":"myrocks/information-schema-tables.html#rocksdb_cfstats","title":"ROCKSDB_CFSTATS","text":""},{"location":"myrocks/information-schema-tables.html#columns_1","title":"Columns","text":"Column Name Type CF_NAME varchar(193) STAT_TYPE varchar(193) VALUE bigint(8)"},{"location":"myrocks/information-schema-tables.html#rocksdb_trx","title":"ROCKSDB_TRX","text":"<p>This table stores mappings of RocksDB transaction identifiers to MySQL client identifiers to enable associating a RocksDB transaction with a MySQL client operation.</p>"},{"location":"myrocks/information-schema-tables.html#columns_2","title":"Columns","text":"Column Name Type TRANSACTION_ID bigint(8) STATE varchar(193) NAME varchar(193) WRITE_COUNT bigint(8) LOCK_COUNT bigint(8) TIMEOUT_SEC int(4) WAITING_KEY varchar(513) WAITING_COLUMN_FAMILY_ID int(4) IS_REPLICATION int(4) SKIP_TRX_API int(4) READ_ONLY int(4) HAS_DEADLOCK_DETECTION int(4) NUM_ONGOING_BULKLOAD int(4) THREAD_ID int(8) QUERY varchar(193)"},{"location":"myrocks/information-schema-tables.html#rocksdb_cf_options","title":"ROCKSDB_CF_OPTIONS","text":""},{"location":"myrocks/information-schema-tables.html#columns_3","title":"Columns","text":"Column Name Type CF_NAME varchar(193) OPTION_TYPE varchar(193) VALUE varchar(193)"},{"location":"myrocks/information-schema-tables.html#rocksdb_compaction_stats","title":"ROCKSDB_COMPACTION_STATS","text":""},{"location":"myrocks/information-schema-tables.html#columns_4","title":"Columns","text":"Column Name Type CF_NAME varchar(193) LEVEL varchar(513) TYPE varchar(513) VALUE double"},{"location":"myrocks/information-schema-tables.html#rocksdb_dbstats","title":"ROCKSDB_DBSTATS","text":""},{"location":"myrocks/information-schema-tables.html#columns_5","title":"Columns","text":"Column Name Type STAT_TYPE varchar(193) VALUE bigint(8)"},{"location":"myrocks/information-schema-tables.html#rocksdb_ddl","title":"ROCKSDB_DDL","text":""},{"location":"myrocks/information-schema-tables.html#columns_6","title":"Columns","text":"Column Name Type TABLE_SCHEMA varchar(193) TABLE_NAME varchar(193) PARTITION_NAME varchar(193) INDEX_NAME varchar(193) COLUMN_FAMILY int(4) INDEX_NUMBER int(4) INDEX_TYPE smallint(2) KV_FORMAT_VERSION smallint(2) TTL_DURATION bigint(8) INDEX_FLAGS bigint(8) CF varchar(193) AUTO_INCREMENT bigint(8) unsigned"},{"location":"myrocks/information-schema-tables.html#rocksdb_index_file_map","title":"ROCKSDB_INDEX_FILE_MAP","text":""},{"location":"myrocks/information-schema-tables.html#columns_7","title":"Columns","text":"Column Name Type COLUMN_FAMILY int(4) INDEX_NUMBER int(4) SST_NAME varchar(193) NUM_ROWS bigint(8) DATA_SIZE bigint(8) ENTRY_DELETES bigint(8) ENTRY_SINGLEDELETES bigint(8) ENTRY_MERGES bigint(8) ENTRY_OTHERS bigint(8) DISTINCT_KEYS_PREFIX varchar(400)"},{"location":"myrocks/information-schema-tables.html#rocksdb_locks","title":"ROCKSDB_LOCKS","text":"<p>This table contains the set of locks granted to MyRocks transactions.</p>"},{"location":"myrocks/information-schema-tables.html#columns_8","title":"Columns","text":"Column Name Type COLUMN_FAMILY_ID int(4) TRANSACTION_ID int(4) KEY varchar(513) MODE varchar(32)"},{"location":"myrocks/information-schema-tables.html#rocksdb_perf_context","title":"ROCKSDB_PERF_CONTEXT","text":""},{"location":"myrocks/information-schema-tables.html#columns_9","title":"Columns","text":"Column Name Type TABLE_SCHEMA varchar(193) TABLE_NAME varchar(193) PARTITION_NAME varchar(193) STAT_TYPE varchar(193) VALUE bigint(8)"},{"location":"myrocks/information-schema-tables.html#rocksdb_perf_context_global","title":"ROCKSDB_PERF_CONTEXT_GLOBAL","text":""},{"location":"myrocks/information-schema-tables.html#columns_10","title":"Columns","text":"Column Name Type STAT_TYPE varchar(193) VALUE bigint(8)"},{"location":"myrocks/information-schema-tables.html#rocksdb_deadlock","title":"ROCKSDB_DEADLOCK","text":"<p>This table records information about deadlocks.</p>"},{"location":"myrocks/information-schema-tables.html#columns_11","title":"Columns","text":"Column Name Type DEADLOCK_ID bigint(8) TRANSACTION_ID bigint(8) CF_NAME varchar(193) WAITING_KEY varchar(513) LOCK_TYPE varchar(193) INDEX_NAME varchar(193) TABLE_NAME varchar(193) ROLLED_BACK bigint(8)"},{"location":"myrocks/install.html","title":"Percona MyRocks Installation Guide","text":"<p>Percona MyRocks is distributed as a separate package that can be enabled as a plugin for Percona Server 5.7 and later versions.</p> <p>Note</p> <p>File formats across different MyRocks variants may not be compatible. Percona Server supports only Percona MyRocks. Migrating from one variant to another requires a logical data dump and reload.</p>"},{"location":"myrocks/install.html#installing-percona-myrocks","title":"Installing Percona MyRocks","text":"<p>It is recommended to install Percona software from official repositories:</p> <ol> <li> <p>Configure Percona repositories as described in Percona Software Repositories Documentation.</p> </li> <li> <p>Install Percona MyRocks using the corresponding package manager:</p> <ul> <li>For Debian or Ubuntu:</li> </ul> <pre><code>$ sudo apt install percona-server-rocksdb-5.7\n</code></pre> <ul> <li>For RHEL or CentOS:</li> </ul> <pre><code>$ sudo yum install Percona-Server-rocksdb-57.x86_64\n</code></pre> </li> </ol> <p>After you install the Percona MyRocks package, you should see the following output:</p> <pre><code>* This release of Percona Server is distributed with RocksDB storage engine.\n* Run the following script to enable the RocksDB storage engine in Percona Server:\n\n       ps-admin --enable-rocksdb -u &lt;mysql_admin_user&gt; -p[mysql_admin_pass] [-S &lt;socket&gt;] [-h &lt;host&gt; -P &lt;port&gt;]\n</code></pre>"},{"location":"myrocks/install.html#enable-myrocks-with-ps-admin","title":"Enable MyRocks with ps-admin","text":"<p>Run the <code>ps-admin</code> script as system root user or with sudo and provide the MySQL root user credentials to properly enable the RocksDB (MyRocks) storage engine:</p> <pre><code>$ sudo ps-admin --enable-rocksdb -u root -pPassw0rd\n</code></pre> <p>You should see the following output:</p> <pre><code>Checking if RocksDB plugin is available for installation ...\nINFO: ha_rocksdb.so library for RocksDB found at /usr/lib64/mysql/plugin/ha_rocksdb.so.\n\nChecking RocksDB engine plugin status...\nINFO: RocksDB engine plugin is not installed.\n\nInstalling RocksDB engine...\nINFO: Successfully installed RocksDB engine plugin.\n</code></pre> <p>Note</p> <p>Running the <code>ps-admin</code> script to enable Percona MyRocks also installs and enables the RocksDB plugin.</p> <p>If the script returns no errors, Percona MyRocks should be successfully enabled on the server. You can verify it as follows:</p> <pre><code>mysql&gt; SHOW ENGINES;\n</code></pre> <p>The output could be the following:</p> <pre><code>+---------+---------+----------------------------------------------------------------------------+--------------+------+------------+\n| Engine  | Support | Comment                                                                    | Transactions | XA   | Savepoints |\n+---------+---------+----------------------------------------------------------------------------+--------------+------+------------+\n| ROCKSDB | YES     | RocksDB storage engine                                                     | YES          | YES  | YES        |\n...\n| InnoDB  | DEFAULT | Percona-XtraDB, Supports transactions, row-level locking, and foreign keys | YES          | YES  | YES        |\n+---------+---------+----------------------------------------------------------------------------+--------------+------+------------+\n10 rows in set (0.00 sec)\n</code></pre> <p>Note that the RocksDB engine is not set to be default, new tables will still be created using the InnoDB (XtraDB) storage engine. To make RocksDB storage engine default, set <code>default-storage-engine=rocksdb</code> in the <code>[mysqld]</code> section of <code>my.cnf</code> and restart Percona Server.</p> <p>Alternatively, you can add <code>ENGINE=RocksDB</code> after the <code>CREATE TABLE</code> statement for every table that you create.</p>"},{"location":"myrocks/install.html#installing-myrocks-plugins","title":"Installing MyRocks Plugins","text":"<p>You can install MyRocks manually with a series of INSTALL PLUGIN statements. You must have the <code>INSERT</code> privilege for the <code>mysql.plugin</code> system table.</p> <p>The following statements install MyRocks:</p> <pre><code>INSTALL PLUGIN ROCKSDB SONAME 'ha_rocksdb.so';\nINSTALL PLUGIN ROCKSDB_CFSTATS SONAME 'ha_rocksdb.so';\nINSTALL PLUGIN ROCKSDB_DBSTATS SONAME 'ha_rocksdb.so';\nINSTALL PLUGIN ROCKSDB_PERF_CONTEXT SONAME 'ha_rocksdb.so';\nINSTALL PLUGIN ROCKSDB_PERF_CONTEXT_GLOBAL SONAME 'ha_rocksdb.so';\nINSTALL PLUGIN ROCKSDB_CF_OPTIONS SONAME 'ha_rocksdb.so';\nINSTALL PLUGIN ROCKSDB_GLOBAL_INFO SONAME 'ha_rocksdb.so';\nINSTALL PLUGIN ROCKSDB_COMPACTION_STATS SONAME 'ha_rocksdb.so';\nINSTALL PLUGIN ROCKSDB_DDL SONAME 'ha_rocksdb.so';\nINSTALL PLUGIN ROCKSDB_INDEX_FILE_MAP SONAME 'ha_rocksdb.so';\nINSTALL PLUGIN ROCKSDB_LOCKS SONAME 'ha_rocksdb.so';\nINSTALL PLUGIN ROCKSDB_TRX SONAME 'ha_rocksdb.so';\nINSTALL PLUGIN ROCKSDB_DEADLOCK SONAME 'ha_rocksdb.so';\n</code></pre>"},{"location":"myrocks/install.html#removing-percona-myrocks","title":"Removing Percona MyRocks","text":"<p>It will not be possible to access tables created using the RocksDB engine with another storage engine after you remove Percona MyRocks. If you need this data, alter the tables to another storage engine. For example, to alter the <code>City</code> table to InnoDB, run the following:</p> <pre><code>mysql&gt; ALTER TABLE City ENGINE=InnoDB;\n</code></pre> <p>To disable and uninstall the RocksDB engine plugins, use the <code>ps-admin</code> script as follows:</p> <pre><code>$ sudo ps-admin --disable-rocksdb -u root -pPassw0rd\n</code></pre> <p>You should see the following output:</p> <pre><code>Checking RocksDB engine plugin status...\nINFO: RocksDB engine plugin is installed.\n\nUninstalling RocksDB engine plugin...\nINFO: Successfully uninstalled RocksDB engine plugin.\n</code></pre> <p>After the engine plugins have been uninstalled, remove the Percona MyRocks package:</p> <ul> <li>For Debian or Ubuntu:</li> </ul> <pre><code>$ sudo apt remove percona-server-rocksdb-5.7\n</code></pre> <ul> <li>For RHEL or CentOS:</li> </ul> <pre><code>$ sudo yum remove Percona-Server-rocksdb-57.x86_64\n</code></pre> <p>Finally, remove all the MyRocks Server Variables from the configuration file (<code>my.cnf</code>) and restart Percona Server.</p>"},{"location":"myrocks/install.html#uninstall-myrocks-plugins","title":"Uninstall MyRocks Plugins","text":"<p>You can uninstall the plugins for MyRocks. You must have the <code>DELETE</code> privilege for the <code>mysql.plugin</code> system table.</p> <p>The following statements remove the MyRocks plugins:</p> <pre><code>UNINSTALL PLUGIN ROCKSDB;\nUNINSTALL PLUGIN ROCKSDB_CFSTATS;\nUNINSTALL PLUGIN ROCKSDB_DBSTATS;\nUNINSTALL PLUGIN ROCKSDB_PERF_CONTEXT;\nUNINSTALL PLUGIN ROCKSDB_PERF_CONTEXT_GLOBAL;\nUNINSTALL PLUGIN ROCKSDB_CF_OPTIONS;\nUNINSTALL PLUGIN ROCKSDB_GLOBAL_INFO;\nUNINSTALL PLUGIN ROCKSDB_COMPACTION_STATS;\nUNINSTALL PLUGIN ROCKSDB_DDL;\nUNINSTALL PLUGIN ROCKSDB_INDEX_FILE_MAP;\nUNINSTALL PLUGIN ROCKSDB_LOCKS;\nUNINSTALL PLUGIN ROCKSDB_TRX;\nUNINSTALL PLUGIN ROCKSDB_DEADLOCK;\n</code></pre>"},{"location":"myrocks/limitations.html","title":"MyRocks Limitations","text":"<p>The MyRocks storage engine lacks the following features compared to InnoDB:</p> <ul> <li> <p>Online DDL</p> </li> <li> <p>ALTER TABLE \u2026 EXCHANGE PARTITION</p> </li> <li> <p>SAVEPOINT</p> </li> <li> <p>Transportable tablespace</p> </li> <li> <p>Foreign keys</p> </li> <li> <p>Spatial indexes</p> </li> <li> <p>Fulltext indexes</p> </li> <li> <p>Gap locks</p> </li> <li> <p>Group Replication</p> </li> <li> <p>Generated Columns</p> </li> <li> <p>Partial Update of LOB in InnoDB</p> </li> </ul> <p>You should also consider the following:</p> <ul> <li> <p><code>\\*_bin</code> (e.g. <code>latin1_bin</code>) or binary collation should be used on <code>CHAR</code> and <code>VARCHAR</code> indexed columns. The following binary collations are supported: <code>binary</code>, <code>latin1_bin</code>, and <code>utf8_bin</code>. By default, MyRocks prevents creating indexes with non-binary collations (including <code>latin1</code>). You can optionally use it by setting rocksdb_strict_collation_exceptions to <code>t1</code> (table names with regex format), but non-binary covering indexes other than <code>latin1</code> (excluding <code>german1</code>) still require a primary key lookup to return the <code>CHAR</code> or <code>VARCHAR</code> column.</p> </li> <li> <p>Either <code>ORDER BY DESC</code> or <code>ORDER BY ASC</code> is slow. This is because of \u201cPrefix Key Encoding\u201d feature in RocksDB. See http://www.slideshare.net/matsunobu/myrocks-deep-dive/58 for details. By default, ascending scan is faster and descending scan is slower. If the \u201creverse column family\u201d is configured, then descending scan will be faster and ascending scan will be slower. Note that InnoDB also imposes a cost when the index is scanned in the opposite order.</p> </li> <li> <p>MyRocks does not support operating as either a source or a replica in any replication topology that is not exclusively row-based. Statement-based and mixed-format binary logging is not supported. For more information, see Replication Formats.</p> </li> <li> <p>When converting from large MyISAM/InnoDB tables, either by using the <code>ALTER</code> or <code>INSERT INTO SELECT</code> statements it\u2019s recommended that you check the Data loading documentation and create MyRocks tables as below (in case the table is sufficiently big it will cause the server to consume all the memory and then be terminated by the OOM killer):</p> </li> </ul> <pre><code> SET session sql_log_bin=0;\nSET session rocksdb_bulk_load=1;\nALTER TABLE large_myisam_table ENGINE=RocksDB;\nSET session rocksdb_bulk_load=0;\n</code></pre> <p>You should see the following output:</p> <pre><code>.. warning::\n\n If you are loading large data without enabling :ref:`rocksdb_bulk_load`\n or :ref:`rocksdb_commit_in_the_middle`, please make sure transaction\n size is small enough. All modifications of the ongoing transactions are\n kept in memory.\n</code></pre> <ul> <li> <p>The`XA protocol &lt;https://dev.mysql.com/doc/refman/5.7/en/xa.html&gt;`_ support, which allows distributed transactions combining multiple separate transactional resources, is an experimental feature in MyRocks: the implementation is less tested, it may lack some functionality and be not as stable as in case of InnoDB.</p> </li> <li> <p>MySQL has spatial data types . These data types are not supported by MyRocks.</p> </li> </ul>"},{"location":"myrocks/status_variables.html","title":"MyRocks status variables","text":"<p>MyRocks status variables provide details about the inner workings of the storage engine and they can be useful in tuning the storage engine to a particular environment.</p> <p>You can view these variables and their values by running:</p> <pre><code>mysql&gt; SHOW STATUS LIKE 'rocksdb%';\n</code></pre>"},{"location":"myrocks/status_variables.html#list-of-status-variables","title":"List of status variables","text":"<p>The following global status variables are available:</p> Name Var Type <code>rocksdb_rows_deleted</code> Numeric <code>rocksdb_rows_inserted</code> Numeric <code>rocksdb_rows_read</code> Numeric <code>rocksdb_rows_updated</code> Numeric <code>rocksdb_rows_expired</code> Numeric <code>rocksdb_system_rows_deleted</code> Numeric <code>rocksdb_system_rows_inserted</code> Numeric <code>rocksdb_system_rows_read</code> Numeric <code>rocksdb_system_rows_updated</code> Numeric <code>rocksdb_memtable_total</code> Numeric <code>rocksdb_memtable_unflushed</code> Numeric <code>rocksdb_queries_point</code> Numeric <code>rocksdb_queries_range</code> Numeric <code>rocksdb_covered_secondary_key_lookups</code> Numeric <code>rocksdb_additional_compactions_trigger</code> Numeric <code>rocksdb_block_cache_add</code> Numeric <code>rocksdb_block_cache_add_failures</code> Numeric <code>rocksdb_block_cache_bytes_read</code> Numeric <code>rocksdb_block_cache_bytes_write</code> Numeric <code>rocksdb_block_cache_data_add</code> Numeric <code>rocksdb_block_cache_data_bytes_insert</code> Numeric <code>rocksdb_block_cache_data_hit</code> Numeric <code>rocksdb_block_cache_data_miss</code> Numeric <code>rocksdb_block_cache_filter_add</code> Numeric <code>rocksdb_block_cache_filter_bytes_evict</code> Numeric <code>rocksdb_block_cache_filter_bytes_insert</code> Numeric <code>rocksdb_block_cache_filter_hit</code> Numeric <code>rocksdb_block_cache_filter_miss</code> Numeric <code>rocksdb_block_cache_hit</code> Numeric <code>rocksdb_block_cache_index_add</code> Numeric <code>rocksdb_block_cache_index_bytes_evict</code> Numeric <code>rocksdb_block_cache_index_bytes_insert</code> Numeric <code>rocksdb_block_cache_index_hit</code> Numeric <code>rocksdb_block_cache_index_miss</code> Numeric <code>rocksdb_block_cache_miss</code> Numeric <code>rocksdb_block_cache_compressed_hit</code> Numeric <code>rocksdb_block_cache_compressed_miss</code> Numeric <code>rocksdb_bloom_filter_prefix_checked</code> Numeric <code>rocksdb_bloom_filter_prefix_useful</code> Numeric <code>rocksdb_bloom_filter_useful</code> Numeric <code>rocksdb_bytes_read</code> Numeric <code>rocksdb_bytes_written</code> Numeric <code>rocksdb_compact_read_bytes</code> Numeric <code>rocksdb_compact_write_bytes</code> Numeric <code>rocksdb_compaction_key_drop_new</code> Numeric <code>rocksdb_compaction_key_drop_obsolete</code> Numeric <code>rocksdb_compaction_key_drop_user</code> Numeric <code>rocksdb_flush_write_bytes</code> Numeric <code>rocksdb_get_hit_l0</code> Numeric <code>rocksdb_get_hit_l1</code> Numeric <code>rocksdb_get_hit_l2_and_up</code> Numeric <code>rocksdb_get_updates_since_calls</code> Numeric <code>rocksdb_iter_bytes_read</code> Numeric <code>rocksdb_memtable_hit</code> Numeric <code>rocksdb_memtable_miss</code> Numeric <code>rocksdb_no_file_closes</code> Numeric <code>rocksdb_no_file_errors</code> Numeric <code>rocksdb_no_file_opens</code> Numeric <code>rocksdb_num_iterators</code> Numeric <code>rocksdb_number_block_not_compressed</code> Numeric <code>rocksdb_number_db_next</code> Numeric <code>rocksdb_number_db_next_found</code> Numeric <code>rocksdb_number_db_prev</code> Numeric <code>rocksdb_number_db_prev_found</code> Numeric <code>rocksdb_number_db_seek</code> Numeric <code>rocksdb_number_db_seek_found</code> Numeric <code>rocksdb_number_deletes_filtered</code> Numeric <code>rocksdb_number_keys_read</code> Numeric <code>rocksdb_number_keys_updated</code> Numeric <code>rocksdb_number_keys_written</code> Numeric <code>rocksdb_number_merge_failures</code> Numeric <code>rocksdb_number_multiget_bytes_read</code> Numeric <code>rocksdb_number_multiget_get</code> Numeric <code>rocksdb_number_multiget_keys_read</code> Numeric <code>rocksdb_number_reseeks_iteration</code> Numeric <code>rocksdb_number_sst_entry_delete</code> Numeric <code>rocksdb_number_sst_entry_merge</code> Numeric <code>rocksdb_number_sst_entry_other</code> Numeric <code>rocksdb_number_sst_entry_put</code> Numeric <code>rocksdb_number_sst_entry_singledelete</code> Numeric <code>rocksdb_number_stat_computes</code> Numeric <code>rocksdb_number_superversion_acquires</code> Numeric <code>rocksdb_number_superversion_cleanups</code> Numeric <code>rocksdb_number_superversion_releases</code> Numeric <code>rocksdb_rate_limit_delay_millis</code> Numeric <code>rocksdb_row_lock_deadlocks</code> Numeric <code>rocksdb_row_lock_wait_timeouts</code> Numeric <code>rocksdb_snapshot_conflict_errors</code> Numeric <code>rocksdb_stall_l0_file_count_limit_slowdowns</code> Numeric <code>rocksdb_stall_locked_l0_file_count_limit_slowdowns</code> Numeric <code>rocksdb_stall_l0_file_count_limit_stops</code> Numeric <code>rocksdb_stall_locked_l0_file_count_limit_stops</code> Numeric <code>rocksdb_stall_pending_compaction_limit_stops</code> Numeric <code>rocksdb_stall_pending_compaction_limit_slowdowns</code> Numeric <code>rocksdb_stall_memtable_limit_stops</code> Numeric <code>rocksdb_stall_memtable_limit_slowdowns</code> Numeric <code>rocksdb_stall_total_stops</code> Numeric <code>rocksdb_stall_total_slowdowns</code> Numeric <code>rocksdb_stall_micros</code> Numeric <code>rocksdb_wal_bytes</code> Numeric <code>rocksdb_wal_group_syncs</code> Numeric <code>rocksdb_wal_synced</code> Numeric <code>rocksdb_write_other</code> Numeric <code>rocksdb_write_self</code> Numeric <code>rocksdb_write_timedout</code> Numeric <code>rocksdb_write_wal</code> Numeric"},{"location":"myrocks/status_variables.html#rocksdb_rows_deleted","title":"<code>rocksdb_rows_deleted</code>","text":"<p>This variable shows the number of rows that were deleted from MyRocks tables.</p>"},{"location":"myrocks/status_variables.html#rocksdb_rows_inserted","title":"<code>rocksdb_rows_inserted</code>","text":"<p>This variable shows the number of rows that were inserted into MyRocks tables.</p>"},{"location":"myrocks/status_variables.html#rocksdb_rows_read","title":"<code>rocksdb_rows_read</code>","text":"<p>This variable shows the number of rows that were read from MyRocks tables.</p>"},{"location":"myrocks/status_variables.html#rocksdb_rows_updated","title":"<code>rocksdb_rows_updated</code>","text":"<p>This variable shows the number of rows that were updated in MyRocks tables.</p>"},{"location":"myrocks/status_variables.html#rocksdb_rows_expired","title":"<code>rocksdb_rows_expired</code>","text":"<p>This variable shows the number of expired rows in MyRocks tables.</p>"},{"location":"myrocks/status_variables.html#rocksdb_rows_filtered","title":"<code>rocksdb_rows_filtered</code>","text":"<p>This variable shows the number of rows that were filtered out for TTL in MyRocks tables.</p>"},{"location":"myrocks/status_variables.html#rocksdb_system_rows_deleted","title":"<code>rocksdb_system_rows_deleted</code>","text":"<p>This variable shows the number of rows that were deleted from MyRocks system tables.</p>"},{"location":"myrocks/status_variables.html#rocksdb_system_rows_inserted","title":"<code>rocksdb_system_rows_inserted</code>","text":"<p>This variable shows the number of rows that were inserted into MyRocks system tables.</p>"},{"location":"myrocks/status_variables.html#ocksdb_system_rows_read","title":"<code>ocksdb_system_rows_read</code>","text":"<p>This variable shows the number of rows that were read from MyRocks system tables.</p>"},{"location":"myrocks/status_variables.html#rocksdb_system_rows_updated","title":"<code>rocksdb_system_rows_updated</code>","text":"<p>This variable shows the number of rows that were updated in MyRocks system tables.</p>"},{"location":"myrocks/status_variables.html#rocksdb_memtable_total","title":"<code>rocksdb_memtable_total</code>","text":"<p>This variable shows the memory usage, in bytes, of all memtables.</p>"},{"location":"myrocks/status_variables.html#rocksdb_memtable_unflushed","title":"<code>rocksdb_memtable_unflushed</code>","text":"<p>This variable shows the memory usage, in bytes, of all unflushed memtables.</p>"},{"location":"myrocks/status_variables.html#rocksdb_queries_point","title":"<code>rocksdb_queries_point</code>","text":"<p>This variable shows the number of single row queries.</p>"},{"location":"myrocks/status_variables.html#rocksdb_queries_range","title":"<code>rocksdb_queries_range</code>","text":"<p>This variable shows the number of multi/range row queries.</p>"},{"location":"myrocks/status_variables.html#rocksdb_covered_secondary_key_lookups","title":"<code>rocksdb_covered_secondary_key_lookups</code>","text":"<p>This variable shows the number of lookups via the secondary index that returned all fields requested directly from the secondary index.</p>"},{"location":"myrocks/status_variables.html#rocksdb_additional_compactions_trigger","title":"<code>rocksdb_additional_compactions_trigger</code>","text":"<p>This variable shows the number of triggered additional compactions. MyRocks triggers an additional compaction if (number of deletions / number of entries) &gt; (rocksdb_compaction_sequential_deletes / rocksdb_compaction_sequential_deletes_window) in the SST file.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_add","title":"<code>rocksdb_block_cache_add</code>","text":"<p>This variable shows the number of blocks added to block cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_add_failures","title":"<code>rocksdb_block_cache_add_failures</code>","text":"<p>This variable shows the number of failures when adding blocks to block cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_bytes_read","title":"<code>rocksdb_block_cache_bytes_read</code>","text":"<p>This variable shows the number of bytes read from cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_bytes_write","title":"<code>rocksdb_block_cache_bytes_write</code>","text":"<p>This variable shows the number of bytes written into cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_data_add","title":"<code>rocksdb_block_cache_data_add</code>","text":"<p>This variable shows the number of data blocks added to block cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_data_bytes_insert","title":"<code>rocksdb_block_cache_data_bytes_insert</code>","text":"<p>This variable shows the number of bytes of data blocks inserted into cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_data_hit","title":"<code>rocksdb_block_cache_data_hit</code>","text":"<p>This variable shows the number of cache hits when accessing the data block from the block cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_data_miss","title":"<code>rocksdb_block_cache_data_miss</code>","text":"<p>This variable shows the number of cache misses when accessing the data block from the block cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_filter_add","title":"<code>rocksdb_block_cache_filter_add</code>","text":"<p>This variable shows the number of filter blocks added to block cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_filter_bytes_evict","title":"<code>rocksdb_block_cache_filter_bytes_evict</code>","text":"<p>This variable shows the number of bytes of bloom filter blocks removed from cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_filter_bytes_insert","title":"<code>rocksdb_block_cache_filter_bytes_insert</code>","text":"<p>This variable shows the number of bytes of bloom filter blocks inserted into cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_filter_hit","title":"<code>rocksdb_block_cache_filter_hit</code>","text":"<p>This variable shows the number of times cache hit when accessing filter block from block cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_filter_miss","title":"<code>rocksdb_block_cache_filter_miss</code>","text":"<p>This variable shows the number of times cache miss when accessing filter block from block cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_hit","title":"<code>rocksdb_block_cache_hit</code>","text":"<p>This variable shows the total number of block cache hits.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_index_add","title":"<code>rocksdb_block_cache_index_add</code>","text":"<p>This variable shows the number of index blocks added to block cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_index_bytes_evict","title":"<code>rocksdb_block_cache_index_bytes_evict</code>","text":"<p>This variable shows the number of bytes of index block erased from cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_index_bytes_insert","title":"<code>rocksdb_block_cache_index_bytes_insert</code>","text":"<p>This variable shows the number of bytes of index blocks inserted into cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_index_hit","title":"<code>rocksdb_block_cache_index_hit</code>","text":"<p>This variable shows the total number of block cache index hits.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_index_miss","title":"<code>rocksdb_block_cache_index_miss</code>","text":"<p>This variable shows the number of times cache hit when accessing index block from block cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_miss","title":"<code>rocksdb_block_cache_miss</code>","text":"<p>This variable shows the total number of block cache misses.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_compressed_hit","title":"<code>rocksdb_block_cache_compressed_hit</code>","text":"<p>This variable shows the number of hits in the compressed block cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_block_cache_compressed_miss","title":"<code>rocksdb_block_cache_compressed_miss</code>","text":"<p>This variable shows the number of misses in the compressed block cache.</p>"},{"location":"myrocks/status_variables.html#rocksdb_bloom_filter_prefix_checked","title":"<code>rocksdb_bloom_filter_prefix_checked</code>","text":"<p>This variable shows the number of times bloom was checked before creating iterator on a file.</p>"},{"location":"myrocks/status_variables.html#rocksdb_bloom_filter_prefix_useful","title":"<code>rocksdb_bloom_filter_prefix_useful</code>","text":"<p>This variable shows the number of times the check was useful in avoiding iterator creation (and thus likely IOPs).</p>"},{"location":"myrocks/status_variables.html#rocksdb_bloom_filter_useful","title":"<code>rocksdb_bloom_filter_useful</code>","text":"<p>This variable shows the number of times bloom filter has avoided file reads.</p>"},{"location":"myrocks/status_variables.html#rocksdb_bytes_read","title":"<code>rocksdb_bytes_read</code>","text":"<p>This variable shows the total number of uncompressed bytes read. It could be either from memtables, cache, or table files.</p>"},{"location":"myrocks/status_variables.html#rocksdb_bytes_written","title":"<code>rocksdb_bytes_written</code>","text":"<p>This variable shows the total number of uncompressed bytes written.</p>"},{"location":"myrocks/status_variables.html#rocksdb_compact_read_bytes","title":"<code>rocksdb_compact_read_bytes</code>","text":"<p>This variable shows the number of bytes read during compaction</p>"},{"location":"myrocks/status_variables.html#rocksdb_compact_write_bytes","title":"<code>rocksdb_compact_write_bytes</code>","text":"<p>This variable shows the number of bytes written during compaction.</p>"},{"location":"myrocks/status_variables.html#rocksdb_compaction_key_drop_new","title":"<code>rocksdb_compaction_key_drop_new</code>","text":"<p>This variable shows the number of key drops during compaction because it was overwritten with a newer value.</p>"},{"location":"myrocks/status_variables.html#rocksdb_compaction_key_drop_obsolete","title":"<code>rocksdb_compaction_key_drop_obsolete</code>","text":"<p>This variable shows the number of key drops during compaction because it was obsolete.</p>"},{"location":"myrocks/status_variables.html#rocksdb_compaction_key_drop_user","title":"<code>rocksdb_compaction_key_drop_user</code>","text":"<p>This variable shows the number of key drops during compaction because user compaction function has dropped the key.</p>"},{"location":"myrocks/status_variables.html#rocksdb_flush_write_bytes","title":"<code>rocksdb_flush_write_bytes</code>","text":"<p>This variable shows the number of bytes written during flush.</p>"},{"location":"myrocks/status_variables.html#rocksdb_get_hit_l0","title":"<code>rocksdb_get_hit_l0</code>","text":"<p>This variable shows the number of <code>Get()</code> queries served by L0.</p>"},{"location":"myrocks/status_variables.html#rocksdb_get_hit_l1","title":"<code>rocksdb_get_hit_l1</code>","text":"<p>This variable shows the number of <code>Get()</code> queries served by L1.</p>"},{"location":"myrocks/status_variables.html#rocksdb_get_hit_l2_and_up","title":"<code>rocksdb_get_hit_l2_and_up</code>","text":"<p>This variable shows the number of <code>Get()</code> queries served by L2 and up.</p>"},{"location":"myrocks/status_variables.html#rocksdb_get_updates_since_calls","title":"<code>rocksdb_get_updates_since_calls</code>","text":"<p>This variable shows the number of calls to <code>GetUpdatesSince</code> function. Useful to keep track of transaction log iterator refreshes</p>"},{"location":"myrocks/status_variables.html#rocksdb_iter_bytes_read","title":"<code>rocksdb_iter_bytes_read</code>","text":"<p>This variable shows the number of uncompressed bytes read from an iterator. It includes size of key and value.</p>"},{"location":"myrocks/status_variables.html#rocksdb_memtable_hit","title":"<code>rocksdb_memtable_hit</code>","text":"<p>This variable shows the number of memtable hits.</p>"},{"location":"myrocks/status_variables.html#rocksdb_memtable_miss","title":"<code>rocksdb_memtable_miss</code>","text":"<p>This variable shows the number of memtable misses.</p>"},{"location":"myrocks/status_variables.html#rocksdb_no_file_closes","title":"<code>rocksdb_no_file_closes</code>","text":"<p>This variable shows the number of time file were closed.</p>"},{"location":"myrocks/status_variables.html#rocksdb_no_file_errors","title":"<code>rocksdb_no_file_errors</code>","text":"<p>This variable shows number of errors trying to read in data from an sst file.</p>"},{"location":"myrocks/status_variables.html#rocksdb_no_file_opens","title":"<code>rocksdb_no_file_opens</code>","text":"<p>This variable shows the number of time file were opened.</p>"},{"location":"myrocks/status_variables.html#rocksdb_num_iterators","title":"<code>rocksdb_num_iterators</code>","text":"<p>This variable shows the number of currently open iterators.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_block_not_compressed","title":"<code>rocksdb_number_block_not_compressed</code>","text":"<p>This variable shows the number of uncompressed blocks.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_db_next","title":"<code>rocksdb_number_db_next</code>","text":"<p>This variable shows the number of calls to <code>next</code>.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_db_next_found","title":"<code>rocksdb_number_db_next_found</code>","text":"<p>This variable shows the number of calls to <code>next</code> that returned data.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_db_prev","title":"<code>rocksdb_number_db_prev</code>","text":"<p>This variable shows the number of calls to <code>prev</code>.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_db_prev_found","title":"<code>rocksdb_number_db_prev_found</code>","text":"<p>This variable shows the number of calls to <code>prev</code> that returned data.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_db_seek","title":"<code>rocksdb_number_db_seek</code>","text":"<p>This variable shows the number of calls to <code>seek</code>.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_db_seek_found","title":"<code>rocksdb_number_db_seek_found</code>","text":"<p>This variable shows the number of calls to <code>seek</code> that returned data.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_deletes_filtered","title":"<code>rocksdb_number_deletes_filtered</code>","text":"<p>This variable shows the number of deleted records that were not required to be written to storage because key did not exist.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_keys_read","title":"<code>rocksdb_number_keys_read</code>","text":"<p>This variable shows the number of keys read.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_keys_updated","title":"<code>rocksdb_number_keys_updated</code>","text":"<p>This variable shows the number of keys updated, if inplace update is enabled.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_keys_written","title":"<code>rocksdb_number_keys_written</code>","text":"<p>This variable shows the number of keys written to the database.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_merge_failures","title":"<code>rocksdb_number_merge_failures</code>","text":"<p>This variable shows the number of failures performing merge operator actions in RocksDB.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_multiget_bytes_read","title":"<code>rocksdb_number_multiget_bytes_read</code>","text":"<p>This variable shows the number of bytes read during RocksDB <code>MultiGet()</code> calls.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_multiget_get","title":"<code>rocksdb_number_multiget_get</code>","text":"<p>This variable shows the number <code>MultiGet()</code> requests to RocksDB.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_multiget_keys_read","title":"<code>rocksdb_number_multiget_keys_read</code>","text":"<p>This variable shows the keys read via <code>MultiGet()</code>.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_reseeks_iteration","title":"<code>rocksdb_number_reseeks_iteration</code>","text":"<p>This variable shows the number of times reseek happened inside an iteration to skip over large number of keys with same userkey.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_sst_entry_delete","title":"<code>rocksdb_number_sst_entry_delete</code>","text":"<p>This variable shows the total number of delete markers written by MyRocks.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_sst_entry_merge","title":"<code>rocksdb_number_sst_entry_merge</code>","text":"<p>This variable shows the total number of merge keys written by MyRocks.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_sst_entry_other","title":"<code>rocksdb_number_sst_entry_other</code>","text":"<p>This variable shows the total number of non-delete, non-merge, non-put keys written by MyRocks.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_sst_entry_put","title":"<code>rocksdb_number_sst_entry_put</code>","text":"<p>This variable shows the total number of put keys written by MyRocks.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_sst_entry_singledelete","title":"<code>rocksdb_number_sst_entry_singledelete</code>","text":"<p>This variable shows the total number of single delete keys written by MyRocks.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_stat_computes","title":"<code>rocksdb_number_stat_computes</code>","text":"<p>This variable was removed in Percona Server for MySQL Percona Server 5.7.23-23.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_superversion_acquires","title":"<code>rocksdb_number_superversion_acquires</code>","text":"<p>This variable shows the number of times the superversion structure has been acquired in RocksDB, this is used for tracking all of the files for the database.</p>"},{"location":"myrocks/status_variables.html#rocksdb_number_superversion_cleanups","title":"<code>rocksdb_number_superversion_cleanups</code>","text":""},{"location":"myrocks/status_variables.html#rocksdb_number_superversion_releases","title":"<code>rocksdb_number_superversion_releases</code>","text":""},{"location":"myrocks/status_variables.html#rocksdb_rate_limit_delay_millis","title":"<code>rocksdb_rate_limit_delay_millis</code>","text":"<p>This variable was removed in Percona Server for MySQL Percona Server 5.7.23-23.</p>"},{"location":"myrocks/status_variables.html#rocksdb_row_lock_deadlocks","title":"<code>rocksdb_row_lock_deadlocks</code>","text":"<p>This variable shows the total number of deadlocks that have been detected since the instance was started.</p>"},{"location":"myrocks/status_variables.html#rocksdb_row_lock_wait_timeouts","title":"<code>rocksdb_row_lock_wait_timeouts</code>","text":"<p>This variable shows the total number of row lock wait timeouts that have been detected since the instance was started.</p>"},{"location":"myrocks/status_variables.html#rocksdb_snapshot_conflict_errors","title":"<code>rocksdb_snapshot_conflict_errors</code>","text":"<p>This variable shows the number of snapshot conflict errors occurring during write transactions that forces the transaction to rollback.</p>"},{"location":"myrocks/status_variables.html#rocksdb_stall_l0_file_count_limit_slowdowns","title":"<code>rocksdb_stall_l0_file_count_limit_slowdowns</code>","text":"<p>This variable shows the slowdowns in write due to L0 being close to full.</p>"},{"location":"myrocks/status_variables.html#rocksdb_stall_locked_l0_file_count_limit_slowdowns","title":"<code>rocksdb_stall_locked_l0_file_count_limit_slowdowns</code>","text":"<p>This variable shows the slowdowns in write due to L0 being close to full and compaction for L0 is already in progress.</p>"},{"location":"myrocks/status_variables.html#rocksdb_stall_l0_file_count_limit_stops","title":"<code>rocksdb_stall_l0_file_count_limit_stops</code>","text":"<p>This variable shows the stalls in write due to L0 being full.</p>"},{"location":"myrocks/status_variables.html#rocksdb_stall_locked_l0_file_count_limit_stops","title":"<code>rocksdb_stall_locked_l0_file_count_limit_stops</code>","text":"<p>This variable shows the stalls in write due to L0 being full and compaction for L0 is already in progress.</p>"},{"location":"myrocks/status_variables.html#rocksdb_stall_pending_compaction_limit_stops","title":"<code>rocksdb_stall_pending_compaction_limit_stops</code>","text":"<p>This variable shows the stalls in write due to hitting limits set for max number of pending compaction bytes.</p>"},{"location":"myrocks/status_variables.html#rocksdb_stall_pending_compaction_limit_slowdowns","title":"<code>rocksdb_stall_pending_compaction_limit_slowdowns</code>","text":"<p>This variable shows the slowdowns in write due to getting close to limits set for max number of pending compaction bytes.</p>"},{"location":"myrocks/status_variables.html#rocksdb_stall_memtable_limit_stops","title":"<code>rocksdb_stall_memtable_limit_stops</code>","text":"<p>This variable shows the stalls in write due to hitting max number of <code>memTables</code> allowed.</p>"},{"location":"myrocks/status_variables.html#rocksdb_stall_memtable_limit_slowdowns","title":"<code>rocksdb_stall_memtable_limit_slowdowns</code>","text":"<p>This variable shows the slowdowns in writes due to getting close to max number of memtables allowed.</p>"},{"location":"myrocks/status_variables.html#rocksdb_stall_total_stops","title":"<code>rocksdb_stall_total_stops</code>","text":"<p>This variable shows the total number of write stalls.</p>"},{"location":"myrocks/status_variables.html#rocksdb_stall_total_slowdowns","title":"<code>rocksdb_stall_total_slowdowns</code>","text":"<p>This variable shows the total number of write slowdowns.</p>"},{"location":"myrocks/status_variables.html#rocksdb_stall_micros","title":"<code>rocksdb_stall_micros</code>","text":"<p>This variable shows how long (in microseconds) the writer had to wait for compaction or flush to finish.</p>"},{"location":"myrocks/status_variables.html#rocksdb_wal_bytes","title":"<code>rocksdb_wal_bytes</code>","text":"<p>This variables shows the number of bytes written to WAL.</p>"},{"location":"myrocks/status_variables.html#rocksdb_wal_group_syncs","title":"<code>rocksdb_wal_group_syncs</code>","text":"<p>This variable shows the number of group commit WAL file syncs that have occurred.</p>"},{"location":"myrocks/status_variables.html#rocksdb_wal_synced","title":"<code>rocksdb_wal_synced</code>","text":"<p>This variable shows the number of times WAL sync was done.</p>"},{"location":"myrocks/status_variables.html#rocksdb_write_other","title":"<code>rocksdb_write_other</code>","text":"<p>This variable shows the number of writes processed by another thread.</p>"},{"location":"myrocks/status_variables.html#rocksdb_write_self","title":"<code>rocksdb_write_self</code>","text":"<p>This variable shows the number of writes that were processed by a requesting thread.</p>"},{"location":"myrocks/status_variables.html#rocksdb_write_timedout","title":"<code>rocksdb_write_timedout</code>","text":"<p>This variable shows the number of writes ending up with timed-out.</p>"},{"location":"myrocks/status_variables.html#rocksdb_write_wal","title":"<code>rocksdb_write_wal</code>","text":"<p>This variable shows the number of Write calls that request WAL.</p>"},{"location":"myrocks/variables.html","title":"MyRocks Server Variables","text":"<p>The MyRocks server variables expose configuration of the underlying RocksDB engine. There several ways to set these variables:</p> <ul> <li> <p>For production deployments, you should have all variables defined in the configuration file.</p> </li> <li> <p>Dynamic variables can be changed at runtime using the <code>SET</code> statement.</p> </li> <li> <p>If you want to test things out, you can set some of the variables when starting <code>mysqld</code> using corresponding command-line options.</p> </li> </ul> <p>If a variable was not set in either the configuration file or as a command-line option, the default value is used.</p> <p>Also, all variables can exist in one or both of the following scopes:</p> <ul> <li> <p>Global scope defines how the variable affects overall server operation.</p> </li> <li> <p>Session scope defines how the variable affects operation for individual client connections.</p> </li> </ul> <p>The following server variables are available:</p> Variable Name <code>rocksdb_access_hint_on_compaction_start</code> <code>rocksdb_advise_random_on_open</code> <code>rocksdb_allow_concurrent_memtable_write</code> <code>rocksdb_allow_to_start_after_corruption</code> <code>rocksdb_allow_mmap_reads</code> <code>rocksdb_allow_mmap_writes</code> <code>rocksdb_alter_column_default_inplace</code> <code>rocksdb_base_background_compactions</code> <code>rocksdb_blind_delete_primary_key</code> <code>rocksdb_block_cache_size</code> <code>rocksdb_block_restart_interval</code> <code>rocksdb_block_size</code> <code>rocksdb_block_size_deviation</code> <code>rocksdb_bulk_load</code> <code>rocksdb_bulk_load_allow_sk</code> <code>rocksdb_bulk_load_allow_unsorted</code> <code>rocksdb_bulk_load_size</code> <code>rocksdb_bytes_per_sync</code> <code>rocksdb_cache_dump</code> <code>rocksdb_cache_index_and_filter_blocks</code> <code>rocksdb_checksums_pct</code> <code>rocksdb_collect_sst_properties</code> <code>rocksdb_commit_in_the_middle</code> <code>rocksdb_commit_time_batch_for_recovery</code> <code>rocksdb_compact_cf</code> <code>rocksdb_compaction_readahead_size</code> <code>rocksdb_compaction_sequential_deletes</code> <code>rocksdb_compaction_sequential_deletes_count_sd</code> <code>rocksdb_compaction_sequential_deletes_file_size</code> <code>rocksdb_compaction_sequential_deletes_window</code> <code>rocksdb_concurrent_prepare</code> <code>rocksdb_create_checkpoint</code> <code>rocksdb_create_if_missing</code> <code>rocksdb_create_missing_column_families</code> <code>rocksdb_datadir</code> <code>rocksdb_db_write_buffer_size</code> <code>rocksdb_deadlock_detect</code> <code>rocksdb_deadlock_detect_depth</code> <code>rocksdb_debug_optimizer_no_zero_cardinality</code> <code>rocksdb_debug_ttl_ignore_pk</code> <code>rocksdb_debug_ttl_read_filter_ts</code> <code>rocksdb_debug_ttl_rec_ts</code> <code>rocksdb_debug_ttl_snapshot_ts</code> <code>rocksdb_default_cf_options</code> <code>rocksdb_delayed_write_rate</code> <code>rocksdb_delete_cf</code> <code>rocksdb_delete_obsolete_files_period_micros</code> <code>rocksdb_disable_file_deletions</code> <code>rocksdb_enable_bulk_load_api</code> <code>rocksdb_enable_insert_with_update_caching</code> <code>rocksdb_enable_iterate_bounds</code> <code>rocksdb_enable_native_partition</code> <code>rocksdb_enable_pipelined_write</code> <code>rocksdb_enable_remove_orphaned_dropped_cfs</code> <code>rocksdb_enable_ttl</code> <code>rocksdb_enable_ttl_read_filtering</code> <code>rocksdb_enable_thread_tracking</code> <code>rocksdb_enable_write_thread_adaptive_yield</code> <code>rocksdb_error_if_exists</code> <code>rocksdb_error_on_suboptimal_collation</code> <code>rocksdb_flush_log_at_trx_commit</code> <code>rocksdb_flush_memtable_on_analyze</code> <code>rocksdb_force_compute_memtable_stats</code> <code>rocksdb_force_compute_memtable_stats_cachetime</code> <code>rocksdb_force_flush_memtable_and_lzero_now</code> <code>rocksdb_force_flush_memtable_now</code> <code>rocksdb_force_index_records_in_range</code> <code>rocksdb_hash_index_allow_collision</code> <code>rocksdb_ignore_unknown_options</code> <code>rocksdb_index_type</code> <code>rocksdb_info_log_level</code> <code>rocksdb_is_fd_close_on_exec</code> <code>rocksdb_keep_log_file_num</code> <code>rocksdb_large_prefix</code> <code>rocksdb_lock_scanned_rows</code> <code>rocksdb_lock_wait_timeout</code> <code>rocksdb_log_file_time_to_roll</code> <code>rocksdb_manifest_preallocation_size</code> <code>rocksdb_manual_compaction_bottommost_level</code> <code>rocksdb_manual_wal_flush</code> <code>rocksdb_master_skip_tx_api</code> <code>rocksdb_max_background_compactions</code> <code>rocksdb_max_background_flushes</code> <code>rocksdb_max_background_jobs</code> <code>rocksdb_max_bottom_pri_background_compactions</code> <code>rocksdb_max_latest_deadlocks</code> <code>rocksdb_max_log_file_size</code> <code>rocksdb_max_manifest_file_size</code> <code>rocksdb_max_open_files</code> <code>rocksdb_max_row_locks</code> <code>rocksdb_max_subcompactions</code> <code>rocksdb_max_total_wal_size</code> <code>rocksdb_merge_buf_size</code> <code>rocksdb_merge_combine_read_size</code> <code>rocksdb_merge_tmp_file_removal_delay_ms</code> <code>rocksdb_new_table_reader_for_compaction_inputs</code> <code>rocksdb_no_block_cache</code> <code>rocksdb_no_create_column_family</code> <code>rocksdb_override_cf_options</code> <code>rocksdb_paranoid_checks</code> <code>rocksdb_partial_index_sort_max_mem</code> <code>rocksdb_pause_background_work</code> <code>rocksdb_perf_context_level</code> <code>rocksdb_persistent_cache_path</code> <code>rocksdb_persistent_cache_size_mb</code> <code>rocksdb_pin_l0_filter_and_index_blocks_in_cache</code> <code>rocksdb_print_snapshot_conflict_queries</code> <code>rocksdb_rate_limiter_bytes_per_sec</code> <code>rocksdb_read_free_rpl</code> <code>rocksdb_read_free_rpl_tables</code> <code>rocksdb_records_in_range</code> <code>rocksdb_reset_stats</code> <code>rocksdb_rollback_on_timeout</code> <code>rocksdb_rpl_skip_tx_api</code> <code>rocksdb_seconds_between_stat_computes</code> <code>rocksdb_signal_drop_index_thread</code> <code>rocksdb_sim_cache_size</code> <code>rocksdb_skip_bloom_filter_on_read</code> <code>rocksdb_skip_fill_cache</code> <code>rocksdb_skip_locks_if_skip_unique_check</code> <code>rocksdb_sst_mgr_rate_bytes_per_sec</code> <code>rocksdb_stats_dump_period_sec</code> <code>rocksdb_stats_level</code> <code>rocksdb_stats_recalc_rate</code> <code>rocksdb_store_row_debug_checksums</code> <code>rocksdb_strict_collation_check</code> <code>rocksdb_strict_collation_exceptions</code> <code>rocksdb_table_cache_numshardbits</code> <code>rocksdb_table_stats_background_thread_nice_value</code> <code>rocksdb_table_stats_max_num_rows_scanned</code> <code>rocksdb_table_stats_recalc_threshold_count</code> <code>rocksdb_table_stats_recalc_threshold_pct</code> <code>rocksdb_table_stats_sampling_pct</code> <code>rocksdb_table_stats_use_table_scan</code> <code>rocksdb_tmpdir</code> <code>rocksdb_two_write_queues</code> <code>rocksdb_trace_block_cache_access</code> <code>rocksdb_trace_queries</code> <code>rocksdb_trace_sst_api</code> <code>rocksdb_track_and_verify_wals_in_manifest</code> <code>rocksdb_unsafe_for_binlog</code> <code>rocksdb_update_cf_options</code> <code>rocksdb_use_adaptive_mutex</code> <code>rocksdb_use_default_sk_cf</code> <code>rocksdb_use_direct_io_for_flush_and_compaction</code> <code>rocksdb_use_direct_reads</code> <code>rocksdb_use_fsync</code> <code>rocksdb_validate_tables</code> <code>rocksdb_verify_row_debug_checksums</code> <code>rocksdb_wal_bytes_per_sync</code> <code>rocksdb_wal_dir</code> <code>rocksdb_wal_recovery_mode</code> <code>rocksdb_wal_size_limit_mb</code> <code>rocksdb_wal_ttl_seconds</code> <code>rocksdb_whole_key_filtering</code> <code>rocksdb_write_batch_max_bytes</code> <code>rocksdb_write_disable_wal</code> <code>rocksdb_write_ignore_missing_column_families</code> <code>rocksdb_write_policy</code>"},{"location":"myrocks/variables.html#rocksdb_access_hint_on_compaction_start","title":"<code>rocksdb_access_hint_on_compaction_start</code>","text":"Option Description Command-line \u2013rocksdb-access-hint-on-compaction-start Dynamic No Scope Global Data type String or numeric Default NORMAL or 1 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the file access pattern once compaction is started, and applied to all input files of compaction. Possible values are:</p> <ul> <li> <p><code>0</code> = <code>NONE</code></p> </li> <li> <p><code>1</code> = <code>NORMAL</code> (default)</p> </li> <li> <p><code>2</code> = <code>SEQUENTIAL</code></p> </li> <li> <p><code>3</code> = <code>WILLNEED</code></p> </li> </ul>"},{"location":"myrocks/variables.html#rocksdb_advise_random_on_open","title":"<code>rocksdb_advise_random_on_open</code>","text":"Option Description Command-line \u2013rocksdb-advise-random-on-open Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to hint at the underlying file system that the file access pattern is random, when a data file is opened. Enabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_allow_concurrent_memtable_write","title":"<code>rocksdb_allow_concurrent_memtable_write</code>","text":"Option Description Command-line \u2013rocksdb-allow-concurrent-memtable-write Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to allow multiple writers to update memtables in parallel. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_allow_to_start_after_corruption","title":"<code>rocksdb_allow_to_start_after_corruption</code>","text":"Option Description Command-line \u2013rocksdb_allow_to_start_after_corruption Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to allow the server to restart once MyRocks reported data corruption. Disabled by default.</p> <p>Once corruption is detected server writes a marker file (named ROCKSDB_CORRUPTED) in the data directory and aborts. If a marker file exists, then mysqld exits on startup with an error message. The restart failure will continue until the problem is solved or until mysqld is started with this variable turned on in the command line.</p> <p>Note</p> <p>Not all memtables support concurrent writes.</p>"},{"location":"myrocks/variables.html#rocksdb_allow_mmap_reads","title":"<code>rocksdb_allow_mmap_reads</code>","text":"Option Description Command-line \u2013rocksdb-allow-mmap-reads Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to allow the OS to map a data file into memory for reads. Disabled by default. If you enable this, make sure that rocksdb_use_direct_reads is disabled.</p>"},{"location":"myrocks/variables.html#rocksdb_allow_mmap_writes","title":"<code>rocksdb_allow_mmap_writes</code>","text":"Option Description Command-line \u2013rocksdb-allow-mmap-writes Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to allow the OS to map a data file into memory for writes. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_alter_column_default_inplace","title":"<code>rocksdb_alter_column_default_inplace</code>","text":"Option Description Command-line \u2013rocksdb-alter-column-default-inplace Dynamic Yes Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server for MySQL 5.7.35-38. Allow inplace alter for the alter column default operation.</p>"},{"location":"myrocks/variables.html#rocksdb_base_background_compactions","title":"<code>rocksdb_base_background_compactions</code>","text":"Option Description Command-line \u2013rocksdb-base-background-compactions Dynamic No Scope Global Data type Numeric Default 1 <p>The variable has been implemented in Percona Server 5.7.19-17. This variable has been replaced in Percona Server 5.7.20-18 by rocksdb_max_background_jobs, which automatically decides how many threads to allocate towards flush/compaction. Specifies the suggested number of concurrent background compaction jobs, submitted to the default LOW priority thread pool in RocksDB. Default is <code>1</code>. The allowed range of values is from <code>-1</code> to <code>64</code>. Maximum depends on the rocksdb_max_background_compactions variable.</p>"},{"location":"myrocks/variables.html#rocksdb_blind_delete_primary_key","title":"<code>rocksdb_blind_delete_primary_key</code>","text":"Option Description Command-line \u2013rocksdb-blind-delete-primary-key Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. Skips verifying if rows exist before executing deletes. The following conditions must be met:</p> <ul> <li> <p>The variable is enabled</p> </li> <li> <p>Only a single table is listed in the <code>DELETE</code> statement</p> </li> <li> <p>The table has only a primary key with no secondary keys</p> </li> </ul>"},{"location":"myrocks/variables.html#rocksdb_block_cache_size","title":"<code>rocksdb_block_cache_size</code>","text":"Option Description Command-line \u2013rocksdb-block-cache-size Dynamic No Scope Global Data type Numeric Default 536870912 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the size of the LRU block cache for RocksDB. This memory is reserved for the block cache, which is in addition to any filesystem caching that may occur.</p> <p>The minimum value is <code>1024</code> because that\u2019s the size of one block.</p> <p>The default value is <code>536870912</code>.</p> <p>The maximum value is <code>9223372036854775807</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_block_restart_interval","title":"<code>rocksdb_block_restart_interval</code>","text":"Option Description Command-line \u2013rocksdb-block-restart-interval Dynamic No Scope Global Data type Numeric Default 16 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the number of keys for each set of delta encoded data. The default value is <code>16</code>. The allowed range is from <code>1</code> to <code>2147483647</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_block_size","title":"<code>rocksdb_block_size</code>","text":"Option Description Command-line \u2013rocksdb-block-size Dynamic No Scope Global Data type Numeric Default 4096 <p>The variable has been implemented in Percona Server 5.7.19-17. The minimum value has changed from <code>0</code> to <code>1024</code> in Percona Server 5.7.20-18. This variable specifies the size of the data block for reading RocksDB data files. The default value is <code>4096</code>. The allowed range is from <code>1024</code> to <code>18446744073709551615</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_block_size_deviation","title":"<code>rocksdb_block_size_deviation</code>","text":"Option Description Command-line \u2013rocksdb-block-size-deviation Dynamic No Scope Global Data type Numeric Default 10 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the threshold for free space allowed in a data block (see rocksdb_block_size). If there is less space remaining, close the block (and write to new block). Default value is <code>10</code>, meaning that the block is not closed until there is less than 10 bits of free space remaining.</p> <p>The allowed range is from <code>1</code> to <code>2147483647</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_bulk_load_allow_sk","title":"<code>rocksdb_bulk_load_allow_sk</code>","text":"Option Description Command-line \u2013rocksdb-bulk-load-allow-sk Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.23-23. Enabling this variable allows secondary keys to be added using the bulk loading feature. This variable can be toggled only when the bulk load is disabled, for example, when rocksdb_bulk_load is <code>OFF</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_bulk_load_allow_unsorted","title":"<code>rocksdb_bulk_load_allow_unsorted</code>","text":"Option Description Command-line \u2013rocksdb-bulk-load-allow-unsorted Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.20-18. By default, the bulk loader requires its input to be sorted in the primary key order. If enabled, unsorted inputs are allowed too, which are then sorted by the bulkloader itself, at a performance penalty.</p>"},{"location":"myrocks/variables.html#rocksdb_bulk_load","title":"<code>rocksdb_bulk_load</code>","text":"Option Description Command-line \u2013rocksdb-bulk-load Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to use bulk load: MyRocks will ignore checking keys for uniqueness or acquiring locks during transactions. Disabled by default. Enable this only if you are certain that there are no row conflicts, for example, when setting up a new MyRocks instance from a MySQL dump.</p> <p>When the rocksdb_bulk_load variable is enabled, it behaves as if the variable rocksdb_commit_in_the_middle is enabled, even if the variable rocksdb_commit_in_the_middle is disabled.</p>"},{"location":"myrocks/variables.html#rocksdb_bulk_load_size","title":"<code>rocksdb_bulk_load_size</code>","text":"Option Description Command-line \u2013rocksdb-bulk-load-size Dynamic Yes Scope Global, Session Data type Numeric Default 1000 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the number of keys to accumulate before committing them to the storage engine when the bulk load is enabled (see rocksdb_bulk_load). The default value is <code>1000</code>, which means that a batch can contain up to 1000 records before they are implicitly committed. The allowed range is from <code>1</code> to <code>1073741824</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_bytes_per_sync","title":"<code>rocksdb_bytes_per_sync</code>","text":"Option Description Command-line \u2013rocksdb-bytes-per-sync Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17 and changed to dynamic in Percona Server 5.7.21-20. Specifies how often should the OS sync files to disk as they are being written, asynchronously, in the background. This operation can be used to smooth out write I/O over time. The default value is <code>0</code> meaning that files are never synced. The allowed range is up to <code>18446744073709551615</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_cache_dump","title":"<code>rocksdb_cache_dump</code>","text":"Option Description Command-line \u2013rocksdb-cache-dump Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. Includes RocksDB block cache content in a core dump. This variable is enabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_cache_index_and_filter_blocks","title":"<code>rocksdb_cache_index_and_filter_blocks</code>","text":"Option Description Command-line \u2013rocksdb-cache-index-and-filter-blocks Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether RocksDB should use the block cache for caching the index and bloomfilter data blocks from each data file. Enabled by default. If you disable this feature, RocksDB will allocate additional memory to maintain these data blocks.</p>"},{"location":"myrocks/variables.html#rocksdb_checksums_pct","title":"<code>rocksdb_checksums_pct</code>","text":"Option Description Command-line \u2013rocksdb-checksums-pct Dynamic Yes Scope Global, Session Data type Numeric Default 100 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the percentage of rows to be checksummed. The default value is <code>100</code> (checksum all rows). The allowed range is from <code>0</code> to <code>100</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_collect_sst_properties","title":"<code>rocksdb_collect_sst_properties</code>","text":"Option Description Command-line \u2013rocksdb-collect-sst-properties Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to collect statistics on each data file to improve optimizer behavior. Enabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_commit_in_the_middle","title":"<code>rocksdb_commit_in_the_middle</code>","text":"Option Description Command-line \u2013rocksdb-commit-in-the-middle Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to commit rows implicitly when a batch contains more than the value of rocksdb_bulk_load_size. This option should only be enabled at the time of data import because it may cause locking errors.</p> <p>This variable is disabled by default. </p> <p>When the rocksdb_bulk_load variable is enabled, it behaves as if the variable rocksdb_commit_in_the_middle is enabled, even if the variable rocksdb_commit_in_the_middle is disabled.</p>"},{"location":"myrocks/variables.html#rocksdb_commit_time_batch_for_recovery","title":"<code>rocksdb_commit_time_batch_for_recovery</code>","text":"Option Description Command-line \u2013rocksdb-commit-time-batch-for-recovery Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.23-23. Specifies whether to write the commit time write batch into the database or not.</p> <p>Note</p> <p>If the commit time write batch is only useful for recovery, then writing to WAL is enough.</p>"},{"location":"myrocks/variables.html#rocksdb_compact_cf","title":"<code>rocksdb_compact_cf</code>","text":"Option Description Command-line \u2013rocksdb-compact-cf Dynamic Yes Scope Global Data type String Default <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the name of the column family to compact.</p>"},{"location":"myrocks/variables.html#rocksdb_compaction_readahead_size","title":"<code>rocksdb_compaction_readahead_size</code>","text":"Option Description Command-line \u2013rocksdb-compaction-readahead-size Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the size of reads to perform ahead of compaction. The default value is <code>0</code>. Set this to at least 2 megabytes (<code>16777216</code>) when using MyRocks with spinning disks to ensure sequential reads instead of random. The maximum allowed value is <code>18446744073709551615</code>.</p> <p>Note</p> <p>If you set this variable to a non-zero value, rocksdb_new_table_reader_for_compaction_inputs is enabled.</p>"},{"location":"myrocks/variables.html#rocksdb_compaction_sequential_deletes","title":"<code>rocksdb_compaction_sequential_deletes</code>","text":"Option Description Command-line \u2013rocksdb-compaction-sequential-deletes Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the threshold to trigger compaction on a file if it has more than this number of sequential delete markers. The default value is <code>0</code> meaning that compaction is not triggered regardless of the number of delete markers. The maximum allowed value is <code>2000000</code> (two million delete markers).</p> <p>Note</p> <p>Depending on workload patterns, MyRocks can potentially maintain large numbers of delete markers, which increases the latency of queries. This compaction feature will reduce latency, but may also increase the MyRocks write rate. Use this variable together with rocksdb_compaction_sequential_deletes_file_size to only perform compaction on large files.</p>"},{"location":"myrocks/variables.html#rocksdb_compaction_sequential_deletes_count_sd","title":"<code>rocksdb_compaction_sequential_deletes_count_sd</code>","text":"Option Description Command-line \u2013rocksdb-compaction-sequential-deletes-count-sd Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to count single deletes as delete markers recognized by rocksdb_compaction_sequential_deletes. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_compaction_sequential_deletes_file_size","title":"<code>rocksdb_compaction_sequential_deletes_file_size</code>","text":"Option Description Command-line \u2013rocksdb-compaction-sequential-deletes-file-size Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the minimum file size required to trigger compaction on it by rocksdb_compaction_sequential_deletes. The default value is <code>0</code>, meaning that compaction is triggered regardless of file size. The allowed range is from <code>-1</code> to <code>9223372036854775807</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_compaction_sequential_deletes_window","title":"<code>rocksdb_compaction_sequential_deletes_window</code>","text":"Option Description Command-line \u2013rocksdb-compaction-sequential-deletes-window Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the size of the window for counting delete markers by rocksdb_compaction_sequential_deletes. The default value is <code>0</code>. The allowed range is up to <code>2000000</code> (two million).</p>"},{"location":"myrocks/variables.html#rocksdb_concurrent_prepare","title":"<code>rocksdb_concurrent_prepare</code>","text":"Option Description Command-line \u2013rocksdb-concurrent_prepare Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been deprecated in the Percona Server for MySQL 5.7.21-20, as it has been renamed upstream to <code>rocksdb_two_write_queues</code>.</p> <p>The variable has been implemented in Percona Server 5.7.20-18. When enabled this variable allows/encourages threads that are using two-phase commit to <code>prepare</code> in parallel. </p>"},{"location":"myrocks/variables.html#rocksdb_create_checkpoint","title":"<code>rocksdb_create_checkpoint</code>","text":"Option Description Command-line \u2013rocksdb-create-checkpoint Dynamic Yes Scope Global Data type String Default <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the directory where MyRocks should create a checkpoint. Empty by default.</p>"},{"location":"myrocks/variables.html#rocksdb_create_if_missing","title":"<code>rocksdb_create_if_missing</code>","text":"Option Description Command-line \u2013rocksdb-create-if-missing Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether MyRocks should create its database if it does not exist. Enabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_create_missing_column_families","title":"<code>rocksdb_create_missing_column_families</code>","text":"Option Description Command-line \u2013rocksdb-create-missing-column-families Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether MyRocks should create new column families if they do not exist. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_datadir","title":"<code>rocksdb_datadir</code>","text":"Option Description Command-line \u2013rocksdb-datadir Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the location of the MyRocks data directory. By default, it is created in the current working directory.</p>"},{"location":"myrocks/variables.html#rocksdb_db_write_buffer_size","title":"<code>rocksdb_db_write_buffer_size</code>","text":"Option Description Command-line \u2013rocksdb-db-write-buffer-size Dynamic No Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the maximum size of all memtables used to store writes in MyRocks across all column families. When this size is reached, the data is flushed to persistent media. The default value is <code>0</code>. The allowed range is up to <code>18446744073709551615</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_deadlock_detect","title":"<code>rocksdb_deadlock_detect</code>","text":"Option Description Command-line \u2013rocksdb-deadlock-detect Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether MyRocks should detect deadlocks. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_deadlock_detect_depth","title":"<code>rocksdb_deadlock_detect_depth</code>","text":"<p>Implemented in Percona Server 5.7.20-18.</p> Option Description Command-line \u2013rocksdb-deadlock-detect-depth Dynamic Yes Scope Global, Session Data type Numeric Default 50 <p>The variable has been implemented in Percona Server 5.7.20-18. Specifies the number of transactions deadlock detection will traverse through before assuming deadlock.</p>"},{"location":"myrocks/variables.html#rocksdb_debug_optimizer_no_zero_cardinality","title":"<code>rocksdb_debug_optimizer_no_zero_cardinality</code>","text":"Option Description Command-line \u2013rocksdb-debug-optimizer-no-zero-cardinality Dynamic Yes Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether MyRocks should prevent zero cardinality by always overriding it with some value.</p>"},{"location":"myrocks/variables.html#rocksdb_debug_ttl_ignore_pk","title":"<code>rocksdb_debug_ttl_ignore_pk</code>","text":"Option Description Command-line \u2013rocksdb-debug-ttl-ignore-pk Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.20-18, for debugging purposes only. If true, compaction filtering will not occur on Primary Key TTL data. This variable is a no-op in non-debug builds.</p>"},{"location":"myrocks/variables.html#rocksdb_debug_ttl_read_filter_ts","title":"<code>rocksdb_debug_ttl_read_filter_ts</code>","text":"Option Description Command-line \u2013rocksdb_debug-ttl-read-filter-ts Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.20-18. For debugging purposes only.  Overrides the TTL read filtering time to time + debug_ttl_read_filter_ts. A value of <code>0</code> denotes that the variable is not set. This variable is a no-op in non-debug builds.</p>"},{"location":"myrocks/variables.html#rocksdb_debug_ttl_rec_ts","title":"<code>rocksdb_debug_ttl_rec_ts</code>","text":"Option Description Command-line \u2013rocksdb-debug-ttl-rec-ts Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.20-18. For debugging purposes only.  Overrides the TTL of records to <code>now()</code> + debug_ttl_rec_ts. The value can be \u00b1 to simulate a record inserted in the past vs a record inserted in the \u201cfuture\u201d. A value of <code>0</code> denotes that the variable is not set. This variable is a no-op in non-debug builds.</p>"},{"location":"myrocks/variables.html#rocksdb_debug_ttl_snapshot_ts","title":"<code>rocksdb_debug_ttl_snapshot_ts</code>","text":"Option Description Command-line \u2013rocksdb-debug-ttl-snapshot-ts Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.20-18. For debugging purposes only.  Sets the snapshot during compaction to <code>now()</code> + rocksdb_debug_set_ttl_snapshot_ts. The value can be \u00b1 to simulate a snapshot in the past vs a snapshot created in the \u201cfuture\u201d. A value of <code>0</code> denotes that the variable is not set. This variable is a no-op in non-debug builds.</p>"},{"location":"myrocks/variables.html#rocksdb_default_cf_options","title":"<code>rocksdb_default_cf_options</code>","text":"Option Description Command-line \u2013rocksdb-default-cf-options Dynamic No Scope Global Data type String Default <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the default column family options for MyRocks. Empty by default.</p>"},{"location":"myrocks/variables.html#rocksdb_delayed_write_rate","title":"<code>rocksdb_delayed_write_rate</code>","text":"Option Description Command-line \u2013rocksdb-delayed-write-rate Dynamic Yes Scope Global Data type Numeric Default 16777216 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the write rate in bytes per second, which should be used if MyRocks hits a soft limit or threshold for writes. Default value is <code>16777216</code> (16 MB/sec). The allowed range is from <code>0</code> to <code>18446744073709551615</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_delete_cf","title":"<code>rocksdb_delete_cf</code>","text":"Option Description Command-line \u2013rocksdb-delete-cf Dynamic Yes Scope Global Data type String Default \u201c\u201d <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. Deletes the column family by name. The default value is \u201c\u201d, an empty string.</p> <p>For example:</p> <pre><code>SET @@global.ROCKSDB_DELETE_CF = 'cf_primary_key';\n</code></pre>"},{"location":"myrocks/variables.html#rocksdb_delete_obsolete_files_period_micros","title":"<code>rocksdb_delete_obsolete_files_period_micros</code>","text":"Option Description Command-line \u2013rocksdb-delete-obsolete-files-period-micros Dynamic No Scope Global Data type Numeric Default 21600000000 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the period in microseconds to delete obsolete files regardless of files removed during compaction. Default value is <code>21600000000</code> (6 hours). Allowed range is up to <code>9223372036854775807</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_enable_bulk_load_api","title":"<code>rocksdb_enable_bulk_load_api</code>","text":"Option Description Command-line \u2013rocksdb-enable-bulk-load-api Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to use the <code>SSTFileWriter</code> feature for bulk loading. This feature bypasses the memtable, but requires keys to be inserted into the table in either ascending or descending order. Enabled by default. If disabled, bulk loading uses the normal write path via the memtable and does not require keys to be inserted in any order.</p>"},{"location":"myrocks/variables.html#rocksdb_enable_insert_with_update_caching","title":"<code>rocksdb_enable_insert_with_update_caching</code>","text":"Option Description Command-line \u2013rocksdb-enable-insert-with-update-caching Dynamic Yes Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. Specifies whether to enable optimization where the read is cached from a failed insertion attempt in INSERT ON DUPLICATE KEY UPDATE.</p>"},{"location":"myrocks/variables.html#rocksdb_enable_iterate_bounds","title":"<code>rocksdb_enable_iterate_bounds</code>","text":"Option Description Command-line \u2013rocksdb-enable-iterate-bounds Dynamic Yes Scope Global, Local Data type Boolean Default TRUE <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. Enables the rocksdb iterator upper bounds and lower bounds in read options.</p> <p>Implemented in Percona Server for MySQL 5.7.35-38.</p>"},{"location":"myrocks/variables.html#rocksdb_enable_native_partition","title":"<code>rocksdb_enable_native_partition</code>","text":"Option Description Command-line \u2013rocksdb-enable-native-partition Dynamic No Scope Global Data type Boolean Default OFF <p>This variable is experimental and should not be used in production.</p> <p>This variable enables native partitioning and may be used when upgrading to 8.0.</p>"},{"location":"myrocks/variables.html#rocksdb_enable_pipelined_write","title":"<code>rocksdb_enable_pipelined_write</code>","text":"Option Description Command-line \u2013rocksdb-enable-pipelined-write Dynamic No Scope Global Data type Boolean Default OFF <p>DBOptions::enable_pipelined_write for RocksDB.</p> <p>The variable has been implemented in Percona Server for MySQL 5.7.35-38.</p> <p>If <code>enable_pipelined_write</code> is <code>true</code>, a separate write thread is maintained for WAL write and memtable write. A write thread first enters the WAL writer queue and then the memtable writer queue. A pending thread on the WAL writer queue only waits for the previous WAL write operations but does not wait for memtable write operations. Enabling the feature may improve write throughput and reduce latency of the prepare phase of a two-phase commit.</p>"},{"location":"myrocks/variables.html#rocksdb_enable_remove_orphaned_dropped_cfs","title":"<code>rocksdb_enable_remove_orphaned_dropped_cfs</code>","text":"Option Description Command-line \u2013rocksdb-enable-remove-orphaned-dropped-cfs Dynamic Yes Scope Global Data type Boolean Default TRUE <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. Enables the removal of dropped column families (cfs) from metadata if the cfs do not exist in the cf manager.</p> <p>The default value is <code>TRUE</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_enable_ttl","title":"<code>rocksdb_enable_ttl</code>","text":"Option Description Command-line \u2013rocksdb-enable-ttl Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to keep expired TTL records during compaction. Enabled by default. If disabled, expired TTL records will be dropped during compaction.</p>"},{"location":"myrocks/variables.html#rocksdb_enable_ttl_read_filtering","title":"<code>rocksdb_enable_ttl_read_filtering</code>","text":"Option Description Command-line \u2013rocksdb-enable-ttl-read-filtering Dynamic Yes Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.20-18. For tables with TTL, expired records are skipped/filtered out during processing and in query results. Disabling this will allow these records to be seen, but as a result, rows may disappear in the middle of transactions as they are dropped during compaction. Use with caution.</p>"},{"location":"myrocks/variables.html#rocksdb_enable_thread_tracking","title":"<code>rocksdb_enable_thread_tracking</code>","text":"Option Description Command-line \u2013rocksdb-enable-thread-tracking Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to enable tracking the status of threads accessing the database. Disabled by default. If enabled, thread status will be available via <code>GetThreadList()</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_enable_write_thread_adaptive_yield","title":"<code>rocksdb_enable_write_thread_adaptive_yield</code>","text":"Option Description Command-line \u2013rocksdb-enable-write-thread-adaptive-yield Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether the MyRocks write batch group leader should wait up to the maximum allowed time before blocking on a mutex. Disabled by default. Enable it to increase throughput for concurrent workloads.</p>"},{"location":"myrocks/variables.html#rocksdb_error_if_exists","title":"<code>rocksdb_error_if_exists</code>","text":"Option Description Command-line \u2013rocksdb-error-if-exists Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to report an error when a database already exists. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_error_on_suboptimal_collation","title":"<code>rocksdb_error_on_suboptimal_collation</code>","text":"Option Description Command-line \u2013rocksdb-error-on-suboptimal-collation Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.23-23. Specifies whether to report an error instead of a warning if an index is created on a char field where the table has a sub-optimal collation (case insensitive). Enabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_flush_log_at_trx_commit","title":"<code>rocksdb_flush_log_at_trx_commit</code>","text":"Option Description Command-line \u2013rocksdb-flush-log-at-trx-commit Dynamic Yes Scope Global, Session Data type Numeric Default 1 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to sync on every transaction commit, similar to innodb_flush_log_at_trx_commit. Enabled by default, which ensures ACID compliance.</p> <p>Possible values:</p> <ul> <li> <p><code>0</code>: Do not sync on transaction commit. This provides better performance but may lead to data inconsistency in case of a crash.</p> </li> <li> <p><code>1</code>: Sync on every transaction commit. This is set by default and recommended as it ensures data consistency, but reduces performance.</p> </li> <li> <p><code>2</code>: Sync every second.</p> </li> </ul>"},{"location":"myrocks/variables.html#rocksdb_flush_memtable_on_analyze","title":"<code>rocksdb_flush_memtable_on_analyze</code>","text":"Option Description Command-line \u2013rocksdb-flush-memtable-on-analyze Dynamic Yes Scope Global, Session Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17 and removed in Percona Server 5.7.21-20. Specifies whether to flush the memtable when running <code>ANALYZE</code> on a table. Enabled by default. This ensures accurate cardinality by including data in the memtable for calculating stats.</p>"},{"location":"myrocks/variables.html#rocksdb_force_compute_memtable_stats","title":"<code>rocksdb_force_compute_memtable_stats</code>","text":"Option Description Command-line \u2013rocksdb-force-compute-memtable-stats Dynamic Yes Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether data in the memtables should be included for calculating index statistics used by the query optimizer. Enabled by default. This provides better accuracy but may reduce performance.</p>"},{"location":"myrocks/variables.html#rocksdb_force_compute_memtable_stats_cachetime","title":"<code>rocksdb_force_compute_memtable_stats_cachetime</code>","text":"Option Description Command-line \u2013rocksdb-force-compute-memtable-stats-cachetime Dynamic Yes Scope Global Data type Numeric Default 60000000 <p>The variable has been implemented in Percona Server 5.7.20-18. Specifies for how long the cached value of memtable statistics should be used instead of computing it every time during the query plan analysis.</p>"},{"location":"myrocks/variables.html#rocksdb_force_flush_memtable_and_lzero_now","title":"<code>rocksdb_force_flush_memtable_and_lzero_now</code>","text":"Option Description Command-line \u2013rocksdb-force-flush-memtable-and-lzero-now Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Works similar to force_flush_memtable_now but also flushes all L0 files.</p>"},{"location":"myrocks/variables.html#rocksdb_force_flush_memtable_now","title":"<code>rocksdb_force_flush_memtable_now</code>","text":"Option Description Command-line \u2013rocksdb-force-flush-memtable-now Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Forces MyRocks to immediately flush all memtables out to data files.</p> <p>Warning</p> <p>Use with caution! Write requests will be blocked until all memtables are flushed.</p>"},{"location":"myrocks/variables.html#rocksdb_force_index_records_in_range","title":"<code>rocksdb_force_index_records_in_range</code>","text":"Option Description Command-line \u2013rocksdb-force-index-records-in-range Dynamic Yes Scope Global, Session Data type Numeric Default 1 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the value used to override the number of rows returned to query optimizer when <code>FORCE INDEX</code> is used. The default value is <code>1</code>. The allowed range is from <code>0</code> to <code>2147483647</code>. Set to <code>0</code> if you do not want to override the returned value.</p>"},{"location":"myrocks/variables.html#rocksdb_hash_index_allow_collision","title":"<code>rocksdb_hash_index_allow_collision</code>","text":"Option Description Command-line \u2013rocksdb-hash-index-allow-collision Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether hash collisions are allowed. Enabled by default, which uses less memory. If disabled, the full prefix is stored to prevent hash collisions.</p>"},{"location":"myrocks/variables.html#rocksdb_ignore_unknown_options","title":"<code>rocksdb_ignore_unknown_options</code>","text":"Option Description Command-line Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.20-18. When enabled, it allows RocksDB to receive unknown options and not exit.</p>"},{"location":"myrocks/variables.html#rocksdb_index_type","title":"<code>rocksdb_index_type</code>","text":"Option Description Command-line \u2013rocksdb-index-type Dynamic No Scope Global Data type Enum Default kBinarySearch <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the type of indexing used by MyRocks:</p> <ul> <li> <p><code>kBinarySearch</code>: Binary search (default).</p> </li> <li> <p><code>kHashSearch</code>: Hash search.</p> </li> </ul>"},{"location":"myrocks/variables.html#rocksdb_info_log_level","title":"<code>rocksdb_info_log_level</code>","text":"Option Description Command-line \u2013rocksdb-info-log-level Dynamic Yes Scope Global Data type Enum Default error_level <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the level for filtering messages written by MyRocks to the <code>mysqld</code> log.</p> <ul> <li> <p><code>debug_level</code>: Maximum logging (everything including debugging log messages)</p> </li> <li> <p><code>info_level</code></p> </li> <li> <p><code>warn_level</code></p> </li> <li> <p><code>error_level</code> (default)</p> </li> <li> <p><code>fatal_level</code>: Minimum logging (only fatal error messages logged)</p> </li> </ul>"},{"location":"myrocks/variables.html#rocksdb_is_fd_close_on_exec","title":"<code>rocksdb_is_fd_close_on_exec</code>","text":"Option Description Command-line \u2013rocksdb-is-fd-close-on-exec Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether child processes should inherit open file handles. Enabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_large_prefix","title":"<code>rocksdb_large_prefix</code>","text":"Option Description Command-line \u2013rocksdb-large-prefix Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.20-18. When enabled, this option allows index key prefixes longer than 767 bytes (up to 3072 bytes). This option mirrors the innodb_large_prefix The values for rocksdb_large_prefix should be the same between source and replica.</p>"},{"location":"myrocks/variables.html#rocksdb_keep_log_file_num","title":"<code>rocksdb_keep_log_file_num</code>","text":"Option Description Command-line \u2013rocksdb-keep-log-file-num Dynamic No Scope Global Data type Numeric Default 1000 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the maximum number of info log files to keep. The default value is <code>1000</code>. The allowed range is from <code>1</code> to <code>18446744073709551615</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_lock_scanned_rows","title":"<code>rocksdb_lock_scanned_rows</code>","text":"Option Description Command-line \u2013rocksdb-lock-scanned-rows Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to hold the lock on rows that are scanned during <code>UPDATE</code> and not actually updated. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_lock_wait_timeout","title":"<code>rocksdb_lock_wait_timeout</code>","text":"Option Description Command-line \u2013rocksdb-lock-wait-timeout Dynamic Yes Scope Global, Session Data type Numeric Default 1 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the number of seconds MyRocks should wait to acquire a row loc before aborting the request. The default value is <code>1</code>. The allowed range is up to <code>1073741824</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_log_file_time_to_roll","title":"<code>rocksdb_log_file_time_to_roll</code>","text":"Option Description Command-line \u2013rocksdb-log-file-time-to-roll Dynamic No Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the period (in seconds) for rotating the info log files. The default value is <code>0</code>, meaning that the log file is not rotated. The allowed range is up to <code>18446744073709551615</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_manifest_preallocation_size","title":"<code>rocksdb_manifest_preallocation_size</code>","text":"Option Description Command-line \u2013rocksdb-manifest-preallocation-size Dynamic No Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the number of bytes to preallocate for the MANIFEST file used by MyRocks to store information about column families, levels, active files, etc. The default value is <code>0</code>. The allowed range is up to <code>18446744073709551615</code>.</p> <p>Note</p> <p>A value of <code>4194304</code> (4 MB) is reasonable to reduce random I/O on XFS.</p>"},{"location":"myrocks/variables.html#rocksdb_manual_compaction_bottommost_level","title":"<code>rocksdb_manual_compaction_bottommost_level</code>","text":"Option Description Command-line \u2013rocksdb-manual-compaction-bottommost-level Dynamic Yes Scope Global, Session Data type Enum Default kForceOptimized <p>The variable has been implemented in Percona Server for MySQL 5.7.35-38.</p> <p>Option for skipping bottommost level compaction during manual compaction. The values are the following:</p> <ul> <li> <p><code>kSkip</code> - Skip bottommost level compaction</p> </li> <li> <p><code>kIfHaveCompactionFilter</code> - Only compact the bottommost level if there is a compaction filter</p> </li> <li> <p><code>kForce</code> - Always compact the bottommost level</p> </li> <li> <p><code>kForceOptimized</code> - The default value. Always compact the bottommost level but in the bottommost level avoid double-compacting files created</p> </li> </ul>"},{"location":"myrocks/variables.html#rocksdb_manual_wal_flush","title":"<code>rocksdb_manual_wal_flush</code>","text":"Option Description Command-line \u2013rocksdb-manual-wal-flush Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.20-18. This variable can be used to disable automatic/timed WAL flushing and instead rely on the application to do the flushing.</p>"},{"location":"myrocks/variables.html#rocksdb_master_skip_tx_api","title":"<code>rocksdb_master_skip_tx_api</code>","text":"Option Description Command-line Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. When enabled, uses the WriteBatch API, which is faster. The session does not hold any lock-on-row access. This variable is not effective on replicas.</p> <p>Note</p> <p>Due to the disabled row locks, improper use of the variable can cause data corruption or inconsistency.</p>"},{"location":"myrocks/variables.html#rocksdb_max_background_compactions","title":"<code>rocksdb_max_background_compactions</code>","text":"Option Description Command-line \u2013rocksdb-max-background-compactions Dynamic Yes Scope Global Data type Numeric Default -1 <p>The variable has been implemented in Percona Server 5.7.19-17. This variable has been replaced in Percona Server 5.7.20-18 by rocksdb_max_background_jobs, which automatically decides how many threads to allocate towards flush/compaction. This variable has been re-implemented in Percona Server for MySQL 5.7.31-34.</p> <p>Sets DBOptions:: max_background_compactions for RocksDB. The default value is <code>-1</code>. The allowed range is up to <code>64</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_max_background_flushes","title":"<code>rocksdb_max_background_flushes</code>","text":"Option Description Command-line \u2013rocksdb-max-background-flushes Dynamic No Scope Global Data type Numeric Default -1 <p>The variable has been implemented in Percona Server 5.7.19-17. This variable has been replaced in Percona Server 5.7.20-18 by rocksdb_max_background_jobs, which automatically decides how many threads to allocate towards flush/compaction. This variable has been re-implemented in Percona Server for MySQL 5.7.31-34.</p> <p>Sets DBOptions:: max_background_flushes for RocksDB. The default value is <code>-1</code>. The allowed range is up to <code>64</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_max_background_jobs","title":"<code>rocksdb_max_background_jobs</code>","text":"Option Description Command-line \u2013rocksdb-max-background-jobs Dynamic Yes Scope Global Data type Numeric Default 2 <p>This variable has been implemented in Percona Server 5.7.20-18 to replace rocksdb_base_background_compactions, rocksdb_max_background_compactions, and rocksdb_max_background_flushes variables. This variable specifies the maximum number of background jobs. It automatically decides how many threads to allocate towards flush/compaction. It was implemented to reduce the number of (confusing) options for users and can tweak and push the responsibility down to RocksDB level.</p>"},{"location":"myrocks/variables.html#rocksdb_max_bottom_pri_background_compactions","title":"<code>rocksdb_max_bottom_pri_background_compactions</code>","text":"Option Description Command-line \u2013rocksdb_max_bottom_pri_background_compactions Dynamic No Data type Unsigned integer Default 0 <p>This variable has been implemented in Percona Server for MySQL 5.7.31-34. Creates a specified number of threads, sets a lower CPU priority, and lets compactions use them. The maximum compaction concurrency is capped by <code>rocksdb_max_background_compactions</code> or <code>rocksdb_max_background_jobs</code></p> <p>The minimum value is <code>0</code> and the maximum value is <code>64</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_max_latest_deadlocks","title":"<code>rocksdb_max_latest_deadlocks</code>","text":"Option Description Command-line \u2013rocksdb-max-latest-deadlocks Dynamic Yes Scope Global Data type Numeric Default 5 <p>This variable has been implemented in Percona Server 5.7.20-18. Specifies the maximum number of recent deadlocks to store.</p>"},{"location":"myrocks/variables.html#rocksdb_max_log_file_size","title":"<code>rocksdb_max_log_file_size</code>","text":"Option Description Command-line \u2013rocksdb-max-log-file-size Dynamic No Scope Global Data type Numeric Default 0 <p>This variable has been implemented in Percona Server 5.7.19-17. Specifies the maximum size for info log files, after which the log is rotated. The default value is <code>0</code>, meaning that only one log file is used. The allowed range is up to <code>18446744073709551615</code>.</p> <p>Also see rocksdb_log_file_time_to_roll.</p>"},{"location":"myrocks/variables.html#rocksdb_max_manifest_file_size","title":"<code>rocksdb_max_manifest_file_size</code>","text":"Option Description Command-line \u2013rocksdb-manifest-log-file-size Dynamic No Scope Global Data type Numeric Default 18446744073709551615 <p>This variable has been implemented in Percona Server 5.7.19-17. Specifies the maximum size of the MANIFEST data file, after which it is rotated. The default value is also the maximum, making it practically unlimited: only one manifest file is used.</p>"},{"location":"myrocks/variables.html#rocksdb_max_open_files","title":"<code>rocksdb_max_open_files</code>","text":"Option Description Command-line \u2013rocksdb-max-open-files Dynamic No Scope Global Data type Numeric Default 1000 <p>This variable has been implemented in Percona Server 5.7.19-17. Default value has changed to <code>1000</code> in Percona Server 5.7.19-17. Specifies the maximum number of file handles opened by MyRocks. Values in the range between <code>0</code> and <code>open_files_limit</code> are taken as they are. If rocksdb_max_open_files value is greater than <code>open_files_limit</code>, it will be reset to \u00bd of <code>open_files_limit</code>, and a warning will be emitted to the <code>mysqld</code> error log. A value of <code>-2</code> denotes auto-tuning: just sets rocksdb_max_open_files value to \u00bd of <code>open_files_limit</code>. Finally, <code>-1</code> means no limit, i.e. an infinite number of file handles.</p> <p>Warning</p> <p>Setting rocksdb_max_open_files to <code>-1</code> is dangerous, as the server may quickly run out of file handles in this case.</p>"},{"location":"myrocks/variables.html#rocksdb_max_row_locks","title":"<code>rocksdb_max_row_locks</code>","text":"Option Description Command-line \u2013rocksdb-max-row-locks Dynamic Yes Scope Global Data type Numeric Default 1048576 <p>This variable has been implemented in Percona Server 5.7.19-17. The default value has changed from <code>1073741824</code> to <code>1048576</code> in Percona Server 5.7.21-21. The scope has changed to <code>Global</code> in Percona Server for MySQL 5.7.32-35. Specifies the limit on the maximum number of row locks a transaction can have before it fails. The default value is also the maximum, making it practically unlimited: transactions never fail due to row locks.</p>"},{"location":"myrocks/variables.html#rocksdb_max_subcompactions","title":"<code>rocksdb_max_subcompactions</code>","text":"Option Description Command-line \u2013rocksdb-max-subcompactions Dynamic No Scope Global Data type Numeric Default 1 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the maximum number of threads allowed for each compaction job. A default value of <code>1</code> means no subcompactions (one thread per compaction job). The allowed range is up to <code>64</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_max_total_wal_size","title":"<code>rocksdb_max_total_wal_size</code>","text":"Option Description Command-line \u2013rocksdb-max-total-wal-size Dynamic No Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the maximum total size of WAL (write-ahead log) files, after which memtables are flushed. The default value is <code>0</code>: the WAL size limit is chosen dynamically. The allowed range is up to <code>9223372036854775807</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_merge_buf_size","title":"<code>rocksdb_merge_buf_size</code>","text":"Option Description Command-line \u2013rocksdb-merge-buf-size Dynamic Yes Scope Global Data type Numeric Default 67108864 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the size (in bytes) of the merge-sort buffers used to accumulate data during secondary key creation. New entries are written directly to the lowest level in the database, instead of updating indexes through the memtable and L0. These values are sorted using merge-sort, with buffers set to 64 MB by default (<code>67108864</code>). The allowed range is from <code>100</code> to <code>18446744073709551615</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_merge_combine_read_size","title":"<code>rocksdb_merge_combine_read_size</code>","text":"Option Description Command-line \u2013rocksdb-merge-combine-read-size Dynamic Yes Scope Global Data type Numeric Default 1073741824 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the size (in bytes) of the merge-combine buffer used for the merge-sort algorithm as described in rocksdb_merge_buf_size. The default size is 1 GB (<code>1073741824</code>). The allowed range is from <code>100</code> to <code>18446744073709551615</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_merge_tmp_file_removal_delay_ms","title":"<code>rocksdb_merge_tmp_file_removal_delay_ms</code>","text":"Option Description Command-line \u2013rocksdb_merge_tmp_file_removal_delay_ms Dynamic Yes Scope Global, Session Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.20-18. Fast secondary index creation creates merge files when needed. After finishing secondary index creation, merge files are removed. By default, the file removal is done without any sleep, so removing GBs of merge files within &lt;1s may happen, which will cause trim stalls on Flash. This variable can be used to rate limit the delay in milliseconds.</p>"},{"location":"myrocks/variables.html#rocksdb_new_table_reader_for_compaction_inputs","title":"<code>rocksdb_new_table_reader_for_compaction_inputs</code>","text":"Option Description Command-line \u2013rocksdb-new-table-reader-for-compaction-inputs Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether MyRocks should create a new file descriptor and table reader for each compaction input. Disabled by default. Enabling this may increase memory consumption, but will also allow pre-fetch options to be specified for compaction input files without impacting table readers used for user queries.</p>"},{"location":"myrocks/variables.html#rocksdb_no_block_cache","title":"<code>rocksdb_no_block_cache</code>","text":"Option Description Command-line \u2013rocksdb-no-block-cache Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to disable the block cache for column families. Variable is disabled by default, meaning that using the block cache is allowed.</p>"},{"location":"myrocks/variables.html#rocksdb_no_create_column_family","title":"<code>rocksdb_no_create_column_family</code>","text":"Option Description Command-line \u2013rocksdb-no-create-column-family Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.23-24. Specifies whether column families can be created implicitly via an index comment. If this variable is set to <code>ON</code>, then column families must already exist or must be present within the rocksdb_override_cf_options for a user to assign and index to a column family.</p>"},{"location":"myrocks/variables.html#rocksdb_override_cf_options","title":"<code>rocksdb_override_cf_options</code>","text":"Option Description Command-line \u2013rocksdb-override-cf-options Dynamic No Scope Global Data type String Default <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies option overrides for each column family. Empty by default.</p>"},{"location":"myrocks/variables.html#rocksdb_paranoid_checks","title":"<code>rocksdb_paranoid_checks</code>","text":"Option Description Command-line \u2013rocksdb-paranoid-checks Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether MyRocks should re-read the data file as soon as it is created to verify correctness. Enabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_pause_background_work","title":"<code>rocksdb_pause_background_work</code>","text":"Option Description Command-line \u2013rocksdb-pause-background-work Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether MyRocks should pause all background operations. Disabled by default. There is no practical reason for a user to ever use this variable because it is intended as a test synchronization tool for the MyRocks MTR test suites.</p> <p>Warning</p> <p>If someone were to set a rocksdb_force_flush_memtable_now to <code>1</code> while rocksdb_pause_background_work is set to <code>1</code>, the client that issued the <code>rocksdb_force_flush_memtable_now=1</code> will be blocked indefinitely until rocksdb_pause_background_work is set to <code>0</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_perf_context_level","title":"<code>rocksdb_perf_context_level</code>","text":"Option Description Command-line \u2013rocksdb-perf-context-level Dynamic Yes Scope Global, Session Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the level of information to capture with the Perf Context plugins. The default value is <code>0</code>. The allowed range is up to <code>5</code>.</p> Value Description 0 Unknown setting 1 Disable perf stats 2 Enable only count stats 3 Enable count stats and time stats except for mutexes 4 Enable count stats, time stats, except for wall time or CPU time for mutexes 5 Enable count and time stats"},{"location":"myrocks/variables.html#rocksdb_persistent_cache_path","title":"<code>rocksdb_persistent_cache_path</code>","text":"Option Description Command-line \u2013rocksdb-persistent-cache-path Dynamic No Scope Global Data type String Default <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the path to the persistent cache. Set this together with rocksdb_persistent_cache_size_mb.</p>"},{"location":"myrocks/variables.html#rocksdb_persistent_cache_size_mb","title":"<code>rocksdb_persistent_cache_size_mb</code>","text":"Option Description Command-line \u2013rocksdb-persistent-cache-size-mb Dynamic No Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the size of the persistent cache in megabytes. Default is <code>0</code> (persistent cache disabled). The allowed range is up to <code>18446744073709551615</code>. Set this together with rocksdb_persistent_cache_path.</p>"},{"location":"myrocks/variables.html#rocksdb_pin_l0_filter_and_index_blocks_in_cache","title":"<code>rocksdb_pin_l0_filter_and_index_blocks_in_cache</code>","text":"Option Description Command-line \u2013rocksdb-pin-l0-filter-and-index-blocks-in-cache Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether MyRocks pins the filter and index blocks in the cache if rocksdb_cache_index_and_filter_blocks is enabled. Enabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_print_snapshot_conflict_queries","title":"<code>rocksdb_print_snapshot_conflict_queries</code>","text":"Option Description Command-line \u2013rocksdb-print-snapshot-conflict-queries Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether queries that generate snapshot conflicts should be logged to the error log. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_rate_limiter_bytes_per_sec","title":"<code>rocksdb_rate_limiter_bytes_per_sec</code>","text":"Option Description Command-line \u2013rocksdb-rate-limiter-bytes-per-sec Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the maximum rate at which MyRocks can write to media via memtable flushes and compaction. The default value is <code>0</code> (the write rate is not limited). The allowed range is up to <code>9223372036854775807</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_read_free_rpl","title":"<code>rocksdb_read_free_rpl</code>","text":"Option Description Command-line \u2013rocksdb-read-free-rpl Dynamic Yes Scope Global Data type Enum Default OFF <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. se read-free replication, which allows no row lookup during replication, on the replica.</p> <p>The options are the following:</p> <ul> <li> <p>OFF - Disables the variable</p> </li> <li> <p>PK_SK - Enables the variable on all tables with a primary key</p> </li> <li> <p>PK_ONLY - Enables the variable on tables where the only key is the primary key</p> </li> </ul>"},{"location":"myrocks/variables.html#rocksdb_read_free_rpl_tables","title":"<code>rocksdb_read_free_rpl_tables</code>","text":"Option Description Command-line \u2013rocksdb-read-free-rpl-tables Dynamic Yes Scope Global, Session Data type String Default <p>This variable is disabled in Percona Server for MySQL 5.7.30-33. We recommend that you use <code>rocksdb_read_free_rpl</code> instead of this variable.</p> <p>The variable has been implemented in Percona Server 5.7.19-17 and disabled in Percona Server for MySQL 5.7.30-33. Lists tables (as a regular expression) that should use read-free replication on the replica (that is, replication without row lookups). Empty by default.</p>"},{"location":"myrocks/variables.html#rocksdb_records_in_range","title":"<code>rocksdb_records_in_range</code>","text":"Option Description Command-line \u2013rocksdb-records-in-range Dynamic Yes Scope Global, Session Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the value to override the result of <code>records_in_range()</code>. The default value is <code>0</code>. The allowed range is up to <code>2147483647</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_reset_stats","title":"<code>rocksdb_reset_stats</code>","text":"Option Description Command-line \u2013rocksdb-reset-stats Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Resets MyRocks internal statistics dynamically (without restarting the server).</p>"},{"location":"myrocks/variables.html#rocksdb_rollback_on_timeout","title":"<code>rocksdb_rollback_on_timeout</code>","text":"Option Description Command-line \u2013rocksdb-rollback-on-timeout Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. By default, only the last statement on a transaction is rolled back. If <code>--rocksdb-rollback-on-timeout=ON</code>, a transaction timeout causes a rollback of the entire transaction.</p>"},{"location":"myrocks/variables.html#rocksdb_rpl_skip_tx_api","title":"<code>rocksdb_rpl_skip_tx_api</code>","text":"Option Description Command-line \u2013rocksdb-rpl-skip-tx-api Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17 and removed in Percona Server 5.7.20-19. The variable has been re-implemented in Percona Server 5.7.21-21. Specifies whether write batches should be used for replication thread instead of the transaction API. Disabled by default.</p> <p>There are two conditions that are necessary to use it: row replication format and replica operating in super read-only mode.</p>"},{"location":"myrocks/variables.html#rocksdb_seconds_between_stat_computes","title":"<code>rocksdb_seconds_between_stat_computes</code>","text":"Option Description Command-line \u2013rocksdb-seconds-between-stat-computes Dynamic Yes Scope Global Data type Numeric Default 3600 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the number of seconds to wait between recomputation of table statistics for the optimizer. During that time, only changed indexes are updated. The default value is <code>3600</code>. The allowed value is from <code>0</code> to <code>4294967295</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_signal_drop_index_thread","title":"<code>rocksdb_signal_drop_index_thread</code>","text":"Option Description Command-line \u2013rocksdb-signal-drop-index-thread Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Signals the MyRocks drop index thread to wake up.</p>"},{"location":"myrocks/variables.html#rocksdb_sim_cache_size","title":"<code>rocksdb_sim_cache_size</code>","text":"Option Description Command-line \u2013rocksdb-sim-cache-size Dynamic No Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.20-18. Enables the simulated cache, which allows us to figure out the hit/miss rate with a specific cache size without changing the real block cache.</p>"},{"location":"myrocks/variables.html#rocksdb_skip_bloom_filter_on_read","title":"<code>rocksdb_skip_bloom_filter_on_read</code>","text":"Option Description Command-line \u2013rocksdb-skip-bloom-filter-on_read Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether bloom filters should be skipped on reads. Disabled by default (bloom filters are not skipped).</p>"},{"location":"myrocks/variables.html#rocksdb_skip_fill_cache","title":"<code>rocksdb_skip_fill_cache</code>","text":"Option Description Command-line \u2013rocksdb-skip-fill-cache Dynamic Yes Scope Global, Session Data type Boolean Default OFF` <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to skip caching data on the read requests. Disabled by default (caching is not skipped).</p>"},{"location":"myrocks/variables.html#rocksdb_skip_locks_if_skip_unique_check","title":"<code>rocksdb_skip_locks_if_skip_unique_check</code>","text":"Option Description Command-line rocksdb_skip_locks_if_skip_unique_check Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server for MySQL 5.7.35-38. Skips row locking when unique checks are disabled.</p>"},{"location":"myrocks/variables.html#rocksdb_sst_mgr_rate_bytes_per_sec","title":"<code>rocksdb_sst_mgr_rate_bytes_per_sec</code>","text":"Option Description Command-line \u2013rocksdb-sst-mgr-rate-bytes-per-sec Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. The default value has changed from <code>67108864</code> to <code>0</code> in Percona Server 5.7.20-18. Specifies the maximum rate for writing to data files. The default value is <code>0</code>. This option is not effective on HDD. The allowed range is from <code>0</code> to <code>18446744073709551615</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_stats_dump_period_sec","title":"<code>rocksdb_stats_dump_period_sec</code>","text":"Option Description Command-line \u2013rocksdb-stats-dump-period-sec Dynamic No Scope Global Data type Numeric Default 600 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the period in seconds for performing a dump of the MyRocks statistics to the info log. The default value is <code>600</code>. The allowed range is up to <code>2147483647</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_stats_level","title":"<code>rocksdb_stats_level</code>","text":"Option Description Command-line \u2013rocksdb-stats-level Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. Controls the RocksDB statistics level. The default value is \u201c0\u201d (kExceptHistogramOrTimers), which is the fastest level. The maximum value is \u201c4\u201d.</p>"},{"location":"myrocks/variables.html#rocksdb_stats_recalc_rate","title":"<code>rocksdb_stats_recalc_rate</code>","text":"Option Description Command-line \u2013rocksdb-stats-recalc-rate Dynamic No Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.23-23. Specifies the number of indexes to recalculate per second. Recalculating index statistics periodically ensures it matches the actual sum from SST files. The default value is <code>0</code>. The allowed range is up to <code>4294967295</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_store_row_debug_checksums","title":"<code>rocksdb_store_row_debug_checksums</code>","text":"Option Description Command-line \u2013rocksdb-store-row-debug-checksums Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to include checksums when writing index or table records. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_strict_collation_check","title":"<code>rocksdb_strict_collation_check</code>","text":"Option Description Command-line \u2013rocksdb-strict-collation-check Dynamic Yes Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to check and verify that table indexes have proper collation settings. Enabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_strict_collation_exceptions","title":"<code>rocksdb_strict_collation_exceptions</code>","text":"Option Description Command-line \u2013rocksdb-strict-collation-exceptions Dynamic Yes Scope Global Data type String Default <p>The variable has been implemented in Percona Server 5.7.19-17. Lists tables (as a regular expression) that should be excluded from verifying case-sensitive collation enforced by rocksdb_strict_collation_check. Empty by default.</p>"},{"location":"myrocks/variables.html#rocksdb_table_cache_numshardbits","title":"<code>rocksdb_table_cache_numshardbits</code>","text":"Option Description Command-line \u2013rocksdb-table-cache-numshardbits Dynamic No Scope Global Data type Numeric Default 6 <p>The variable has been implemented in Percona Server 5.7.19-17. Max value has been changed from <code>2147483647</code> to <code>19</code> in Percona Server 5.7.20-18. Specifies the number of table caches. The default value is <code>6</code>. The allowed range is from <code>0</code> to <code>19</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_table_stats_background_thread_nice_value","title":"<code>rocksdb_table_stats_background_thread_nice_value</code>","text":"Option Description Command-line \u2013rocksdb-table-stats-background-thread-nice-value Dynamic Yes Scope Global Data type Numeric Default 19 <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. The nice value for index stats. The minimum = -20 (THREAD_PRIO_MIN) The maximum = 19 (THREAD_PRIO_MAX)</p>"},{"location":"myrocks/variables.html#rocksdb_table_stats_max_num_rows_scanned","title":"<code>rocksdb_table_stats_max_num_rows_scanned</code>","text":"Option Description Command-line \u2013rocksdb-table-stats-max-num-rows-scanned Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. The maximum number of rows to scan in a table scan is based on a cardinality calculation. The minimum is <code>0</code> (every modification triggers a stats recalculation). The maximum is <code>18,446,744,073,709,551,615</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_table_stats_recalc_threshold_count","title":"<code>rocksdb_table_stats_recalc_threshold_count</code>","text":"Option Description Command-line \u2013rocksdb-table-stats-recalc-threshold-count Dynamic Yes Scope Global Data type Numeric Default 100 <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. The number of modified rows to trigger a stats recalculation. This is a dependent variable for stats recalculation. The minimum is <code>0</code>. The maximum is <code>18,446,744,073,709,551,615</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_table_stats_recalc_threshold_pct","title":"<code>rocksdb_table_stats_recalc_threshold_pct</code>","text":"Option Description Command-line \u2013rocksdb-table-stats-recalc-threshold-pct Dynamic Yes Scope Global Data type Numeric Default 10 <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. The percentage of the number of modified rows over the total number of rows to trigger stats recalculations. This is a dependent variable for stats recalculation. The minimum value is <code>0</code> The maximum value is <code>100</code> (RDB_TBL_STATS_RECALC_THRESHOLD_PCT_MAX).</p>"},{"location":"myrocks/variables.html#rocksdb_table_stats_sampling_pct","title":"<code>rocksdb_table_stats_sampling_pct</code>","text":"Option Description Command-line \u2013rocksdb-table-stats-sampling-pct Dynamic Yes Scope Global Data type Numeric Default 10 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the percentage of entries to sample when collecting statistics about table properties. The default value is <code>10</code>. The allowed range is from <code>0</code> to <code>100</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_table_stats_use_table_scan","title":"<code>rocksdb_table_stats_use_table_scan</code>","text":"Option Description Command-line \u2013rocksdb-table-stats-use-table-scan Dynamic Yes Scope Global Data type Boolean Default FALSE <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. Enables table-scan-based index calculations. The default value is <code>FALSE</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_tmpdir","title":"<code>rocksdb_tmpdir</code>","text":"Option Description Command-line \u2013rocksdb-tmpdir Dynamic Yes Scope Global, Session Data type String Default <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the path to the directory for temporary files during DDL operations.</p>"},{"location":"myrocks/variables.html#rocksdb_trace_block_cache_access","title":"<code>rocksdb_trace_block_cache_access</code>","text":"Option Description Command-line \u2013rocksdb-trace-block-cache-access Dynamic Yes Scope Global Data type String Default \"\" <p>The variable has been implemented in Percona Server for MySQL 5.7.30-33. Defines the block cache trace option string. The format is \u201csampling frequency: max_trace_file_size:trace_file_name.\u201d The sampling frequency value and max_trace_file_size value are positive integers. The block accesses are saved to the <code>rocksdb_datadir/block_cache_traces/trace_file_name</code>. The default value is an empty string (\"\").</p>"},{"location":"myrocks/variables.html#rocksdb_trace_queries","title":"<code>rocksdb_trace_queries</code>","text":"Option Description Command-line \u2013rocksdb-trace-queries Dynamic Yes Scope Global Data type String Default \"\" <p>The variable has been implemented in Percona Server for MySQL 5.7.35-38. This variable is a trace option string. The format is <code>sampling_frequency:max_trace_file_size:trace_file_name</code>. The <code>sampling_frequency</code> value and <code>max_trace_file_size</code> value are positive integers. The queries are saved to the rocksdb_datadir/queries_traces/trace_file_name.</p> <p>The file size unit is measured in bytes.</p> <p>The sampling frequency specifies that one request is sampled from <code>sampling_frequency</code> requests. If the request is <code>1</code>, all the requests are traced. If the request is <code>5</code>, then for every five requests, one request is traced.</p>"},{"location":"myrocks/variables.html#rocksdb_trace_sst_api","title":"<code>rocksdb_trace_sst_api</code>","text":"Option Description Command-line \u2013rocksdb-trace-sst-api Dynamic Yes Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to generate trace output in the log for each call to <code>SstFileWriter</code>. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_track_and_verify_wals_in_manifest","title":"<code>rocksdb_track_and_verify_wals_in_manifest</code>","text":"Option Description Command-line \u2013rocksdb-track-and-verify-wals-in-manifest Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server for MySQL 5.7.35-38.</p> <p>DBOptions::track_and_verify_wals_in_manifest for RocksDB</p> <p>If true, the log numbers and sizes of the synced WALs are tracked in Manifest, then, during a DB recovery, if a synced WAL is missing from the disk, or the size of the WAL does not match the recorded size in Manifest, an error is reported adn the recovery is aborted.</p> <p>Note</p> <p>This option does not work with a secondary instance.</p>"},{"location":"myrocks/variables.html#rocksdb_two_write_queues","title":"<code>rocksdb_two_write_queues</code>","text":"Option Description Command-line \u2013rocksdb-track-and-verify-wals-in-manifest Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.21-20. When enabled this variable allows/encourages threads that are using two-phase commit to <code>prepare</code> in parallel.</p>"},{"location":"myrocks/variables.html#rocksdb_unsafe_for_binlog","title":"<code>rocksdb_unsafe_for_binlog</code>","text":"Option Description Command-line \u2013rocksdb-unsafe-for-binlog Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to allow statement-based binary logging which may break consistency. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_update_cf_options","title":"<code>rocksdb_update_cf_options</code>","text":"Option Description Command-line \u2013rocksdb-update-cf-options Dynamic No Scope Global Data type String Default <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies option updates for each column family. Empty by default.</p>"},{"location":"myrocks/variables.html#rocksdb_use_adaptive_mutex","title":"<code>rocksdb_use_adaptive_mutex</code>","text":"Option Description Command-line \u2013rocksdb-use-adaptive-mutex Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to use an adaptive mutex which spins in user space before resorting to the kernel. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_use_default_sk_cf","title":"<code>rocksdb_use_default_sk_cf</code>","text":"Option Description Command-line \u2013rocksdb-use-default-sk-cf Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server for MySQL 5.7.32-35. Use <code>default_sk</code> column family for secondary keys.</p>"},{"location":"myrocks/variables.html#rocksdb_use_direct_io_for_flush_and_compaction","title":"<code>rocksdb_use_direct_io_for_flush_and_compaction</code>","text":"Option Description Command-line \u2013rocksdb-use-direct-io-for-flush-and-compaction Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to write to data files directly, without caches or buffers. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_use_direct_reads","title":"<code>rocksdb_use_direct_reads</code>","text":"Option Description Command-line \u2013rocksdb-use-direct-reads Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to read data files directly, without caches or buffers. Disabled by default. If you enable this, make sure that rocksdb_allow_mmap_reads is disabled.</p>"},{"location":"myrocks/variables.html#rocksdb_use_fsync","title":"<code>rocksdb_use_fsync</code>","text":"Option Description Command-line \u2013rocksdb-use-fsync Dynamic No Scope Global Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether MyRocks should use <code>fsync</code> instead of <code>fdatasync</code> when requesting a sync of a data file. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_validate_tables","title":"<code>rocksdb_validate_tables</code>","text":"Option Description Command-line \u2013rocksdb-validate-tables Dynamic No Scope Global Data type Numeric Default 1 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to verify that MySQL <code>.frm</code> files match MyRocks tables.</p> <ul> <li> <p><code>0</code>: do not verify.</p> </li> <li> <p><code>1</code>: verify and fail on error (default).</p> </li> <li> <p><code>2</code>: verify and continue with the error.</p> </li> </ul>"},{"location":"myrocks/variables.html#rocksdb_verify_row_debug_checksums","title":"<code>rocksdb_verify_row_debug_checksums</code>","text":"Option Description Command-line \u2013rocksdb-verify-row-debug-checksums Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to verify checksums when reading index or table records. Disabled by default.</p>"},{"location":"myrocks/variables.html#rocksdb_wal_bytes_per_sync","title":"<code>rocksdb_wal_bytes_per_sync</code>","text":"Option Description Command-line \u2013rocksdb-wal-bytes-per-sync Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. The variable has been changed to dynamic in Percona Server 5.7.21-20. Specifies how often should the OS sync WAL (write-ahead log) files to the disk as they are being written, asynchronously, in the background. This operation can be used to smooth out write I/O over time. The default value is <code>0</code>, meaning that files are never synced. The allowed range is up to <code>18446744073709551615</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_wal_dir","title":"<code>rocksdb_wal_dir</code>","text":"Option Description Command-line \u2013rocksdb-wal-dir Dynamic No Scope Global Data type String Default <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the path to the directory where MyRocks stores WAL files.</p>"},{"location":"myrocks/variables.html#rocksdb_wal_recovery_mode","title":"<code>rocksdb_wal_recovery_mode</code>","text":"Option Description Command-line \u2013rocksdb-wal-recovery-mode Dynamic Yes Scope Global Data type Numeric Default 2 <p>Note</p> <p>In version 5.7.31-34 and later, the default is changed from <code>1</code> to <code>2</code>.</p> <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the level of tolerance when recovering write-ahead logs (WAL) files after a system crash.</p> <p>The following are the options:</p> <ul> <li> <p><code>0</code>: if the last WAL entry is corrupted, truncate the entry and either start the server normally or refuse to start.</p> </li> <li> <p><code>1</code>: if a WAL entry is corrupted, the server fails to start and does not recover from the crash.</p> </li> <li> <p><code>2</code> (default): if a corrupted WAL entry is detected, truncate all entries after the detected corrupted entry. You can select this setting for replication replicas.</p> </li> <li> <p><code>3</code>: If a corrupted WAL entry is detected, skip only the corrupted entry and continue the apply WAL entries. This option can be dangerous.</p> </li> </ul>"},{"location":"myrocks/variables.html#rocksdb_wal_size_limit_mb","title":"<code>rocksdb_wal_size_limit_mb</code>","text":"Option Description Command-line \u2013rocksdb-wal-size-limit-mb Dynamic No Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the maximum size of all WAL files in megabytes before attempting to flush memtables and delete the oldest files. The default value is <code>0</code> (never rotated). The allowed range is up to <code>9223372036854775807</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_wal_ttl_seconds","title":"<code>rocksdb_wal_ttl_seconds</code>","text":"Option Description Command-line \u2013rocksdb-wal-ttl-seconds Dynamic No Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies the timeout in seconds before deleting archived WAL files. The default is <code>0</code> (WAL files are not archived). The allowed range is up to <code>9223372036854775807</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_whole_key_filtering","title":"<code>rocksdb_whole_key_filtering</code>","text":"Option Description Command-line \u2013rocksdb-whole-key-filtering Dynamic No Scope Global Data type Boolean Default ON <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether the bloomfilter should use the whole key for filtering instead of just the prefix. Enabled by default. Make sure that lookups use the whole key for matching.</p>"},{"location":"myrocks/variables.html#rocksdb_write_batch_max_bytes","title":"<code>rocksdb_write_batch_max_bytes</code>","text":"Option Description Command-line \u2013rocksdb-write-batch-max-bytes Dynamic Yes Scope Global Data type Numeric Default 0 <p>The variable has been implemented in Percona Server 5.7.20-18. Specifies the maximum size of a RocksDB write batch in bytes. <code>0</code> means no limit. In case the user exceeds the limit following error is shown:</p> <p><code>ERROR HY000: Status error 10 received from RocksDB: Operation aborted: Memory limit reached</code>.</p>"},{"location":"myrocks/variables.html#rocksdb_write_disable_wal","title":"<code>rocksdb_write_disable_wal</code>","text":"Option Description Command-line \u2013rocksdb-write-disable-wal Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Lets you temporarily disable the writes to WAL files, which can be useful for bulk loading.</p>"},{"location":"myrocks/variables.html#rocksdb_write_ignore_missing_column_families","title":"<code>rocksdb_write_ignore_missing_column_families</code>","text":"Option Description Command-line \u2013rocksdb-write-ignore-missing-column-families Dynamic Yes Scope Global, Session Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.19-17. Specifies whether to ignore writes to column families that do not exist. Disabled by default (writes to non-existent column families are not ignored).</p>"},{"location":"myrocks/variables.html#rocksdb_write_policy","title":"<code>rocksdb_write_policy</code>","text":"Option Description Command-line \u2013rocksdb-write-policy Dynamic No Scope Global Data type String Default write_committed <p>The variable has been implemented in Percona Server 5.7.23-23. Specifies when two-phase commit data are actually written into the database. Allowed values are <code>write_committed</code>, <code>write_prepared</code>, and <code>write_unprepared</code>.</p> <p>Default value is <code>write_committed</code> which means data are written at commit time. If the value is set to <code>write_prepared</code>, then data are written after the prepare phase of a two-phase transaction. If the value is set to <code>write_unprepared</code>, then data are written before the prepare phase.</p>"},{"location":"performance/aio_page_requests.html","title":"Multiple page asynchronous I/O requests","text":"<p>The I/O unit size in InnoDB is only one page, even if the server is doing read ahead. A 16KB I/O unit size is small for sequential reads and less efficient than a larger I/O unit size. InnoDB uses Linux asynchronous I/O (<code>aio</code>) by default. By submitting multiple consecutive 16KB read requests at once, Linux internally can merge requests, and reads are more efficient. This feature can submit multiple page I/O requests and works in the background.</p> <p>You can manage the feature with the linear read-ahead technique. The technique adds pages to the buffer pool based on the buffer pool pages being accessed sequentially. The configuration parameter, <code>innodb_read_ahead_threshold</code> controls this process.</p> <p>On an HDD RAID 1+0 environment, more than 1000MB/s disk reads can be achieved by submitting 64 consecutive pages requests at once, while only 160MB/s disk reads is shown by submitting single page request.</p>"},{"location":"performance/aio_page_requests.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server 5.7.20-18: Feature ported from the Facebook MySQL patch.</li> </ul>"},{"location":"performance/aio_page_requests.html#status-variables","title":"Status Variables","text":""},{"location":"performance/aio_page_requests.html#innodb_buffered_aio_submitted","title":"<code>Innodb_buffered_aio_submitted</code>","text":"<p>Implemented in Percona Server 5.7.20-18.</p> Option Description Data type Numeric Scope Global <p>This variable shows the number of submitted buffered asynchronous I/O requests.</p>"},{"location":"performance/aio_page_requests.html#see-also","title":"See also","text":"<p>Optimizing full table scans in InnoDB</p> <p>Bug #68659  InnoDB Linux native aio should submit more i/o requests at once</p>"},{"location":"performance/innodb_numa_support.html","title":"Improved NUMA support","text":"<p>In cases where the buffer pool memory allocation is bigger than the size of  the node, the system starts swapping already allocated memory, even if  memory is available on another node. This happens if the default  <code>NUMA</code> memory allocation policy is selected. In that case, the system  favors one node more than another, which causes the node to run out of  memory. If the allocation policy is interleaving, the memory is  allocated in a round-robin fashion over the available node. This method  uses the upstream innodb_numa_interleave. This feature extends the upstream implementation by implementing the flush_caches variable.</p> <p>It is generally recommended to enable all options to maximize the performance effects on the <code>NUMA</code> architecture.</p>"},{"location":"performance/innodb_numa_support.html#version-specific-information","title":"Version Specific Information","text":"<p>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</p> <p>Percona Server 5.7.22-22: Feature reverted from the upstream implementation back to the one ported from Percona Server for MySQL 5.6, in which innodb_numa_interleave variable not only enables NUMA memory interleaving at InnoDB buffer pool allocation but allocates buffer pool with MAP_POPULATE, forcing interleaved allocation at the buffer pool initialization time.</p>"},{"location":"performance/innodb_numa_support.html#command-line-options-for-mysqld_safe","title":"Command-line Options for mysqld_safe","text":""},{"location":"performance/innodb_numa_support.html#flush_caches","title":"<code>flush_caches</code>","text":"Option Description Command-line Yes Config file Yes Location mysqld_safe Dynamic No Data type Boolean Default 0 (OFF) Range 0/1 <p>When enabled (set to <code>1</code>) this will flush and purge buffers/caches before starting the server to help ensure <code>NUMA</code> allocation fairness across nodes. This option is useful for establishing a consistent and predictable behavior for normal usage and/or benchmarking.</p> <p>See also</p> <p>The MySQL \u201cswap insanity\u201d problem and the effects of the NUMA architecture</p> <p>A brief update on NUMA and MySQL</p>"},{"location":"performance/prefix_index_queries_optimization.html","title":"Prefix Index Queries Optimization","text":"<p>Percona Server for MySQL 5.6 has ported the Prefix Index Queries Optimization feature from the Facebook patch for MySQL.</p> <p>Prior to this InnoDB would always fetch the clustered index for all prefix columns in an index, even when the value of a particular record was smaller than the prefix length. This implementation optimizes that case to use the record from the secondary index and avoid the extra lookup.</p>"},{"location":"performance/prefix_index_queries_optimization.html#status-variables","title":"Status Variables","text":""},{"location":"performance/prefix_index_queries_optimization.html#innodb_secondary_index_triggered_cluster_reads","title":"<code>Innodb_secondary_index_triggered_cluster_reads</code>","text":"<p>Implemented in Percona Server for MySQL 5.7.18-14.</p> Option Description Data type Numeric Scope Global <p>This variable shows the number of times secondary index lookup triggered cluster lookup.</p>"},{"location":"performance/prefix_index_queries_optimization.html#innodb_secondary_index_triggered_cluster_reads_avoided","title":"<code>Innodb_secondary_index_triggered_cluster_reads_avoided</code>","text":"<p>Implemented in Percona Server for MySQL 5.7.18-14.</p> Option Description Data type Numeric Scope Global This variable shows the number of times prefix optimization avoided triggering cluster lookup."},{"location":"performance/query_cache_enhance.html","title":"Query Cache Enhancements","text":"<p>This page describes the enhancements for the query cache. At the moment three features are available:</p> <ul> <li> <p>Disabling the cache completely</p> </li> <li> <p>Diagnosing contention more easily</p> </li> <li> <p>Ignoring comments</p> </li> </ul>"},{"location":"performance/query_cache_enhance.html#diagnosing-contention-more-easily","title":"Diagnosing contention more easily","text":"<p>This features provides a new thread state - <code>Waiting on query cache mutex</code>. It has always been difficult to spot query cache bottlenecks because these bottlenecks usually happen intermittently and are not directly reported by the server. This new thread state appear in the output of SHOW PROCESSLIST, easing diagnostics.</p> <p>Imagine that we run three queries simultaneously (each one in a separate thread):</p> <pre><code>&gt; mysql&gt; SELECT number from t where id &gt; 0;\n&gt; mysql&gt; SELECT number from t where id &gt; 0;\n&gt; mysql&gt; SELECT number from t where id &gt; 0;\n</code></pre> <p>If we experience query cache contention, the output of <code>SHOW PROCESSLIST</code> will look like this:</p> <p><pre><code>&gt; mysql&gt; SHOW PROCESSLIST;\n</code></pre> The output is similar to the following: <pre><code>Id      User    Host            db      Command Time    State                          Info\n2       root    localhost       test    Sleep   2       NULL\n3       root    localhost       test    Query   2       Waiting on query cache mutex  SELECT number from t where id &gt; 0;\n4       root    localhost       test    Query   1       Waiting on query cache mutex  SELECT number from t where id &gt; 0;\n5       root    localhost       test    Query   0       NULL\n</code></pre></p>"},{"location":"performance/query_cache_enhance.html#ignoring-comments","title":"Ignoring comments","text":"<p>This feature adds an option to make the server ignore comments when checking for a query cache hit. For example, consider these two queries:</p> <pre><code>mysql&gt; /* first query  */ select name from users where users.name like 'Bob%';\nmysql&gt; /* retry search */ select name from users where users.name like 'Bob%';\n</code></pre> <p>By default, (option off), the queries are considered different, so the server will execute them both and cache them both.</p> <p>If the option is enabled, the queries are considered identical, so the server will execute and cache the first one and will serve the second one directly from the query cache.</p>"},{"location":"performance/query_cache_enhance.html#system-variables","title":"System Variables","text":""},{"location":"performance/query_cache_enhance.html#query_cache_strip_comments","title":"<code>query_cache_strip_comments</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Boolean Default Off Makes the server ignore comments when checking for a query cache hit."},{"location":"performance/query_cache_enhance.html#other-reading","title":"Other Reading","text":"<ul> <li> <p>MySQL general thread states</p> </li> <li> <p>Query cache freezes</p> </li> </ul>"},{"location":"performance/query_limit_records.html","title":"Limiting the Estimation of Records in a Query","text":"<p>Availability: This feature is *technical preview quality.</p> <p>This page describes an alternative when running queries against a large number of table partitions. When a query runs, InnoDB estimates the records in each partition. This process can result in more pages read and more disk I/O, if the buffer pool must fetch the pages from disk. This process increases the query time if there are a large number of partitions.</p> <p>The addition of two variables make it possible to override records_in_range which effectively bypasses the process.</p> <p>Warning</p> <p>The use of these variables may result in improper index selection by the optimizer.</p>"},{"location":"performance/query_limit_records.html#innodb_records_in_range","title":"<code>innodb_records_in_range</code>","text":"Option Description Command-line <code>--innodb-records-in-range</code> Scope Global Dynamic Yes Data type Numeric Default 0 <p>Availability: This feature is technical preview quality.</p> <p>The variable provides a method to limit the number of records estimated for a query.</p> <pre><code>mysql&gt; SET @@GLOBAL.innodb_records_in_range=100;\n100\n</code></pre>"},{"location":"performance/query_limit_records.html#innodb_force_index_records_in_range","title":"<code>innodb_force_index_records_in_range</code>","text":"Option Description Command-line \u2013innodb-force-index-records-in-range Scope Global Dynamic Yes Data type Numeric Default 0 <p>Availability: This feature is technical preview quality.</p> <p>This variable provides a method to override the records_in_range result when a FORCE INDEX is used in a query.</p> <pre><code>mysql&gt; SET @@GLOBAL.innodb_force_index_records_in_range=100;\n100\n</code></pre>"},{"location":"performance/query_limit_records.html#using-the-favor_range_scan-optimizer-switch","title":"Using the favor_range_scan optimizer switch","text":"<p>Availability: The feature is technical preview quality.</p> <p>In specific scenarios, the optimizer chooses to scan a table instead of using a range scan. The conditions are the following:</p> <ul> <li> <p>Table with an extremely large number of rows</p> </li> <li> <p>Compound primary keys made of two or more columns</p> </li> <li> <p>WHERE clause contains multiple range conditions</p> </li> </ul> <p>The optimizer_switch controls the behavior of the optimizer. The favor_range_scan switch arbitrarily lowers the cost of a range scan by a factor of 10.</p> <p>The available values are:</p> <ul> <li> <p>ON</p> </li> <li> <p>OFF (Default)</p> </li> <li> <p>DEFAULT</p> </li> </ul> <pre><code>mysql&gt; SET optimizer_switch='favor_range_scan=on';\n</code></pre>"},{"location":"performance/threadpool.html","title":"Thread Pool","text":"<p>Servers continually execute queries from multiple clients. In MySQL, for each connection query, the server creates a thread, processes the query, and then destroys the thread. This method can have disadvantages because the server must consume resources to create, process, and destroy the thread. Therefore, when the number of connections grows, the server performance drops. Too many active threads impact performance because of context switching, and thread contention.</p> <p>A thread pool is distinct from connection pooling. A thread pool has the following advantages:</p> <ul> <li> <p>Limits the number of threads running on the server</p> </li> <li> <p>Minimizes wasting resources by creating and then destroying threads</p> </li> </ul> <p>This feature ensures that multiple connections using a thread pool will not cause the server to churn through resources or cause a server exit when the server runs out of memory. A thread pool reuses threads and is most efficient for the short queries associated with transactions.</p> <p>To enable the thread-pool feature, the thread_handling variable should be set to the <code>pool-of-threads</code> value. This can be done by adding the following to the MySQL configuration file <code>my.cnf</code>:</p> <pre><code>...\nthread_handling=pool-of-threads\n...\n</code></pre> <p>Although the default values for the thread-pool should provide good performance, additional tuning can be performed with the dynamic system variables described in System Variables.</p>"},{"location":"performance/threadpool.html#priority-connection-scheduling","title":"Priority connection scheduling","text":"<p>Even though thread pool puts a limit on the number of concurrently running queries, the number of open transactions may remain high, because connections with already started transactions are put to the end of the queue. Higher number of open transactions has a number of implications on the currently running queries. To improve the performance new thread_pool_high_prio_tickets variable has been introduced.</p> <p>This variable controls the high priority queue policy. Each new connection is assigned this many tickets to enter the high priority queue. Whenever a query has to be queued to be executed later because no threads are available, the thread pool puts the connection into the high priority queue if the following conditions apply:</p> <ul> <li> <p>The connection has an open transaction in the server.</p> </li> <li> <p>The number of high priority tickets of this connection is non-zero.</p> </li> </ul> <p>If both the above conditions hold, the connection is put into the high priority queue and its tickets value is decremented. Otherwise, the connection is put into the common queue with the initial tickets value specified with this option.</p> <p>Each time the thread pool looks for a new connection to process, first it checks the high priority queue, and picks connections from the common queue only when the high priority one is empty.</p> <p>The goal is to minimize the number of open transactions in the server. In many cases it is beneficial to give short-running transactions a chance to commit faster and thus deallocate server resources and locks without waiting in the same queue with other connections that are about to start a new transaction, or those that have run out of their high priority tickets.</p> <p>The default thread pool behavior is to always put events from already started transactions into the high priority queue, as we believe that results in better performance in the vast majority of cases.</p> <p>With the value of <code>0</code>, all connections are always put into the common queue, i.e. no priority scheduling is used as in the original implementation in MariaDB. The higher is the value, the more chances each transaction gets to enter the high priority queue and commit before it is put in the common queue.</p> <p>In some cases it is required to prioritize all statements for a specific connection regardless of whether they are executed as a part of a multi-statement transaction or in the autocommit mode. Or vice versa, some connections may require using the low priority queue for all statements unconditionally. To implement this new thread_pool_high_prio_mode variable has been introduced in Percona Server for MySQL.</p>"},{"location":"performance/threadpool.html#low-priority-queue-throttling","title":"Low priority queue throttling","text":"<p>One case that can limit the performance of thread-pool and even lead to deadlocks under high concurrency is the situation when thread groups are oversubscribed due to active threads reaching the oversubscribe limit, but all or most worker threads are actually waiting on locks currently held by a transaction from another connection that is not currently in the thread-pool.</p> <p>In this case, those threads in the pool that have marked themselves inactive are not accounted to the oversubscribe limit. As a result, the number of threads (both active and waiting) in the pool grows until it hits thread_pool_max_threads value. If the connection executing the transaction which is holding the lock has managed to enter the thread-pool by then, we get a large (depending on the thread_pool_max_threads value) number of concurrently running threads, and thus, suboptimal performance as a result. Otherwise, we get a deadlock as no more threads can be created to process those transactions and release the lock.</p> <p>Such situations are prevented by throttling the low priority queue when the total number of worker threads (both active and waiting ones) reaches the oversubscribe limit. That is, if there are too many worker threads, do not start new transactions and create new threads until queued events from the already started transactions are processed.</p>"},{"location":"performance/threadpool.html#handling-of-long-network-waits","title":"Handling of Long Network Waits","text":"<p>Certain types of workloads (large result sets, BLOBs, slow clients) can have longer waits on network I/O (socket reads and writes). Whenever server waits, this should be communicated to the Thread Pool, so it can start new query by either waking a waiting thread or sometimes creating a new one. This implementation has been ported from MariaDB patch MDEV-156.</p>"},{"location":"performance/threadpool.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: <code>Thread Pool</code> feature ported from Percona Server for MySQL 5.6.</li> </ul>"},{"location":"performance/threadpool.html#system-variables","title":"System Variables","text":""},{"location":"performance/threadpool.html#thread_handling","title":"<code>thread_handling</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type String Default one-thread-per-connection <p>This variable defines how the server handles threads for connections from the client.</p> Values Description one-thread-per-connection One thread handles all requests for a connection pool-of-threads A thread pool handles requests for all connections no-threads A single thread for all connections for debugging mode ### <code>thread_pool_idle_timeout</code> Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 60 (seconds) This variable can be used to limit the time an idle thread should wait before exiting."},{"location":"performance/threadpool.html#thread_pool_high_prio_mode","title":"<code>thread_pool_high_prio_mode</code>","text":"Option Description Command-line Yes Config file Yes Scope Global, Session Dynamic Yes Data type String Default transactions Allowed values transactions, statements, none <p>This variable is used to provide more fine-grained control over high priority scheduling either globally or per connection.</p> <p>The following values are allowed:</p> <ul> <li> <p><code>transactions</code> (the default). In this mode, only statements from already started transactions may go into the high priority queue depending on the number of high priority tickets currently available in a connection (see thread_pool_high_prio_tickets).</p> </li> <li> <p><code>statements</code>. In this mode, all individual statements go into the high priority queue, regardless of connection\u2019s transactional state and the number of available high priority tickets. This value can be used to prioritize <code>AUTOCOMMIT</code> transactions or other kinds of statements such as administrative ones for specific connections. Note that setting this value globally essentially disables high priority scheduling, since in this case all statements from all connections will use a single queue (the high priority one)</p> </li> <li> <p><code>none</code>. This mode disables high priority queue for a connection. Some connections (e.g. monitoring) may be insensitive to execution latency and/or never allocate any server resources that would otherwise impact performance in other connections and thus, do not really require high priority scheduling. Note that setting thread_pool_high_prio_mode to <code>none</code> globally has essentially the same effect as setting it to <code>statements</code> globally: all connections will always use a single queue (the low priority one in this case).</p> </li> </ul>"},{"location":"performance/threadpool.html#thread_pool_high_prio_tickets","title":"<code>thread_pool_high_prio_tickets</code>","text":"Option Description Command-line Yes Config file Yes Scope Global, Session Dynamic Yes Data type Numeric Default 4294967295 This variable controls the high priority queue policy. Each new connection is assigned this many tickets to enter the high priority queue. Setting this variable to <code>0</code> disables the high priority queue."},{"location":"performance/threadpool.html#thread_pool_max_threads","title":"<code>thread_pool_max_threads</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 100000 This variable can be used to limit the maximum number of threads in the pool. Once this number is reached no new threads will be created."},{"location":"performance/threadpool.html#thread_pool_oversubscribe","title":"<code>thread_pool_oversubscribe</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 3 The higher the value of this parameter the more threads can be run at the same time, if the values is lower than <code>3</code> it could lead to more sleeps and wake-ups."},{"location":"performance/threadpool.html#thread_pool_size","title":"<code>thread_pool_size</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default Number of processors This variable can be used to define the number of threads that can use the CPU at the same time."},{"location":"performance/threadpool.html#thread_pool_stall_limit","title":"<code>thread_pool_stall_limit</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type Numeric Default 500 (ms) The number of milliseconds before a running thread is considered stalled. When this limit is reached thread pool will wake up or create another thread. This is being used to prevent a long-running query from monopolizing the pool."},{"location":"performance/threadpool.html#extra_port","title":"<code>extra_port</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type Numeric Default 0 <p>This variable can be used to specify an additional port that Percona Server for MySQL will listen on. This can be used in case no new connections can be established due to all worker threads being busy or being locked when <code>pool-of-threads</code> feature is enabled. To connect to the extra port the following command can be used:</p> <pre><code>mysql --port='extra-port-number' --protocol=tcp\n</code></pre>"},{"location":"performance/threadpool.html#extra_max_connections","title":"<code>extra_max_connections</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 1 <p>This variable can be used to specify the maximum allowed number of connections plus one extra <code>SUPER</code> users connection on the extra_port. This can be used with the extra_port variable to access the server in case no new connections can be established due to all worker threads being busy or being locked when <code>pool-of-threads</code> feature is enabled.</p>"},{"location":"performance/threadpool.html#status-variables","title":"Status Variables","text":""},{"location":"performance/threadpool.html#threadpool_idle_threads","title":"<code>Threadpool_idle_threads</code>","text":"Option Description Data type Numeric Scope Global <p>This status variable shows the number of idle threads in the pool.</p>"},{"location":"performance/threadpool.html#threadpool_threads","title":"<code>Threadpool_threads</code>","text":"Option Description Data type Numeric Scope Global <p>This status variable shows the number of threads in the pool.</p> <p>Note</p> <p>When thread-pool is enabled, the value of the thread_cache_size variable is ignored. The Threads_cached status variable contains <code>0</code> in this case.</p>"},{"location":"performance/threadpool.html#other-reading","title":"Other Reading","text":"<ul> <li> <p>Thread pool in MariaDB 5.5</p> </li> <li> <p>Thread pool implementation in Oracle MySQL</p> </li> </ul>"},{"location":"performance/xtradb_performance_improvements_for_io-bound_highly-concurrent_workloads.html","title":"XtraDB Performance Improvements for I/O-Bound Highly-Concurrent Workloads","text":""},{"location":"performance/xtradb_performance_improvements_for_io-bound_highly-concurrent_workloads.html#priority-refill-for-the-buffer-pool-free-list","title":"Priority refill for the buffer pool free list","text":"<p>In highly-concurrent I/O-bound workloads the following situation may happen:</p> <ol> <li> <p>Buffer pool free lists are used faster than they are refilled by the LRU cleaner thread.</p> </li> <li> <p>Buffer pool free lists become empty and more and more query and utility (i.e. purge) threads stall, checking whether a buffer pool free list has became non-empty, sleeping, performing single-page LRU flushes.</p> </li> <li> <p>The number of buffer pool free list mutex waiters increases.</p> </li> <li> <p>When the LRU manager thread (or a single page LRU flush by a query thread) finally produces a free page, it is starved from putting it on the buffer pool free list as it must acquire the buffer pool free list mutex too. However, being one thread in up to hundreds, the chances of a prompt acquisition are low.</p> </li> </ol> <p>This is addressed by delegating all the LRU flushes to the LRU manager thread, never attempting to evict a page or perform an LRU single page flush by a query thread, and introducing a backoff algorithm to reduce buffer pool free list mutex pressure on empty buffer pool free lists. This is controlled through a new system variable innodb_empty_free_list_algorithm.</p>"},{"location":"performance/xtradb_performance_improvements_for_io-bound_highly-concurrent_workloads.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li> <p>Percona Server for MySQL 5.7.10-1: Feature partially ported from Percona Server for MySQL 5.6</p> </li> <li> <p>Percona Server for MySQL 5.7.10-3: Implemented support for multi-threaded LRU</p> </li> <li> <p>Percona Server for MySQL 5.7.11-4: Implemented support for parallel doublewrite buffer</p> </li> </ul>"},{"location":"performance/xtradb_performance_improvements_for_io-bound_highly-concurrent_workloads.html#innodb_empty_free_list_algorithm","title":"<code>innodb_empty_free_list_algorithm</code>","text":"Option Description Command-line Yes Config File Yes Scope Global Dynamic Yes Data type legacy, backoff Default backoff <p>When <code>legacy</code> option is set, server uses the upstream algorithm and when the <code>backoff</code> is selected, Percona implementation will be used.</p>"},{"location":"performance/xtradb_performance_improvements_for_io-bound_highly-concurrent_workloads.html#multi-threaded-lru-flusher","title":"Multi-threaded LRU flusher","text":"<p>Percona Server for MySQL 5.7.10-3 has  introduced a true multi-threaded LRU flushing. In this scheme, each buffer pool instance has its own dedicated LRU manager thread that is tasked with performing LRU flushes and evictions to refill the free list of that buffer pool instance. Existing multi-threaded flusher no longer does any LRU flushing and is tasked with flush list flushing only.</p> <p>This has been done to address the shortcomings of the existing MySQL 5.7 multi-threaded flusher:</p> <ul> <li> <p>All threads still synchronize on each coordinator thread iteration. If a particular flushing job is stuck on one of the worker threads, the rest will idle until the stuck one completes.</p> </li> <li> <p>The coordinator thread heuristics focus on flush list adaptive flushing without considering the state of free lists, which might be in need of urgent refill for a subset of buffer pool instances on a loaded server.</p> </li> <li> <p>LRU flushing is serialized with flush list flushing for each buffer pool instance, introducing the risk that the right flushing mode will not happen for a particular instance because it is being flushed in the other mode.</p> </li> </ul> <p>The following InnoDB metrics are no longer accounted, as their semantics do not make sense under the current LRU flushing design: <code>buffer_LRU_batch_flush_avg_time_slot</code>, <code>buffer_LRU_batch_flush_avg_pass</code>, <code>buffer_LRU_batch_flush_avg_time_thread</code>, <code>buffer_LRU_batch_flush_avg_time_est</code>.</p> <p>The need for InnoDB recovery thread writer threads is also removed, consequently all associated code is deleted.</p>"},{"location":"performance/xtradb_performance_improvements_for_io-bound_highly-concurrent_workloads.html#parallel-doublewrite-buffer","title":"Parallel doublewrite buffer","text":"<p>The legacy doublewrite buffer is shared between all the buffer pool instances and all the flusher threads. It collects all the page write requests into a single buffer, and, when the buffer fills, writes it out to disk twice, blocking any new write requests until the writes complete. This becomes a bottleneck with increased flusher parallelism, limiting the effect of extra cleaner threads. In addition, single page flushes, if they are performed, are subject to above and also contend on the doublewrite mutex.</p> <p>To address these issues Percona Server for MySQL Percona Server for MySQL 5.7.11-4 has introduced private doublewrite buffers for each buffer pool instance, for each batch flushing mode (LRU or flush list). For example, with four buffer pool instances, there will be eight doublewrite shards. Only one flusher thread can access any shard at a time, and each shard is added to and flushed completely independently of the rest. This does away with the mutex and the event wait does not block other threads from proceeding anymore, it only waits for the asynchronous I/O to complete. The only inter-thread synchronization is between the flusher thread and I/O completion threads.</p> <p>The new doublewrite buffer is contained in a new file, where all the shards are contained, at different offsets. This file is created on startup, and removed on a clean shutdown. If it\u2019s found on a crashed instance startup, its contents are read and any torn pages are restored. If it\u2019s found on a clean instance startup, the server startup is aborted with an error message.</p> <p>The location of the doublewrite file is governed by a new innodb_parallel_doublewrite_path global, read-only system variable. It defaults to <code>xb_doublewrite</code> in the data directory. The variable accepts both absolute and relative paths. In the latter case they are treated as relative to the data directory. The doublewrite file is not a tablespace from InnoDB internals point of view.</p> <p>The legacy InnoDB doublewrite buffer in the system tablespace continues to address doublewrite needs of single page flushes, and they are free to use the whole of that buffer (128 pages by default) instead of the last eight pages as currently used. Note that single page flushes will not happen in Percona Server for MySQL unless innodb_empty_free_list_algorithm is set to <code>legacy</code> value.</p> <p>The existing system tablespace is not touched in any way for this feature implementation, ensuring that cleanly-shutdown instances may be freely moved between different server flavors.</p>"},{"location":"performance/xtradb_performance_improvements_for_io-bound_highly-concurrent_workloads.html#interaction-with-innodb_flush_method","title":"Interaction with innodb_flush_method","text":"<p>Regardless of innodb_flush_method setting, the parallel doublewrite file is opened with <code>O_DIRECT</code> flag to remove OS caching, then its access is further governed by the exact value set: if it\u2019s set to <code>O_DSYNC</code>, the parallel doublewrite is opened with <code>O_SYNC</code> flag too. Further, if it\u2019s one of <code>O_DSYNC</code>, <code>O_DIRECT_NO_FSYNC</code>, or <code>ALL_O_DIRECT</code>, then the doublewrite file is not flushed after a batch of writes to it is completed. With other innodb_flush_method values the doublewrite buffer is flushed only if setting <code>O_DIRECT</code> has failed.</p>"},{"location":"performance/xtradb_performance_improvements_for_io-bound_highly-concurrent_workloads.html#innodb_parallel_doublewrite_path","title":"<code>innodb_parallel_doublewrite_path</code>","text":"Option Description Command-line Yes Scope Global Dynamic No Data type String Default xb_doublewrite <p>This variable is used to specify the location of the parallel doublewrite file. It accepts both absolute and relative paths. In the latter case they are treated as relative to the data directory.</p> <p>Percona Server for MySQL has introduced several options, only available in builds compiled with <code>UNIV_PERF_DEBUG</code> C preprocessor define.</p>"},{"location":"performance/xtradb_performance_improvements_for_io-bound_highly-concurrent_workloads.html#innodb_sched_priority_master","title":"<code>innodb_sched_priority_master</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Boolean"},{"location":"performance/xtradb_performance_improvements_for_io-bound_highly-concurrent_workloads.html#other-reading","title":"Other Reading","text":"<ul> <li> <p>Page cleaner thread tuning</p> </li> <li> <p>Bug #74637 - make dirty page flushing more adaptive</p> </li> <li> <p>Bug #67808 - in innodb engine, double write and multi-buffer pool instance reduce concurrency</p> </li> <li> <p>Bug #69232 - buf_dblwr-&gt;mutex can be split into two</p> </li> </ul>"},{"location":"release-notes/5.7.40-43.html","title":"Percona Server for MySQL 5.7.40-43 (2022-11-28)","text":"Release date November 28, 2022 Install instructions Install Percona Server for MySQL Download this version Percona Server for MySQL <p>Percona Server for MySQL 5.7.40-43 includes all the features and bug fixes available in MySQL 5.7.40 Community Edition in addition to enterprise-grade features developed by Percona.</p> <p>Percona Server for MySQL is a free, fully compatible, enhanced, and open source drop-in replacement for any MySQL database. It provides superior performance, scalability, and instrumentation.</p> <p>Percona Server for MySQL is trusted by thousands of enterprises to provide better performance and concurrency for their most demanding workloads. It delivers more value to MySQL server users with optimized performance, greater performance scalability and availability, enhanced backups, and increased visibility.</p> <p>For paid support, managed services or consulting services, contact Percona Sales</p> <p>For training, contact Percona Training - Start learning now</p>"},{"location":"release-notes/5.7.40-43.html#release-highlights","title":"Release highlights","text":"<p>Improvements and bug fixes provided by Oracle for MySQL 5.7.40 and included in Percona Server for MySQL are the following:</p> <ul> <li> <p>ISO 8601 timestamps in log messages did not consider daylight saving time when <code>--log-timestamps=SYSTEM</code> was used.</p> </li> <li> <p>The <code>GRANT OPTION</code> privilege was treated as related to database operations.</p> </li> <li> <p>In specific cases, a <code>TRUNCATE TABLE</code> operation failed to release an acquired mutex.</p> </li> <li> <p>A descending b-tree scan raised a debug assertion failure in debug builds.</p> </li> </ul> <p>Find the full list of bug fixes and changes in the MySQL 5.7.40 Release Notes.</p>"},{"location":"release-notes/5.7.40-43.html#bug-fixes","title":"Bug fixes","text":"<ul> <li>PS-1098: Manually rotating the log files by calling <code>audit_log_flush</code> multiple times caused incorrect file rotation and the wrong log file to be used.</li> <li>PS-8327: <code>ALTER TABLE ... CHECK PARTITION</code> inside the stored procedure caused a server exit.</li> </ul>"},{"location":"release-notes/5.7.40-43.html#platform-support","title":"Platform support","text":"<p>Percona Server for MySQL 5.7.40-43 does not support Ubuntu 22.04.</p>"},{"location":"release-notes/5.7.40-43.html#useful-links","title":"Useful links","text":"<p>The Percona Server for MySQL GitHub location</p> <p>Contribute to the documentation</p>"},{"location":"release-notes/5.7.41-44.html","title":"Percona Server for MySQL 5.7.41-44 (2023-03-02)","text":"Release date March 2, 2023 Install instructions Install Percona Server for MySQL <p>Percona Server for MySQL 5.7.41-44 includes all the features and bug fixes available in MySQL 5.7.41 Community Edition in addition to enterprise-grade features developed by Percona.</p> <p>Percona Server for MySQL is a free, fully compatible, enhanced, and open source drop-in replacement for any MySQL database. It provides superior performance, scalability, and instrumentation.</p> <p>Percona Server for MySQL is trusted by thousands of enterprises to provide better performance and concurrency for their most demanding workloads. It delivers more value to MySQL server users with optimized performance, greater performance scalability and availability, enhanced backups, and increased visibility.</p>"},{"location":"release-notes/5.7.41-44.html#release-highlights","title":"Release highlights","text":"<p>Percona has removed an Oracle patch for <code>mysqldump</code> that performed, at the beginning of the dump, a <code>FLUSH_TABLES_WITH_READ_LOCK</code> to get consistent <code>GTID_EXECUTED</code> because the patch required additional user privileges, even when the user does not use GTID-based replication. The following bugs based on this patch were submitted to Oracle:</p> <ul> <li> <p>MySQL 109701</p> </li> <li> <p>MySQL 109685</p> </li> </ul> <p>The Percona solution uses <code>START TRANSACTION WITH CONSISTENT SNAPSHOT</code>.</p> <p>Improvements and bug fixes provided by Oracle for MySQL 5.7.41 and included in Percona Server for MySQL are the following:</p> <ul> <li> <p>Updated the linked OpenSSL library for MySQL Server to 1.1.s</p> </li> <li> <p>Updated the bundled zlib library to zlib 1.2.13. This zlib library version is the minimum supported.</p> </li> <li> <p>While the SQL thread handled a transaction, issuing <code>STOP SLAVE_SQL_THREAD</code> caused replication to stop immediately instead of waiting for the event group to complete before the shutdown.</p> </li> </ul> <p>Find the full list of bug fixes and changes in the MySQL 5.7.41 Release Notes.</p>"},{"location":"release-notes/5.7.41-44.html#bug-fixes","title":"Bug fixes","text":"<ul> <li>PS-7538: With <code>innodb_optimize_fulltext_only</code> enabled, running <code>OPTIMIZE TABLE</code> on a table with an FTS index caused a server exit.</li> </ul>"},{"location":"release-notes/5.7.41-44.html#platform-support","title":"Platform support","text":"<p>This release adds support for Ubuntu 22.04.</p> <p>This release add support for Red Hat Enterprise Linux 9 and compatible derivatives.</p>"},{"location":"release-notes/5.7.41-44.html#useful-links","title":"Useful links","text":"<p>The Percona Server for MySQL GitHub location</p> <p>Contribute to the documentation</p> <p>For training, contact Percona Training - Start learning now</p>"},{"location":"release-notes/5.7.42-45.html","title":"Percona Server for MySQL 5.7.42-45 (2023-05-23)","text":"Release date May 23, 2023 Install instructions Install Percona Server for MySQL <p>Percona Server for MySQL 5.7.42-45 includes all the features and bug fixes available in MySQL 5.7.42 Community Edition in addition to enterprise-grade features developed by Percona.</p> <p>Percona Server for MySQL is a freely available, fully compatible, enhanced, and open source drop-in replacement for any MySQL database. It provides superior and optimized performance, greater scalability and availability, enhanced backups, increased visibility, and instrumentation .</p> <p>Percona Server for MySQL is trusted by thousands of enterprises to provide better performance and concurrency for their most demanding workloads. </p>"},{"location":"release-notes/5.7.42-45.html#release-highlights","title":"Release highlights","text":"<p>Improvements and bug fixes provided by Oracle for MySQL 5.7.42 and included in Percona Server for MySQL are the following:</p> <ul> <li>In InnoDB, online DDL operations are prevented from accessing out-of-bounds memory</li> </ul> <p>Find the full list of bug fixes and changes in the MySQL 5.7.42 Release Notes.</p>"},{"location":"release-notes/5.7.42-45.html#bug-fixes","title":"Bug fixes","text":"<ul> <li> <p>PS-8719: Audit log plugin stalled on flush.</p> </li> <li> <p>PS-8749: Fixed bad compression stats in INFORMATION_SCHEMA.INNODB_CMP. </p> </li> </ul>"},{"location":"release-notes/5.7.42-45.html#useful-links","title":"Useful links","text":"<p>The Percona Server for MySQL GitHub location</p> <p>Contribute to the documentation</p> <p>Download product binaries, packages, and tarballs at Percona Product Downloads</p> <p>For training, contact Percona Training - Start learning now</p>"},{"location":"release-notes/5.7.42-46.html","title":"Percona Server for MySQL 5.7.42-46 (2023-06-01)","text":"Release date June 01, 2023 Install instructions Install Percona Server for MySQL <p>Percona Server for MySQL 5.7.42-46 includes all the features and bug fixes available in MySQL 5.7.42 Community Edition in addition to enterprise-grade features developed by Percona.</p> <p>Percona Server for MySQL is a freely available, fully compatible, enhanced, and open source drop-in replacement for any MySQL database. It provides superior and optimized performance, greater scalability and availability, enhanced backups, increased visibility, and instrumentation .</p> <p>Percona Server for MySQL is trusted by thousands of enterprises to provide better performance and concurrency for their most demanding workloads. </p>"},{"location":"release-notes/5.7.42-46.html#release-highlights","title":"Release highlights","text":"<p>Improvements and bug fixes provided by Oracle for MySQL 5.7.42 and included in Percona Server for MySQL are the following:</p> <ul> <li>In InnoDB, online DDL operations are prevented from accessing out-of-bounds memory</li> </ul> <p>Find the full list of bug fixes and changes in the MySQL 5.7.42 Release Notes.</p>"},{"location":"release-notes/5.7.42-46.html#bug-fixes","title":"Bug fixes","text":"<ul> <li>PS-8776: The <code>mysqldump</code> client utility ignored <code>--set-gtid-purged=OFF</code>.</li> </ul>"},{"location":"release-notes/5.7.42-46.html#useful-links","title":"Useful links","text":"<p>The Percona Server for MySQL GitHub location</p> <p>Contribute to the documentation</p> <p>Download product binaries, packages, and tarballs at Percona Product Downloads</p> <p>For training, contact Percona Training - Start learning now</p>"},{"location":"release-notes/5.7.43-47.html","title":"Percona Server for MySQL 5.7.43-47 (2023-08-17)","text":"<p>Percona Server for MySQL 5.7.43-47 includes all the features and bug fixes available in MySQL 5.7.43 Community Edition in addition to enterprise-grade features developed by Percona.</p> <p>Percona Server for MySQL is a freely available, fully compatible, enhanced, and open source drop-in replacement for any MySQL database. It provides superior and optimized performance, greater scalability and availability, enhanced backups, increased visibility, and instrumentation .</p> <p>Percona Server for MySQL is trusted by thousands of enterprises to provide better performance and concurrency for their most demanding workloads. </p>"},{"location":"release-notes/5.7.43-47.html#release-highlights","title":"Release highlights","text":"<p>Improvements and bug fixes provided by Oracle for MySQL 5.7.43 and included in Percona Server for MySQL are the following:</p> <ul> <li>OpenSSL 1.1.1 library has been upgraded to OpenSSL 3.0.9.</li> </ul> <p>Find the full list of bug fixes and changes in the MySQL 5.7.43 Release Notes.</p>"},{"location":"release-notes/5.7.43-47.html#bug-fixes","title":"Bug fixes","text":"<p>This release merges the MySQL 5.7.43 code base. This release does not contain new improvements or new bug fixes from Percona. </p>"},{"location":"release-notes/5.7.43-47.html#useful-links","title":"Useful links","text":"<p>Install Percona Server for MySQL</p> <p>The Percona Server for MySQL GitHub location</p> <p>Contribute to the documentation</p> <p>Download product binaries, packages, and tarballs at Percona Product Downloads</p> <p>For training, contact Percona Training - Start learning now</p>"},{"location":"release-notes/Percona-Server-5.7.10-1.html","title":"Percona Server for MySQL 5.7.10-1 (2015-12-14)","text":"<p>Percona is glad to announce the first Release Candidate release of Percona Server for MySQL 5.7.10-1 on December 14<sup>th</sup>, 2015 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.10, including all the bug fixes in it, Percona Server for MySQL 5.7.10-1 is the current Release Candidate release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.10-1 milestone at Launchpad</p> <p>This release contains all the bug fixes from the latest Percona Server for MySQL 5.6 release (currently Percona Server for MySQL 5.6.27-76.0).</p>"},{"location":"release-notes/Percona-Server-5.7.10-1.html#new-features","title":"New Features","text":"<ul> <li>Percona Server for MySQL 5.7.10-1 is not available on either the RHEL 5 family of Linux distributions or Debian* 6 (squeeze).</li> </ul> <p>A complete list of the changes between Percona Server for MySQL 5.6 and 5.7 can be found in Changed in Percona Server 5.7.</p>"},{"location":"release-notes/Percona-Server-5.7.10-1.html#known-issues","title":"Known issues","text":"<p>MeCab Full-Text Parser Plugin  has not been included in this release.</p> <p>PAM Authentication Plugin currently isn\u2019t working correctly.</p> <p>The following variables do not work correctly:</p> <ul> <li> <p><code>innodb_show_verbose_locks</code></p> </li> <li> <p><code>innodb_show_locks_held</code></p> </li> </ul> <p>In Percona Server for MySQL 5.7 <code>super_read_only</code> feature has been replaced with upstream implementation. There are currently two known issues compared to Percona Server for MySQL 5.6 implementation:</p> <ul> <li>Bug #78963: If the <code>relay_log_info_repository</code>= TABLE, using<code>super_read_only</code>aborts the <code>STOP SLAVE</code> and could lead to a server crash in debug builds.</li> <li>Bug #79328, passing <code>super_read_only</code> as a server option has no effect. </li> </ul> <p>Using a primary key with a <code>BLOB</code> in the TokuDB table could lead to a server crash (#916).</p> <p>Using XA transactions with TokuDB could lead to a server crash(#900).</p> <p>Percona TokuBackup has not been included in this release.</p>"},{"location":"release-notes/Percona-Server-5.7.10-1.html#bugs-fixed","title":"Bugs Fixed","text":"<p>Running <code>ALTER TABLE</code> without specifying the storage engine (without <code>ENGINE=</code> clause) or <code>OPTIMIZE TABLE</code> when <code>enforce_storage_engine</code> was enabled could lead to unrequested and unexpected storage engine changes. If done for a system table, it would circumvent regular system table storage engine compatibility checks, resulting in crashes or otherwise broken server operation. Bug fixed #1488055.</p> <p>Some transaction deadlocks did not increase the <code>INFORMATION_SCHEMA.INNODB_METRICS</code> <code>lock_deadlocks</code> counter. Bug fixed #1466414 (upstream #77399).</p> <p>Removed excessive locking during the buffer pool resize when checking whether AHI is enabled. Bug fixed #1525215 (upstream #78894).</p> <p>Removed unnecessary code in the InnoDB error monitor thread. Bug fixed #1521564 (upstream #79477).</p> <p>With Expanded Fast Index Creation enabled, DDL queries involving InnoDB temporary tables would cause later queries on the same tables to produce warnings that their indexes were not found in the index translation table. Bug fixed #1233431.</p> <p>Other bugs fixed: #371752 (upstream #45379), #1441362 (upstream #56155), #1385062 (upstream #74810), #1519201 (upstream #79391), #1515602, #1506697 (upstream #57552), #1501089 (upstream #75239), #1447527 (upstream #75368), #1384658 (upstream #74619), #1384656 (upstream #74584), and #1192052.</p>"},{"location":"release-notes/Percona-Server-5.7.10-2.html","title":"Percona Server for MySQL 5.7.10-2 (2016-02-05)","text":"<p>Percona is glad to announce the second Release Candidate release of Percona Server for MySQL 5.7.10-2 on February 5<sup>th</sup>, 2016 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.10, including all the bug fixes in it, Percona Server for MySQL 5.7.10-2 is the current Release Candidate release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.10-2 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.10-2.html#new-features","title":"New Features","text":"<p>A complete list of changes between Percona Server for MySQL 5.6 and 5.7 can be seen in Changed in Percona Server 5.7.</p> <p>The 5.7 binlog group commit algorithm is now supported in TokuDB as well.</p> <p>New TokuDB index statistics reporting has been implemented to be compatible with the changes implemented in upstream 5.7. Following the InnoDB example, the default value for <code>tokudb_cardinality_scale_percent</code> has been changed from <code>50%</code> to <code>100%</code>. Implementing this feature also addresses a server crash deep in the optimizer code.</p>"},{"location":"release-notes/Percona-Server-5.7.10-2.html#known-issues","title":"Known Issues","text":"<p>In Percona Server for MySQL 5.7 super_read_only feature has been replaced with upstream implementation. There are currently two known issues compared to Percona Server for MySQL 5.6 implementation:</p> <ul> <li> <p>Bug #78963, <code>super_read_only</code> aborts <code>STOP SLAVE</code> if <code>relay_log_info_repository</code> is set to <code>TABLE</code> which could lead to a server crash in Debug builds.</p> </li> <li> <p>Bug #79328, <code>super_read_only</code> set as a server option has no effect. InnoDB crash recovery might fail if <code>innodb_flush_method</code> is set to <code>ALL_O_DIRECT</code>. The workaround is to set this variable to a different value before starting up the crashed instance (bug #1529885).</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.10-2.html#bugs-fixed","title":"Bugs Fixed","text":"<p>Clustering secondary index could not be created on a partitioned TokuDB table. Bug fixed #1527730 (#720).</p> <p>Percona TokuBackup was failing to compile with Percona Server for MySQL 5.7. Bug fixed #123.</p> <p>Granting privileges to a user authenticating with PAM Authentication Plugin could lead to a server crash. Bug fixed #1521474. TokuDB status variables were missing from Percona Server for MySQL <code>5.7.10-1</code>. Bug fixed #1527364 (#923).</p> <p>Attempting to rotate the audit log file would result in the audit log file name<code>foo.log.%u</code> (literally) instead of a numeric suffix. Bug fixed#1528603.</p> <p>Adding an index to an InnoDB temporary table while <code>expand_fast_index_creation</code> was enabled could lead to server assertion. Bug fixed #1529555.</p> <p>TokuDB would not be upgraded on Debian/Ubuntu distributions while performing an upgrade from Percona Server for MySQL 5.6 to Percona Server for MySQL 5.7 even if explicitly requested. Bug fixed #1533580.</p> <p>The server would assert when both TokuDB and InnoDB tables were used within one transaction on a replication slave which has binary log enabled and slave updates logging disabled. Bug fixed #1534249 (upstream bug #80053).</p> <p>MeCab Full-Text Parser Plugin has not been included in the previous release. Bug fixed #1534617.</p> <p>Fixed server assertion caused by <code>Performance Schema</code> memory key mix-up in <code>SET STATEMENT ... FOR ...</code> statements. Bug fixed #1534874.</p> <p>The service name on CentOS 6 has been renamed from <code>mysqld</code> back to <code>mysql</code>.This change requires a manual service restart after being upgraded from Percona Server for MySQL <code>5.7.10-1</code></p> <p>Bug fixed #1542332. Setting the <code>innodb_sched_priority_purge</code> (available only in debug builds) while purge threads were stopped would cause a server crash. Bug fixed #1368552.</p> <p>Enabling TokuDB with the <code>ps_tokudb_admin</code> script inside the Docker container would cause an error due to insufficient privileges even when running as root. In order for this script to be used inside docker containers, this error has been changed to a warning that a check is impossible. Bug fixed #1520890.</p> <p>Write-heavy workload with a small buffer pool could lead to a deadlock when free buffers are exhausted. Bug fixed #1521905.</p> <p>InnoDB status will start printing negative values for spin rounds per wait, if the wait number, even though being accounted as a signed 64-bit integer, will not fit into a signed 32-bit integer. Bug fixed #1527160 (upstream#79703).</p> <p>Percona Server for MySQL 5.7 couldn\u2019t be restarted after TokuDB has been installed with the <code>ps_tokudb_admin</code> script. Bug fixed #1527535.</p> <p>Fixed memory leak when <code>utility_user</code>is enabled. Bug fixed #1530918.</p> <p>Page cleaner worker threads were not instrumented for <code>Performance Schema</code>. Bug fixed #1532747 (upstream bug #79894).</p> <p>The busy server was preferring LRU flushing over flush list flushing too strongly which could lead to performance degradation. Bug fixed #1534114.</p> <p><code>libjemalloc.so.1</code> was missing from the binary tarball. Bug fixed #1537129.</p> <p>When <code>cmake/make/make_binary_distribution</code> workflow was used to produce binary tarballs it would produce tarballs with <code>mysql-...</code> naming instead of <code>percona-server-...</code>. Bug fixed #1540385.</p> <p>Added proper memory cleanup if for some reason a table is unable to be opened from a dead closed state. This prevents an assertion from happening the next time the table is attempted to be opened. Bug fixed #917.</p> <p>The variable <code>tokudb_support_xa</code> has been modified to prevent setting it to anything but <code>ON</code>/<code>ENABLED</code> and to print a SQL warning anytime an attempt is made to change it, just like <code>innodb_support_xa</code>. Bug fixed #928.</p> <p>Other bugs fixed: #1179451, #1534246, #1524763, #1525109 (upstream #79569), #1530102, #897, #898, #899, #900, #901, #902, #903, #905, #906, #907, #908, #909, #910, #911, #912, #913, #915, #919, and #904.</p>"},{"location":"release-notes/Percona-Server-5.7.10-3.html","title":"Percona Server for MySQL 5.7.10-3 (2016-02-23)","text":"<p>Percona is glad to announce the first GA (Generally Available) release of Percona Server for MySQL 5.7.10-3 on February 23<sup>rd</sup>, 2016 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.10, including all the bug fixes in it, Percona Server for MySQL 5.7.10-3 is the current Generally Available release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.10-3 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.10-3.html#new-features","title":"New Features","text":"<p>A complete list of changes between Percona Server for MySQL 5.6 and 5.7 can be seen in Changed in Percona Server 5.7.</p> <p>Percona Server for MySQL has implemented a multi-threaded asynchronous LRU flusher. This work also allows to safely use of the <code>backoff</code> value for the <code>innodb_empty_free_list_algorithm</code> server system variable, and its default has been changed accordingly.</p>"},{"location":"release-notes/Percona-Server-5.7.10-3.html#known-issues","title":"Known Issues","text":"<p>In Percona Server for MySQL 5.7 super_read_only feature has been replaced with upstream implementation. There are currently two known issues compared to Percona Server for MySQL 5.6 implementation:</p> <ul> <li> <p>Bug #78963, <code>super_read_only</code> aborts <code>STOP SLAVE</code> if variable <code>relay_log_info_repository</code> is set to <code>TABLE</code> which could lead to a server crash in Debug builds.</p> </li> <li> <p>Bug #79328, <code>super_read_only</code> set as a server option has no effect.</p> </li> </ul> <p>InnoDB crash recovery might fail if <code>innodb_flush_method</code> is set to <code>ALL_O_DIRECT</code>. The workaround is to set this variable to a different value before starting up the crashed instance (bug #1529885).</p>"},{"location":"release-notes/Percona-Server-5.7.10-3.html#bugs-fixed","title":"Bugs Fixed","text":"<p>Percona Server for MySQL <code>5.7.10-1</code> didn\u2019t write the initial root password into the log file <code>/var/log/mysqld.log</code> during the installation on CentOS 6. Bug fixed #1541769.</p> <p>The cardinality of partitioned TokuDB tables became inaccurate after the changes introduced by the TokuDB Background ANALYZE TABLE feature in Percona Server for MySQL <code>5.7.10-1</code>. Bug fixed #925.</p> <p>Running the <code>TRUNCATE TABLE</code> while TokuDB Background ANALYZE TABLE is enabled could lead to a server crash once analyze job tries to access the truncated table. Bug fixed #938.</p> <p>Percona TokuBackup would fail with an unclear error if the backup process found <code>mysqld_safe.pid</code> file (owned by root) inside the <code>datadir</code>. Fixed by excluding the <code>pid</code> file by default. Bug fixed #125.</p> <p>The PAM Authentication Plugin build warning has been fixed. Bug fixed #1541601.</p>"},{"location":"release-notes/Percona-Server-5.7.11-4.html","title":"Percona Server for MySQL 5.7.11-4 (2016-03-15)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.11-4 on March 15<sup>th</sup>, 2016 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.11, including all the bug fixes in it, Percona Server for MySQL 5.7.11-4 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.11-4 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.11-4.html#new-features","title":"New Features","text":"<p>Percona Server for MySQL has implemented a Parallel doublewrite buffer.</p> <p>The TokuDB Background ANALYZE TABLE feature is now enabled by default (<code>tokudb_analyze_in_background</code> is set to <code>ON</code> by default). Variable <code>tokudb_auto_analyze</code> default value has been changed from <code>0</code> to <code>30</code>. (#935)</p> <p>Suppress Warning Messages feature has been removed from Percona Server for MySQL 5.7 because MySQL 5.7.11 has implemented a new system variable, log_statements_unsafe_for_binlog, which implements the same effect.</p>"},{"location":"release-notes/Percona-Server-5.7.11-4.html#bugs-fixed","title":"Bugs Fixed","text":"<p>If <code>pid-file</code> option wasn\u2019t specified with the full path, Ubuntu/Debian <code>sysvinit</code> script wouldn\u2019t notice the server is actually running and it will timeout or in some cases even hang. Bug fixed #1549333.</p> <p>Buffer pool may fail to remove dirty pages for a particular tablespace from the flush list, as requested by, for example, <code>DROP TABLE</code> or <code>TRUNCATE TABLE</code> commands. This could lead to a crash. Bug fixed #1552673.</p> <p>Audit Log Plugin worker thread may crash on write call writing fewer bytes than requested. Bug fixed #1552682 (upstream #80606).</p> <p>Percona Server for MySQL 5.7 <code>systemd</code> script now takes the last option specified in <code>my.cnf</code> if the same option is specified multiple times. Previously it would try to take all values which would break the script and server would fail to start. Bug fixed #1554976.</p> <p><code>mysqldumpslow</code> script has been removed because it was not compatible with Percona Server for MySQL extended slow query log format. Please use pt-query-digest from Percona Toolkit instead. Bug fixed #856910.</p> <p>Other bugs fixed: #1521120, #1549301 (upstream #80496), and #1554043 (upstream #80607).</p>"},{"location":"release-notes/Percona-Server-5.7.12-5.html","title":"Percona Server for MySQL 5.7.12-5 (2016-06-06)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.12-5 on June 6<sup>th</sup>, 2016 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.12, including all the bug fixes in it, Percona Server for MySQL 5.7.12-5 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.12-5 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.12-5.html#bugs-fixed","title":"Bugs Fixed","text":"<p><code>MEMORY</code> storage engine did not support JSON columns. Bug fixed #1536469.</p> <p>When Read Free Replication was enabled for TokuDB and there was no explicit primary key for the replicated TokuDB table there could be duplicated records in the table on update operation. The fix disables Read Free Replication for tables without explicit primary key and does rows lookup for <code>UPDATE</code> and <code>DELETE</code> binary log events and issues warning. Bug fixed #1536663 (#950).</p> <p>Attempting to execute a non-existing prepared statement with Response Time Distribution plugin enabled could lead to a server crash. Bug fixed #1538019.</p> <p>TokuDB was using different memory allocators, this was causing <code>safemalloc</code> warnings in debug builds and crashes because memory accounting didn\u2019t add up. Bug fixed #1546538 (#962).</p> <p>Adding an index to an InnoDB temporary table while <code>expand_fast_index_creation</code> was enabled could lead to server assertion. Bug fixed #1554622.</p> <p>Percona Server for MySQL was missing the <code>innodb_numa_interleave</code> server variable. Bug fixed #1561091 (upstream #80288).</p> <p>Running <code>SHOW STATUS</code> in parallel to online buffer pool resizing could lead to a server crash. Bug fixed #1577282.</p> <p>InnoDB crash recovery might fail if <code>innodb_flush_method</code> was set to <code>ALL_O_DIRECT</code>. Bug fixed #1529885.</p> <p>Fixed heap allocator/deallocator mismatch in Metrics for scalability measurement. Bug fixed #1581051.</p> <p>Percona Server for MySQL is now built with the system <code>zlib</code> library instead of the older bundled one. Bug fixed #1108016.</p> <p><code>CMake</code> would fail if TokuDB tests passed. Bug fixed #1521566.</p> <p>Reduced the memory overhead per page in the InnoDB buffer pool. The fix was based on the Facebook patch #91e979e. Bug fixed #1536693 (upstream #72466).</p> <p>The <code>CREATE TABLE ... LIKE ...</code> statement could create a system table with an unsupported enforced engine. Bug fixed #1540338.</p> <p>Change buffer merge could throttle to 5% of I/O capacity on an idle server. Bug fixed #1547525.</p> <p>Parallel doublewrite memory was not freed with <code>innodb_fast_shutdown</code> set to <code>2</code>. Bug fixed #1578139.</p> <p>The server will now show a more descriptive error message when Percona Server for MySQL fails with <code>errno == 22 \"Invalid argument\"</code> if <code>innodb_flush_method</code> was set to <code>ALL_O_DIRECT</code>. Bug fixed #1578604.</p> <p>The error log warning <code>Too many connections</code> was only printed for connection attempts when <code>max_connections</code> + one <code>SUPER</code> have connected. If the extra <code>SUPER</code> is not connected, the warning was not printed for a non-SUPER connection attempt. Bug fixed #1583553.</p> <p><code>apt-cache show</code> command for <code>percona-server-client</code> was showing <code>innotop</code> included as part of the package. Bug fixed #1201074.</p> <p>A replication slave would fail to connect to a master running 5.5. Bug fixed #1566642 (upstream #80962).</p> <p>Upgrade logic for figuring if TokuDB upgrade can be performed from the version on disk to the current version was broken due to regression introduced when fixing #684 in Percona Server for MySQL <code>5.7.11-4</code>. Bug fixed #717.</p> <p>Fixed <code>jemalloc</code> version parsing error. Bug fixed #528.</p> <p>If <code>ALTER TABLE</code> was run while <code>tokudb_auto_analyze</code> variable was enabled it would trigger auto-analysis, which could lead to a server crash if <code>ALTER TABLE DROP KEY</code> was used because it would be operating on the old table/key meta-data. Bug fixed #945.</p> <p>The <code>tokudb_pk_insert_mode</code> session variable has been deprecated and the behavior will be that of the former <code>tokudb_pk_insert_mode</code> set to <code>1</code>. The optimization will be used were safe and not used were not safe. Bug fixed #952.</p> <p>Bug in TokuDB Index Condition Pushdown was causing <code>ORDER BY DESC</code> to reverse the scan outside of the WHERE bounds. This would cause a query to hang in a <code>sending data</code> state for several minutes in some environments with large amounts of data (3 billion records) if the <code>ORDER BY DESC</code> statement was used. Bugs fixed #988, #233, and #534.</p> <p>Other bugs fixed: #1510564 (upstream #78981), #1533482 (upstream #79999), #1553166, #1496282 (#964), #1496786 (#956), #1566790, #718, #914, #937, #954, #955, #970, #971, #972, #976, #977, #981, #982, #637, and #982.</p>"},{"location":"release-notes/Percona-Server-5.7.13-6.html","title":"Percona Server for MySQL 5.7.13-6 (2016-07-16)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.13-6 on July 6<sup>th</sup>, 2016 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.13, including all the bug fixes in it, Percona Server for MySQL 5.7.13-6 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.13-6 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.13-6.html#new-features","title":"New Features","text":"<p>TokuDB MTR suite is now part of the default MTR suite in Percona Server for MySQL 5.7.</p>"},{"location":"release-notes/Percona-Server-5.7.13-6.html#bugs-fixed","title":"Bugs Fixed","text":"<p>Querying the <code>GLOBAL_TEMPORARY_TABLES</code> table would cause a server crash if temporary table owning threads would execute new queries. Bug fixed #1581949.</p> <p><code>IMPORT TABLESPACE</code> and undo tablespace truncate could get stuck indefinitely with a writing workload in parallel. Bug fixed #1585095.</p> <p>Requesting to flush the whole of the buffer pool with doublewrite parallel buffer wasn\u2019t working correctly. Bug fixed #1586265.</p> <p>Audit Log Plugin would hang when trying to write a log record of <code>audit_log_buffer_size</code> length. Bug fixed #1588439.</p> <p>Audit log in <code>ASYNC</code> mode could skip log records that don\u2019t fit into the log buffer. Bug fixed #1588447.</p> <p>In order to support <code>innodb_flush_method</code> being set to <code>ALL_O_DIRECT</code>, the log I/O buffers were aligned to <code>innodb_log_write_ahead_size</code>. This missed that the variable is dynamic and could still cause the server to crash. Bug fixed #1597143.</p> <p>InnoDB tablespace import would fail when trying to import a table with a different data directory. Bug fixed #1548597 (upstream #76142).</p> <p>The Audit Log plugin was truncating SQL queries to 512 bytes. Bug fixed #1557293.</p> <p><code>mysqlbinlog</code> did not free the existing connection before opening a new remote one. Bug fixed #1587840 (upstream #81675).</p> <p>Fixed a memory leak in <code>mysqldump</code>. Bug fixed #1588845 (upstream #81714).</p> <p>Transparent Huge Pages check will now only happen if <code>tokudb_check_jemalloc</code> option is set. Bugs fixed #939 and #713.</p> <p>Logging in <code>ydb</code> environment validation functions now prints more useful context. Bug fixed #722.</p> <p>Other bugs fixed: #1541698 (upstream #80261), #1587426 (upstream, #81657), #1589431, #956, #964,</p>"},{"location":"release-notes/Percona-Server-5.7.14-7.html","title":"Percona Server for MySQL 5.7.14-7 (2016-08-23)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.14-7 on August 23<sup>rd</sup>, 2016 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.13, including all the bug fixes in it, Percona Server for MySQL 5.7.14-7 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.14-7 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.14-7.html#new-features","title":"New Features","text":"<p>Percona Server for MySQL Audit Log Plugin now supports filtering by user, database, and sql_command.</p> <p>Percona Server for MySQL now supports tree map file block allocation strategy for TokuDB.</p>"},{"location":"release-notes/Percona-Server-5.7.14-7.html#bugs-fixed","title":"Bugs Fixed","text":"<p>Fixed the potential cardinality <code>0</code> issue for TokuDB tables if <code>ANALYZE TABLE</code> finds only deleted rows and no actual logical rows before it times out. Bug fixed #1607300 (#1006, #732).</p> <p>TokuDB <code>database.table.index</code> names longer than 256 characters could cause server crash if background analyze table status was checked while running. Bug fixed #1005.</p> <p>PAM Authentication Plugin would abort authentication while checking UNIX user group membership if there were more than a thousand members. Bug fixed #1608902.</p> <p>If <code>DROP DATABASE</code> would fail to delete some of the tables in the database, the partially-executed command is logged in the binlog as <code>DROP TABLE t1, t2, ...</code>  for the tables for which drop succeeded. A slave might fail to replicate such <code>DROP TABLE</code> statement if there exist foreign key relationships to any of the dropped tables and the slave has a different schema from master. Fix by checking, on the master, whether any of the database to be dropped tables participate in a Foreign Key relationship, and fail the <code>DROP DATABASE</code> statement immediately. Bug fixed #1525407 (upstream #79610).</p> <p>PAM Authentication Plugin didn\u2019t support spaces in the UNIX user group names. Bug fixed #1544443.</p> <p>Due to security reasons <code>ld_preload</code> libraries can now only be loaded from the system directories (<code>/usr/lib64</code>, <code>/usr/lib</code>) and the MySQL installation base directory.</p> <p>In the client library, any EINTR received during network I/O was not handled correctly. Bug fixed #1591202 (upstream #82019).</p> <p><code>SHOW GLOBAL STATUS</code> was locking more than the upstream implementation which made it less suitable to be called with high frequency. Bug fixed #1592290.</p> <p>The included <code>.gitignore</code> in the percona-server source distribution had a line <code>\\*.spec</code>, which means someone trying to check in a copy of the percona-server source would be missing the spec file required to build the RPMs. Bug fixed #1600051.</p> <p>Audit Log Plugin did not transcode queries. Bug fixed #1602986.</p> <p>If the changed page bitmap redo log tracking thread stops due to any reason, then shutdown will wait for a long time for the log tracker thread to quit, which it never does. Bug fixed #1606821.</p> <p>Changed page tracking was initialized too late by InnoDB. Bug fixed #1612574.</p> <p>Fixed stack buffer overflow if <code>--ssl-cipher</code> had more than 4000 characters. Bug fixed #1596845 (upstream #82026).</p> <p>Audit Log Plugin events did not report the default database. Bug fixed #1435099.</p> <p>Canceling the TokuDB Background ANALYZE TABLE job twice or while it was in the queue could lead to server assertion. Bug fixed #1004.</p> <p>Fixed various spelling errors in comments and function names. Bug fixed #728 (Otto Kek\u00e4l\u00e4inen)</p> <p>Implemented a set of fixes to make PerconaFT build and run on the AArch64 (64-bit ARMv8) architecture. Bug fixed #726 (Alexey Kopytov).</p> <p>Other bugs fixed: #1542874 (upstream #80296), #1610242, #1604462 (upstream #82283), #1604774 (upstream #82307), #1606782, #1607359, #1607606, #1607606, #1607671, #1609422, #1610858, #1612551, #1613663, #1613986, #1455430, #1455432, #1581195, #998, #1003, and #730.</p>"},{"location":"release-notes/Percona-Server-5.7.14-8.html","title":"Percona Server for MySQL 5.7.14-8 (2016-09-21)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.14-8 on September 21<sup>st</sup>, 2016 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.14, including all the bug fixes in it, Percona Server for MySQL 5.7.14-8 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.14-8 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.14-8.html#bugs-fixed","title":"Bugs Fixed","text":"<p>Limiting <code>ld_preload</code> libraries to be loaded from specific directories in <code>mysqld_safe</code> didn\u2019t work correctly for relative paths. Bug fixed #1624247.</p> <p>Fixed a possible privilege escalation that could be used when running <code>REPAIR TABLE</code> on a <code>MyISAM</code> table. Bug fixed #1624397.</p> <p>The general query log and slow query log cannot be written to files ending in <code>.ini</code> and <code>.cnf</code> anymore. Bug fixed #1624400.</p> <p>Implemented restrictions on symlinked files (<code>error_log</code>, <code>pid_file</code>) that can\u2019t be used with <code>mysqld_safe</code>. Bug fixed #1624449.</p> <p>Other bugs fixed: #1553938.</p>"},{"location":"release-notes/Percona-Server-5.7.15-9.html","title":"Percona Server for MySQL 5.7.15-9 (2016-10-21)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.15-9 on October 21<sup>st</sup>, 2016 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.15, including all the bug fixes in it, Percona Server for MySQL 5.7.15-9 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.15-9 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.15-9.html#new-features","title":"New Features","text":"<p>A new TokuDB <code>tokudb_dir_per_db</code> option has been introduced to address two TokuDB shortcomings, the renaming of data files on table/index rename, and the ability to group data files together within a directory that represents a single database. This feature is enabled by default.</p>"},{"location":"release-notes/Percona-Server-5.7.15-9.html#bugs-fixed","title":"Bugs Fixed","text":"<p>The Audit Log plugin malformed record could be written after <code>audit_log_flush</code> was set to <code>ON</code> in <code>ASYNC</code> and <code>PERFORMANCE</code> modes. Bug fixed #1613650.</p> <p>Running <code>SELECT DISTINCT x...ORDER BY y LIMIT N,N</code> could lead to a server crash. Bug fixed #1617586.</p> <p>Workloads with statements that take non-transactional locks (<code>LOCK TABLES</code>, global read lock, and similar) could have caused deadlocks when running under Thread Pool with high priority queue enabled and <code>thread_pool_high_prio_mode</code> set to <code>transactions</code>. Fixed by placing such statements into the high priority queue even with the above <code>thread_pool_high_prio_mode</code> setting. Bugs fixed #1619559 and #1374930.</p> <p>Fixed memory leaks in Audit Log Plugin. Bug fixed #1620152 (upstream #71759).</p> <p>A server could crash due to a <code>glibc</code> bug in handling short-lived detached threads. Bug fixed #1621012 (upstream #82886).</p> <p><code>QUERY_RESPONSE_TIME_READ</code> and <code>QUERY_RESPONSE_TIME_WRITE</code> were returning <code>QUERY_RESPONSE_TIME</code> table data if accessed through a name that is not full uppercase. Bug fixed #1552428.</p> <p>Cipher <code>ECDHE-RSA-AES128-GCM-SHA256</code> was listed in the list of supported ciphers but it wasn\u2019t supported. Bug fixed #1622034 (upstream #82935).</p> <p>Successful recovery of a torn page from the doublewrite buffer was shown as a warning in the error log. Bug fixed #1622985.</p> <p>LRU manager threads could run too long on a server shutdown, causing a server crash. Bug fixed #1626069.</p> <p><code>tokudb_default</code> was not recognized by Percona Server for MySQL as a valid row format. Bug fixed #1626206.</p> <p>InnoDB <code>ANALYZE TABLE</code> didn\u2019t remove its table from the background statistics processing queue. Bug fixed #1626441 (upstream #71761).</p> <p>Upstream merge for #81657 to 5.6 was incorrect. Bug fixed #1626936 (upstream #83124).</p> <p>Fixed multi-threaded slave thread leaks that happened in case of thread create failure. Bug fixed #1619622 (upstream #82980).</p> <p>The shutdown waiting for a purge to complete was undiagnosed for the first minute. Bug fixed #1616785.</p> <p>Other bugs fixed: #1614439, #1614949, #1624993 (#736), #1613647, #1615468, #1617828, #1617833, #1626002 (upstream #83073), #904714, #1610102, #1610110, #1613728, #1614885, #1615959, #1616333, #1616404, #1616768, #1617150, #1617216, #1617267, #1618478, #1618819, #1619547, #1619572, #1620583, #1622449, #1623011, #1624992 (#1014), #735, #1626500, #1628913, #952920, and #964.</p>"},{"location":"release-notes/Percona-Server-5.7.16-10.html","title":"Percona Server for MySQL 5.7.16-10 (2016-11-28)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.16-10 on November 28<sup>th</sup>, 2016 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.16, including all the bug fixes in it, Percona Server for MySQL 5.7.16-10 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.16-10 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.16-10.html#deprecated-features","title":"Deprecated Features","text":"<p>Metrics for scalability measurement feature is now deprecated. Users who have installed this plugin but are not using its capability are advised to uninstall the plugin due to known crashing bugs.</p>"},{"location":"release-notes/Percona-Server-5.7.16-10.html#bugs-fixed","title":"Bugs Fixed","text":"<p>When a stored routine would call an administrative command such as <code>OPTIMIZE TABLE</code>, <code>ANALYZE TABLE</code>, <code>ALTER TABLE</code>, <code>CREATE/DROP INDEX</code>, etc. the effective value of <code>log_slow_sp_statements</code> was overwritten by the value of <code>log_slow_admin_statements</code>. Bug fixed #719368.</p> <p>The server wouldn\u2019t start after crash with with <code>innodb_force_recovery</code> set to <code>6</code> if a parallel doublewrite file existed. Bug fixed #1629879.</p> <p>The Thread Pool <code>thread limit reached</code> and <code>failed to create thread</code> messages are now printed on the first occurrence as well. Bug fixed #1636500.</p> <p><code>INFORMATION_SCHEMA.TABLE_STATISTICS</code> and <code>INFORMATION_SCHEMA.INDEX_STATISTICS</code> tables were not correctly updated for TokuDB. Bug fixed #1629448.</p> <p>Other bugs fixed: #1633061, #1633430, and #1635184.</p>"},{"location":"release-notes/Percona-Server-5.7.17-11.html","title":"Percona Server for MySQL 5.7.17-11 (2017-02-03)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.17-11 on February 3<sup>rd</sup>, 2017 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.17, including all the bug fixes in it, Percona Server for MySQL 5.7.17-11 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.17-11 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.17-11.html#new-features","title":"New Features","text":"<p>Percona Server for MySQL has implemented support for per-column <code>VARCHAR/BLOB</code> compression for the XtraDB storage engine. This also features compression dictionary support, to improve compression ratio for relatively short individual rows, such as JSON data.</p> <p>The Kill Idle Transactions feature has been re-implemented by setting a connection socket read timeout value instead of periodically scanning the internal InnoDB transaction list. This makes the feature applicable to any transactional storage engine, such as TokuDB, and, in future, MyRocks. This re-implementation is also addressing some existing bugs, including server crashes: #1166744, #1179136, #907719, and #1369373.</p>"},{"location":"release-notes/Percona-Server-5.7.17-11.html#bugs-fixed","title":"Bugs Fixed","text":"<p>Logical row counts for TokuDB tables could get inaccurate over time. Bug fixed #1651844 (#732).</p> <p>Repeated execution of <code>SET STATEMENT ... FOR &lt;SELECT FROM view&gt;</code> could lead to a server crash. Bug fixed #1392375.</p> <p><code>CREATE TEMPORARY TABLE</code> would create a transaction in binary log on a read only server. Bug fixed #1539504 (upstream #83003).</p> <p>Using Per-query variable statement with subquery temporary tables could cause a memory leak. Bug fixed #1635927.</p> <p>Fixed new compilation warnings with GCC 6. Bugs fixed #1641612 and #1644183.</p> <p>A server could crash if a bitmap write I/O error happens in the background log tracking thread while a <code>FLUSH CHANGED_PAGE_BITMAPS</code> is executing concurrently. Bug fixed #1651656.</p> <p>TokuDB was using wrong function to calculate free space in data files. Bug fixed #1656022 (#1033).</p> <p><code>CONCURRENT_CONNECTIONS</code> column in the <code>USER_STATISTICS</code> table was showing incorrect values. Bug fixed #728082.</p> <p>Audit Log Plugin when set to <code>JSON</code> format was not escaping characters properly. Bug fixed #1548745.</p> <p>InnoDB index dives did not detect some of the concurrent tree changes, which could return bogus estimates. Bug fixed #1625151 (upstream #84366).</p> <p><code>INFORMATION_SCHEMA.INNODB_CHANGED_PAGES</code> queries would needlessly read potentially incomplete bitmap data past the needed LSN range. Bug fixed #1625466.</p> <p>Percona Server for MySQL <code>cmake</code> compiler would always attempt to build RocksDB even if <code>-DWITHOUT_ROCKSDB=1</code> argument was specified. Bug fixed #1638455.</p> <p>Lack of free pages in the buffer pool is not diagnosed with <code>innodb_empty_free_list_algorithm</code> set to <code>backoff</code> (which is the default). Bug fixed #1657026.</p> <p><code>mysqld_safe</code> now limits the use of <code>rm</code> and <code>chown</code> to avoid privilege escalation. <code>chown</code> can now be used only for <code>/var/log</code> directory. Bug fixed #1660265. Thanks to Dawid Golunski (https://legalhackers.com).</p> <p>Renaming a TokuDB table to a non-existent database with <code>tokudb_dir_per_db</code> enabled would lead to a server crash. Bug fixed #1030.</p> <p>Read Free Replication optimization could not be used for TokuDB partition tables. Bug fixed #1012.</p> <p>Other bugs fixed: #1486747, #1617715, #1633988, #1638198 (upstream #82823), #1642230, #1646384, #1640810, #1647530, #1651121, #1658843, #1156772, #1644583, #1648389, #1648737, #1650256, and #1647723.</p>"},{"location":"release-notes/Percona-Server-5.7.17-12.html","title":"Percona Server for MySQL 5.7.17-12 (2017-03-24)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.17-12 on March 24<sup>th</sup>, 2017 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.17, including all the bug fixes in it, Percona Server for MySQL 5.7.17-12 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.17-12 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.17-12.html#new-features","title":"New Features","text":"<p>Percona Server for MySQL has implemented new mysqldump <code>--order-by-primary-desc</code> option. This feature tells <code>mysqldump</code> to take the backup by descending primary key order (<code>PRIMARY KEY DESC</code>) which can be useful if the storage engine is using a reverse order column family for a primary key.</p> <p>mysqldump will now detect when MyRocks is installed and available by seeing if there is a session variable named <code>rocksdb_skip_fill_cache</code> and setting it to <code>1</code> if it exists.</p> <p>Now mysqldump automatically enables the session variable <code>rocksdb_bulk_load</code> if it is supported by the target server.</p>"},{"location":"release-notes/Percona-Server-5.7.17-12.html#bugs-fixed","title":"Bugs Fixed","text":"<p>If the variable <code>thread_handling</code> was set to <code>pool-of-threads</code> in the MySQL configuration file, the server couldn\u2019t be gracefully shut down by a <code>SIGTERM</code> signal. Bug fixed #1537554.</p> <p>When <code>innodb_ft_result_cache_limit</code> was exceeded by internal memory allocated by InnoDB during the FT scan not all memory was released which could lead to server assertion. Bug fixed #1634932 (upstream #83648).</p> <p>Executing the <code>FLUSH LOGS</code> on a read-only slave with a user that doesn\u2019t have the <code>SUPER</code> privilege would result in <code>Error 1290</code>. Bug fixed #1652852 (upstream #84350).</p> <p><code>FLUSH LOGS</code> was disabled with <code>read_only</code> and <code>super_read_only</code> variables. Bug fixed #1654682 (upstream #84437).</p> <p>If <code>SHOW BINLOGS</code> or <code>PERFORMANCE_SCHEMA.GLOBAL_STATUS</code> query, and a transaction commit would run in parallel, they could deadlock. Bug fixed #1657128.</p> <p>A long-running binary log commit would block <code>SHOW STATUS</code>, which in turn could block a number of other operations such as client connects and disconnects. Bug fixed  #1646100.</p> <p>Log tracking initialization did not find the last valid bitmap data correctly. Bug fixed #1658055.</p> <p>A query using a range scan with a complex range condition could lead to a server crash. Bug fixed #1660591 (upstream #84736).</p> <p>Race condition between buffer pool page optimistic access and eviction could lead to a server crash. Bug fixed #1664280.</p> <p>If Audit Log Plugin was unable to create a file pointed by <code>audit_log_file</code>, the server would crash during the startup. Bug fixed #1666496.</p> <p>A <code>DROP TEMPORARY TABLE ...</code>  for a table created by a <code>CREATE TEMPORARY TABLE ... SELECT ...</code> would get logged in the binary log on a disconnect with mixed mode replication. Bug fixed #1671013.</p> <p>TokuDB did not use index with even if cardinality was good. Bug fixed #1671152.</p> <p>Row-based replication events were not reflected in <code>Rows_updated</code> fields in the User Statistics <code>INFORMATION_SCHEMA</code> tables. Bug fixed #995624.</p> <p>When <code>DuplicateWeedout</code> strategy was used for joins, use was not reported in the query plan info output extension for the slow query log. Bug fixed #1592694.</p> <p>It was impossible to use column compression dictionaries with partitioned InnoDB tables. Bug fixed #1653104.</p> <p>Diagnostics for OpenSSL errors have been improved. Bug fixed #1660339 (upstream #75311).</p> <p>Other bugs fixed: #1665545, #1650321, #1654501, #1663251, #1659548, #1663452, #1670834, #1672871, #1626545, #1658006, #1658021, #1659218, #1659746, #1660239, #1660243, #1660348, #1662163 (upstream #81467), #1664219, #1664473, #1671076, and #1671123.</p>"},{"location":"release-notes/Percona-Server-5.7.17-13.html","title":"Percona Server for MySQL 5.7.17-13 (2017-04-05)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.17-13 on April 5<sup>th</sup>, 2017 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.17, including all the bug fixes in it, Percona Server for MySQL 5.7.17-13 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.17-13 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.17-13.html#bugs-fixed","title":"Bugs Fixed","text":"<p>MyRocks storage engine detection implemented in <code>mysqldump</code> in Percona Server for MySQL <code>5.6.17-12</code> was using a deprecated <code>INFORMATION_SCHEMA.SESSION_VARIABLES</code> table, causing <code>mysqldump</code> failures on servers running with the <code>show_compatibility_56</code> variable set to <code>OFF</code>. Bug fixed #1676401.</p>"},{"location":"release-notes/Percona-Server-5.7.18-14.html","title":"Percona Server for MySQL 5.7.18-14 (2017-05-12)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.18-14 on May 12, 2017 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.18, including all the bug fixes in it, Percona Server for MySQL 5.7.18-14 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.18-14 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.18-14.html#new-features","title":"New Features","text":"<p>Percona Server for MySQL 5.7 packages are now available for Ubuntu 17.04 (Zesty Zapus).</p> <p>Percona Server for MySQL now supports Prefix Index Queries Optimization. This feature was ported from a Facebook MySQL patch.</p> <p>Percona Server for MySQL has implemented support for Gap locks detection for transactional storage engines, like MyRocks, that do not support gap locks. This feature was ported from a Facebook MySQL patch.</p> <p><code>tokudb_dir_cmd</code> can now be used to edit the TokuDB directory map. This feature is currently considered Experimental.</p>"},{"location":"release-notes/Percona-Server-5.7.18-14.html#bugs-fixed","title":"Bugs Fixed","text":"<p>A deadlock could occur in I/O-bound workloads when the server was using several small buffer pool instances in combination with small redo log files and variable </p> <p><code>innodb_empty_free_list_algorithm</code> set to <code>backoff</code> algorithm. Bug fixed  #1651657.</p> <p>Fixed a memory leak in Percona TokuBackup. Bug fixed #1669005.</p> <p>Compressed columns with dictionaries could not be added to a partitioned table by using <code>ALTER TABLE</code>. Bug fixed #1671492.</p> <p>Fixed a memory leak that happened in case of a failure to create a multi-threaded slave worker thread. Bug fixed #1675716.</p> <p>In-Place upgrade from Percona Server for MySQL 5.6 to 5.7 by using standalone packages would fail if <code>/var/lib/mysql</code> wasn\u2019t defined as the <code>datadir</code>. Bug fixed #1687276.</p> <p>A combination of using any audit API-using plugin, like Audit Log Plugin and Response Time Distribution, with multi-byte collation connection and a <code>PREPARE</code> statement with a parse error could lead to a server crash. Bug fixed #1688698 (upstream #86209).</p> <p>Fix for a #1433432 bug that caused a performance regression due to suboptimal LRU manager thread flushing heuristics. Bug fixed #1631309.</p> <p>Creating Compressed columns with dictionaries in MyISAM tables by specifying partition engines would not result in an error. Bug fixed #1631954.</p> <p>It was not possible to configure basedir as a symlink. Bug fixed #1639735.</p> <p>Replication slave did not report <code>Seconds_Behind_Master</code> correctly when running in multi-threaded slave mode. Bug fixed #1654091 (upstream #84415).</p> <p><code>DROP TEMPORARY TABLE</code> would create a transaction in binary log on a read-only server. Bug fixed #1668602 (upstream #85258).</p> <p>Processing GTIDs in the relay log that were already been executed were causing write/fsync amplification. Bug fixed #1669928 (upstream #85141).</p> <p>Text/BLOB fields were not handling sorting of the empty string consistently between InnoDB and filesort. Bug fixed #1674867 (upstream #81810) by porting a Facebook patch for MySQL.</p> <p>InnoDB adaptive hash index was using a partitioning algorithm that would produce uneven distribution when the server contained many tables with an identical schema. Bug fixed #1679155 (upstream #81814).</p> <p>For plugin variables that are signed numbers, doing a <code>SHOW VARIABLES</code> would always show an unsigned number. Fixed by porting a Facebook patch for MySQL.</p> <p>Other bugs fixed: #1629250 (upstream #83245), #1660828 (upstream #84786), #1664519 (upstream #84940), #1674299, #1670588 (upstream #84173), #1672389, #1674507, #1675623, #1650294, #1659224, #1662908, #1669002, #1671473, #1673800, #1674284, #1676441, #1676705, #1676847 (upstream #85671), #1677130 (upstream #85678), #1677162, #1677943, #1678692, #1680510 (upstream #85838), #1683993, #1684012, #1684078, #1684264, #1687386, #1687432, #1687600, and #1674281.</p>"},{"location":"release-notes/Percona-Server-5.7.18-15.html","title":"Percona Server for MySQL 5.7.18-15 (2017-05-26)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.18-15 on May 26, 2017 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.18, including all the bug fixes in it, Percona Server for MySQL 5.7.18-15 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.18-15 milestone at Launchpad</p>"},{"location":"release-notes/Percona-Server-5.7.18-15.html#bugs-fixed","title":"Bugs Fixed","text":"<p>The server could crash when querying the partitioning table with a single partition. Bug fixed #1657941 (upstream #76418).</p> <p>Running a query on the InnoDB table with ngram full-text parser and a <code>LIMIT</code> clause could lead to a server crash. Bug fixed #1679025 (upstream #85835).</p>"},{"location":"release-notes/Percona-Server-5.7.18-16.html","title":"Percona Server for MySQL 5.7.18-16 (2017-07-28)","text":"<p>Percona is glad to announce the GA (Generally Available) release of Percona Server for MySQL 5.7.18-16 on July 28, 2017 (Downloads are available here and from the Percona Software Repositories).</p> <p>Based on MySQL 5.7.18, including all the bug fixes in it, Percona Server for MySQL 5.7.18-16 is the current GA release in the Percona Server for MySQL 5.7 series. All of Percona\u2019s software is open-source and free, all the details of the release can be found in the 5.7.18-16 milestone at Launchpad</p> <p>Please note that RHEL 5, CentOS 5, and Ubuntu versions 12.04 and older are not supported in future releases of Percona Server for MySQL and no further packages are added for these distributions.</p>"},{"location":"release-notes/Percona-Server-5.7.18-16.html#new-feature","title":"New Feature","text":"<p>Percona Server for MySQL is now available on Debian 9 (stretch). The support only covers the <code>amd64</code> architecture.</p> <p>Percona Server for MySQL can now be built with the support of OpenSSL 1.1.</p> <p>MyRocks storage engine has been merged into Percona Server for MySQL.</p> <p>TokuDB is able to kill a query that is awaiting an FT locktree lock.</p> <p>TokuDB enables using the <code>MySQL DEBUG_SYNC</code> facility within Percona FT.</p>"},{"location":"release-notes/Percona-Server-5.7.18-16.html#bugs-fixed","title":"Bugs Fixed","text":"<p>Row counts in TokuDB could be lost intermittently after restarts. Bug fixed #2.</p> <p>In TokuDB, two races in the fractal tree lock manager could significantly affect transactional throughput for some applications that used a small number of concurrent transactions.  These races manifested as transactions unnecessarily waiting for an available lock. Bug fixed #3.</p> <p>Percona FT could assert when opening a dictionary with no useful information to the error log. Bug fixed #23.</p> <p>Percona FT could assert for various reasons deserializing nodes with no useful error output. Bug fixed #24.</p> <p>It was not possible to build Percona Server for MySQL on Debian 9 (stretch) due to issues with OpenSSL 1.1. Bug fixed #1702903 (upstream #83814).</p> <p>Packaging was using the <code>dpkg --verify</code> command which is not available on wheezy/precise. Bug fixed #1694907.</p> <p>Enabling and disabling the slow query log rotation spuriously added the version suffix to the next slow query log file name. Bug fixed #1704056.</p> <p>With two client connections to a server (debug server build), the server could crash after one of the clients set the global option <code>userstat</code> and flushed the client statistics (<code>FLUSH CLIENT_STATISTICS</code>), and then both clients were closed. Bug fixed #1661488.</p> <p>Percona FT did not pass cmake flags on to snappy cmake. Bug fixed #41.  The progress status for partitioned TokuDB table ALTERs was misleading. Bug fixed #42.</p> <p>When a client application connecting to the Aurora cluster end point using SSL (<code>--ssl-verify-server-cert</code> or <code>--ssl-mode=VERIFY_IDENTITY</code> option), wildcard and  enabled SSL certificates were ignored. See also Compatibility Matrix.  Note that the <code>--ssl-verify-server-cert</code> option is deprecated in Percona Server for MySQL 5.7. Bug fixed #1673656 (upstream #68052).</p> <p>Killing a stored procedure execution could result in an assert failure on a debug server build. Bug fixed #1689736 (upstream #86260).</p> <p>The <code>SET STATEMENT .. FOR</code> statement changed the global instead of the session value of a variable if the statement occurred immediately after the <code>SET GLOBAL</code> or <code>SHOW GLOBAL STATUS</code> command. Bug fixed #1385352.</p> <p>When running <code>SHOW ENGINE INNODB STATUS</code>, the <code>Buffer pool size, bytes</code> entry contained 0. Bug fixed #1586262.</p> <p>The synchronization between the LRU manager and page cleaner threads was not done at shutdown. Bug fixed #1689552.</p> <p>Removed spurious <code>lock_wait_timeout_thread</code> wakeups, potentially reducing <code>lock_sys_wait_mutex</code> contention. Patch by Inaam Rama merged from <code>WebScaleSQL</code>. Bug fixed #1704267 (upstream #72123).</p> <p>Other bugs fixed: #1686603, #6, #44, #65, #1160986, #1686934, #1688319, #1689989, #1690012, #1691682, #1697700, #1699788, #1121072, and #1684601 (upstream #86016).</p> <p>Note</p> <p>Due to new package dependency, Ubuntu/Debian users should use <code>apt-get dist-upgrade</code> or <code>apt-get install percona-server-server-5.7</code> to upgrade.</p>"},{"location":"release-notes/Percona-Server-5.7.18-16.html#compatibility-matrix","title":"Compatibility Matrix","text":"Feature YaSSL OpenSSL &lt; 1.0.2 OpenSSL &gt;= 1.0.2 \u2018commonName\u2019 validation Yes Yes Yes SAN validation No Yes Yes Wildcards support No No Yes"},{"location":"release-notes/Percona-Server-5.7.19-17.html","title":"Percona Server 5.7.19-17 (2017-08-31)","text":"<p>Percona is glad to announce the release of Percona Server 5.7.19-17 on August 31, 2017. Downloads are available here and from the Percona Software Repositories.</p> <p>This release is based on MySQL 5.7.19 and includes all the bug fixes in it. Percona Server 5.7.19-17 is now the current GA release in the 5.7 series. All software developed by Percona is open-source and free. Details of this release can be found in the 5.7.19-17 milestone on Launchpad.</p> <p>Note</p> <p>Red Hat Enterprise Linux 5 (including CentOS 5 and other derivatives), Ubuntu 12.04, and older versions are no longer supported by Percona software. The reason for this announcement is that these platforms have reached the end of life, will not receive updates, and are not recommended for use in production.</p>"},{"location":"release-notes/Percona-Server-5.7.19-17.html#new-features","title":"New Features","text":"<ul> <li>Included the Percona MyRocks storage engine</li> </ul> <p>Note</p> <p>MyRocks for Percona Server is currently experimental and not recommended for production deployments until further notice. You are encouraged to try it in a testing environment and provide feedback or report bugs.</p> <ul> <li>#1708087: Added the <code>mysql-helpers</code> script to handle checking for missing <code>datadir</code> during startup. Also fixes #1635364.</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.19-17.html#platform-support","title":"Platform Support","text":"<ul> <li>Stopped providing packages for Ubuntu 12.04 due to its end of life.</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.19-17.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>#1669414: Fixed handling of failure to set <code>O_DIRECT</code> on parallel doublewrite buffer file.</p> </li> <li> <p>#1705729: Fixed the <code>postinst</code> script to correctly locate the <code>datadir</code>. Also fixes #1698019.</p> </li> <li> <p>#1709811: Fixed <code>yum upgrade</code> to not enable the <code>mysqld</code> service if it was disabled before the upgrade.</p> </li> <li> <p>#1709834: Fixed the <code>mysqld_safe</code> script to correctly locate the <code>basedir</code>.</p> </li> <li> <p>Other fixes: #1698996, #1706055, #1706262, #1706981, #1686340 (upstream #86799), #1654256 (upstream #84640), and #1651941 (upstream #84442).</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.19-17.html#tokudb-changes","title":"TokuDB Changes","text":"<ul> <li> <p>TDB-70: Removed redundant <code>fsync</code> of TokuDB redo log during binlog group commit flush stage. This fixes the issue that prevented TokuDB to run in reduced durability mode when the binlog was enabled.</p> </li> <li> <p>TDB-72: Fixed issue when renaming a table with non-alphanumeric characters in its name.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.20-18.html","title":"Percona Server 5.7.20-18 (2017-12-14)","text":"<p>Percona is glad to announce the release of Percona Server 5.7.20-18 on December 14. Downloads are available here and from the Percona Software Repositories.</p> <p>This release is based on MySQL 5.7.20 and includes all the bug fixes in it. Percona Server 5.7.20-18 is now the current GA release in the 5.7 series. All software developed by Percona is open-source and free. Details of this release can be found in the 5.7.20-18 milestone on Launchpad.</p>"},{"location":"release-notes/Percona-Server-5.7.20-18.html#new-features","title":"New Features","text":"<ul> <li> <p>Percona Server for MySQL packages are now available for Ubuntu 17.10 (Artful).</p> </li> <li> <p>As part of InnoDB Full-Text Search improvements a new <code>innodb_ft_ignore_stopwords</code> variable has been implemented which controls whether InnoDB Full-Text Search should ignore the stopword list when building/updating an FTS index. This feature is also fixing bug #1679135 (upstream #84420).</p> </li> <li> <p>Percona Server for MySQL has implemented InnoDB Page Fragmentation Counters.</p> </li> <li> <p>Percona Server for MySQL has implemented support for multiple page asynchronous I/O requests. This feature was ported from a Facebook MySQL patch.</p> </li> <li> <p>Percona Server for MySQL has implemented TokuDB integration with <code>PERFORMANCE_SCHEMA</code>.</p> </li> <li> <p>As part of Data at Rest Encryption, Percona Server for MySQL has implemented support for innodb_general_tablespace_encryption and Loading the Keyring Plugin. This feature is considered BETA quality.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.20-18.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>Percona Server for MySQL 5.7 docker images did not include TokuDB. Bugs fixed #1682419 and #1699241.</p> </li> <li> <p>If an I/O syscall returned an error during the server shutdown with Thread Pool enabled, a mutex could be left locked. Bug fixed #1702330 (Daniel Black).</p> </li> <li> <p>Dynamic row format feature to support <code>BLOB/VARCHAR</code> in <code>MEMORY</code> tables requires all the key columns to come before any <code>BLOB</code> columns. This requirement however was not enforced, allowing creating MEMORY tables in unsupported column configurations, which then crashed or lose data in usage. Bug fixed #1731483.</p> </li> <li> <p>After fixing bug #1668602, bug #1539504, and bug #1313901, <code>CREATE/DROP TEMPORARY TABLE</code> statements were forbidden incorrectly in transactional contexts, including function and trigger calls, even when they required no binary logging at all. Bug fixed #1711781.</p> </li> <li> <p>Running <code>ANALYZE TABLE</code> while a long-running query is accessing the same table in parallel could lead to a situation where new queries on the same table are blocked in a <code>Waiting for table flush</code> state. Fixed by stopping <code>ANALYZE TABLE</code> flushing affected InnoDB and TokuDB tables from the table definition cache. Bug fixed #1704195 (upstream #87065).</p> </li> <li> <p>The <code>CREATE TABLE ... LIKE ...</code> statement did not use source <code>row_format</code> on the target TokuDB table. Bug fixed #76.</p> </li> <li> <p>TokuDB would encode an already encoded database name for a directory name. Bug fixed #74.</p> </li> <li> <p>Optimizer would pick the wrong index for TokuDB tables having a hot created index, unless <code>ALTER TABLE</code> was run. Bug fixed #35.</p> </li> </ul> <p>Other bugs fixed: #1720810, #83, #80, and #75.</p>"},{"location":"release-notes/Percona-Server-5.7.20-18.html#myrocks-changes","title":"MyRocks Changes","text":"<ul> <li> <p>RocksDB has implemented a FlushWAL API which improves upon the performance of MySQL 2-phase-commit during binary log group commit flush stage. This feature adds support for using the FlushWAL API in MyRocks and also matches <code>rocksdb_flush_log_at_trx_commit</code> variable with <code>innodb_flush_log_at_trx_commit</code> behavior. Two implement this feature <code>rocksdb_manual_wal_flush</code> and <code>rocksdb_concurrent_prepare</code> variables have been implemented.</p> </li> <li> <p>New - <code>rocksdb_force_compute_memtable_stats_cachetime</code> variable has been implemented that can be used to specify how long the cached value of memtable statistics should be used instead of computing it every time during the query plan analysis.</p> </li> <li> <p>New - <code>rocksdb_large_prefix</code> variable has been implemented which, when enabled, allows index key prefixes longer than 767 bytes (up to 3072 bytes). This option mirrors the innodb_large_prefix The values for this variable should be the same between master and slave.</p> </li> <li> <p>New - <code>rocksdb_max_background_jobs</code> variable has been implemented to replace <code>rocksdb_base_background_compactions</code>, <code>rocksdb_max_background_compactions</code>, and <code>rocksdb_max_background_flushes</code> variables. This variable specifies the maximum number of background jobs. It automatically decides how many threads to allocate towards flush/compaction. It was implemented to reduce the number of (confusing) options for users and can tweak and push the responsibility down to the RocksDB level.</p> </li> <li> <p>New - <code>rocksdb_sim_cache_size</code> variable has been implemented to enable the simulated cache. This can be used to figure out the hit/miss rate with a specific cache size without changing the real block cache.</p> </li> <li> <p>Input can be now sorted by the Primary Key during the bulkload by enabling the <code>rocksdb_bulk_load_allow_unsorted</code> variable.</p> </li> <li> <p>New - <code>rocksdb_ignore_unknown_options</code> variable has been implemented, which when enabled (default) allows RocksDB to receive unknown options and not exit.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.20-19.html","title":"Percona Server 5.7.20-19 (2018-01-03)","text":"<p>Percona is glad to announce the release of Percona Server 5.7.20-19 on January 3, 2018. Downloads are available here and from the Percona Software Repositories.</p> <p>This release is based on MySQL 5.7.20 and includes all the bug fixes in it. Percona Server 5.7.20-19 is now the current GA (Generally Available) release in the 5.7 series. All software developed by Percona is open-source and free. Details of this release can be found in the 5.7.20-19 milestone on Launchpad.</p>"},{"location":"release-notes/Percona-Server-5.7.20-19.html#new-features","title":"New Features","text":"<ul> <li> <p>Now MyRocks Storage Engine has General Availability status.</p> </li> <li> <p>Binary log encryption has been implemented and can be now switched on using the <code>encrypt_binlog</code> variable.</p> </li> <li> <p><code>innodb_print_lock_wait_timeout_info</code> variable, introduced in the previous release, but undocumented, allows to log information about all lock wait timeout errors.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.20-19.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>Intermediary slave with Blackhole storage engine couldn\u2019t record updates from master to its own binary log in case the master has the <code>binlog_rows_query_log_events</code> option enabled. Bug fixed #1722789 (upstream #88057).</p> </li> <li> <p>Help command in the MySQL command line client provided a link to an older version of the Percona Server for MySQL manual. Bug fixed #1708073.</p> </li> <li> <p>A regression in the <code>mysqld_safe</code> script forced it to print an extra error when stopping the MySQL service. Bugs fixed #1738742.</p> </li> <li> <p>Blackhole storage engine was incompatible with a newer length limit of the InnoDB index key prefixes. Bug fixed #1733049 (upstream #53588).</p> </li> <li> <p>Heartbeats received by a slave were reacted with <code>mysql.slave_master_info</code> table sync on each of them even with <code>sync_master_info</code> set to zero, causing a huge increase in write load. Bug fixed #1708033 (upstream #85158).</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.20-19.html#myrocks-changes","title":"MyRocks Changes","text":"<ul> <li> <p>The replication writebatch functionality has been removed from Percona Server for MySQL 5.7 due to the unsafety of the current implementation.</p> </li> <li> <p>The variables <code>rocksdb_block_cachecompressed_hit</code>, <code>rocksdb_block_cachecompressed_miss</code>, and <code>rocksdb_getupdatessince_calls</code> were renamed to <code>rocksdb_block_cache_compressed_hit</code>, <code>rocksdb_block_cache_compressed_miss</code>, and <code>rocksdb_get_updates_since_calls</code> respectively.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.21-20.html","title":"Percona Server 5.7.21-20 (2018-02-19)","text":"<p>Percona is glad to announce the release of Percona Server 5.7.21-20 on February 19, 2018. Downloads are available here and from the Percona Software Repositories.</p> <p>This release is based on MySQL 5.7.21 and includes all the bug fixes in it. Percona Server 5.7.21-20 is now the current GA (Generally Available) release in the 5.7 series. All software developed by Percona is open-source and free.</p>"},{"location":"release-notes/Percona-Server-5.7.21-20.html#new-features","title":"New Features","text":"<ul> <li> <p>A new string variable <code>version_suffix</code> allows changing suffix for the Percona Server for MySQL version string returned by the read-only <code>version</code> variable. Also <code>version_comment</code> is converted from a global read-only to a global read-write variable.</p> </li> <li> <p>A new <code>keyring_vault_timeout</code> variable allows setting the number of seconds for the Vault server connection timeout. Bug fixed #298.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.21-20.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>The mysqld startup script was unable to detect jemalloc library location for preloading, and that prevented starting the Percona Server for MySQL on systemd-based machines. Bugs fixed #3784 and #3791.</p> </li> <li> <p>There was a problem with fulltext search, which could find a word with punctuation marks in natural language mode only, but not in boolean mode. Bugs fixed #258, #2501 (upstream #86164).</p> </li> <li> <p>Build errors were present on FreeBSD (caused by fixing the bug #255 in Percona Server for MySQL <code>5.6.38-83.0</code> and on MacOS (caused by fixing the bug #264 in Percona Server for MySQL <code>5.7.20-19</code>. Bugs fixed #2284 and #2286.</p> </li> <li> <p>A bunch of fixes were introduced to remove GCC 7 compilation warnings for the Percona Server for MySQL build. Bugs fixed #3780 (upstream #89420, #89421, and #89422).</p> </li> <li> <p>CMake error took place at compilation with bundled zlib. Bug fixed #302.</p> </li> <li> <p>A GCC 7 warning fix introduced a regression in Percona Server for MySQL that led to a wrong SQL query built to access the remote server when the Federated storage engine was used. Bug fixed #1134.</p> </li> <li> <p>It was possible to enable <code>encrypt_binlog</code> with no binary or relay logging enabled. Bug fixed #287.</p> </li> <li> <p>Long buffer wait times were occurring on busy servers in case of the <code>IMPORT TABLESPACE</code> command. Bug fixed #276.</p> </li> <li> <p>Server queries that contained JSON special characters and were logged by Audit Log Plugin in JSON format caused invalid output due to lack of escaping. Bug fixed #1115.</p> </li> <li> <p>Percona Server now uses Travis CI  for additional tests. Bug fixed #3777.</p> </li> </ul> <p>Other bugs fixed: #257, #264, #1090 (upstream #78048), #1109, #1127, #2204, #2414, #2415, #3767, #3794, and #3804 (upstream #89598).</p> <p>This release also contains fixes for the following CVE issues: CVE-2018-2565, CVE-2018-2573, CVE-2018-2576, CVE-2018-2583, CVE-2018-2586, CVE-2018-2590, CVE-2018-2612, CVE-2018-2600, CVE-2018-2622, CVE-2018-2640, CVE-2018-2645, CVE-2018-2646, CVE-2018-2647, CVE-2018-2665, CVE-2018-2667, CVE-2018-2668, CVE-2018-2696, CVE-2018-2703, CVE-2017-3737.</p>"},{"location":"release-notes/Percona-Server-5.7.21-20.html#myrocks-changes","title":"MyRocks Changes","text":"<ul> <li> <p>A new behavior makes Percona Server for MySQL fail to restart on detected data corruption; <code>rocksdb_allow_to_start_after_corruption</code> variable can be passed to <code>mysqld</code> as a command line parameter to switch off this restart failure.</p> </li> <li> <p>A new cmake option <code>ALLOW_NO_SSE42</code> was introduced to allow MyRocks build on hosts not supporting SSE 4.2 instructions set, which makes MyRocks usable without FastCRC32-capable hardware. Bug fixed MYR-207.</p> </li> <li> <p><code>rocksdb_bytes_per_sync</code> and <code>rocksdb_wal_bytes_per_sync</code> variables were turned into dynamic ones.</p> </li> <li> <p><code>rocksdb_flush_memtable_on_analyze</code> variable has been removed.</p> </li> <li> <p><code>rocksdb_concurrent_prepare</code> is now deprecated, as it has been renamed in upstream to <code>rocksdb_two_write_queues</code>.</p> </li> <li> <p><code>rocksdb_row_lock_deadlocks</code> and <code>rocksdb_row_lock_wait_timeouts</code> global status counters were added to track the number of deadlocks and the number of row lock wait timeouts.</p> </li> <li> <p>Creating a table with string indexed column to non-binary collation now generates a warning about using inefficient collation instead of an error. Bug fixed MYR-223.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.21-20.html#tokudb-changes","title":"TokuDB Changes","text":"<ul> <li> <p>A memory leak was fixed in the PerconaFT library, caused by not destroying PFS key objects on shutdown. Bug fixed TDB-98.</p> </li> <li> <p>A clang-format configuration was added to PerconaFT and TokuDB. Bug fixed TDB-104.</p> </li> <li> <p>A data race was fixed in minicron utility of the PerconaFT. Bug fixed TDB-107.</p> </li> <li> <p>Row count and cardinality decrease to zero took place after long-running <code>REPLACE</code> load.</p> </li> </ul> <p>Other bugs fixed: TDB-48, TDB-78, TDB-93, and TDB-99.</p>"},{"location":"release-notes/Percona-Server-5.7.21-21.html","title":"Percona Server 5.7.21-21 (2018-04-24)","text":"<p>Percona is glad to announce the release of Percona Server 5.7.21-21 on April 24, 2018. Downloads are available here and from the Percona Software Repositories.</p> <p>This release is based on MySQL 5.7.21 and includes all the bug fixes in it. Percona Server 5.7.21-21 is now the current GA (Generally Available) release in the 5.7 series. This version of Percona Server for MySQL marks the following encryption features, previously available as beta, as GA: Vault keyring plugin, encryption for InnoDB general tablespaces, and encryption for binary log files.</p> <p>All software developed by Percona is open-source and free.</p>"},{"location":"release-notes/Percona-Server-5.7.21-21.html#new-features","title":"New Features","text":"<ul> <li> <p>A new variable, <code>innodb_temp_tablespace_encrypt</code>, is introduced to turn encryption of temporary tablespace and temporary InnoDB file-per-table tablespaces on/off. Bug fixed #3821.</p> </li> <li> <p>A new variable, <code>innodb_encrypt_online_alter_logs</code>, simultaneously turns on encryption of files used by InnoDB for merge sort, online DDL logs, and temporary tables created by InnoDB for online DDL. Bug fixed #3819.</p> </li> <li> <p>A new variable, <code>innodb_encrypt_tables</code>, can be set to <code>ON</code>, making InnoDB tables encrypted by default, to <code>FORCE</code>, disabling creation of unencrypted tables, or <code>OFF</code>, restoring the like-before behavior. Bug fixed #1525.</p> </li> <li> <p>Query response time plugin now can be disabled at session level with use of a new variable, <code>query_response_time_session_stats</code>.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.21-21.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>Attempting to use a partially-installed query response time plugin could have caused server crash. Bug fixed #3959.</p> </li> <li> <p>There was a server crash caused by a materialized temporary table from semi-join optimization with key length larger than 1000 bytes. Bug fixed #296.</p> </li> <li> <p>A regression in the original 5.7 port was causing integer overflow with <code>thread_pool_stall_limit</code> variable values bigger than 2 seconds. Bug fixed #1095.</p> </li> <li> <p>A memory leak took place in Percona Server for MySQL when performance schema is used in conjunction with thread pooling. Bug fixed #1096.</p> </li> <li> <p>A code clean-up was done to fix compilation with clang, both general warnings (bug fixed #3814, upstream #89646) and clang 6 specific warnings and errors (bug fixed #3893, upstream #90111).</p> </li> <li> <p>Compilation warning was fixed for -DWITH_QUERY_RESPONSE_TIME=ON CMake compilation option, which makes QRT to be linked statically. Bug fixed #3841.</p> </li> <li> <p>Percona Server for MySQL returned empty result for <code>SELECT</code> query if number of connections exceeded 65535. Bug fixed #314 (upstream #89313).</p> </li> <li> <p>A clean-up in Percona Server for MySQL binlog-related code was made to avoid uninitialized memory comparison. Bug fixed #3925 (upstream #90238).</p> </li> <li> <p><code>mysqldump</code> utility with <code>--innodb-optimize-keys</code> option was incorrectly working with foreign keys on the same table, producing invalid SQL statements. Bugs fixed #1125 and #3863.</p> </li> <li> <p>A fix of the mysqld startup script failed to detect jemalloc library location for preloading, thus not starting on systemd based machines, introduced in Percona Server for MySQL <code>5.7.21-20</code>, was improved to take into account previously created configuration file. Bug fixed #3850.</p> </li> <li> <p>The possibility of a truncated bitmap file name was fixed in InnoDB logging subsystem. Bug fixed #3926.</p> </li> <li> <p>Temporary file I/O was not instrumented for Performance Schema. Bug fixed #3937 (upstream #90264).</p> </li> <li> <p>A crash in the unsafe query warning checks with views took place for <code>UPDATE</code> statement in case of statement binlogging format. Bug fixed #290.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.21-21.html#myrocks-changes","title":"MyRocks Changes","text":"<ul> <li> <p>A re-implemented variable, <code>rpl_skip_tx_api</code>, allows a user to turn on simple RocksDB write batches functionality, increasing replication performance by the transaction API skip. Bug fixed MYR-47.</p> </li> <li> <p>Decoding value-less padded varchar fields could under some circumstances cause assertion and/or data corruption. Bug fixed MYR-232.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.21-21.html#tokudb-changes","title":"TokuDB Changes","text":"<ul> <li> <p>Two new variables introduced to facilitate the TokuDB fast updates feature,<code>tokudb_enable_fast_update</code>,  and <code>tokudb_enable_fast_upsert</code>. Bugs fixed #63 and #148.</p> </li> <li> <p>A set of compilation fixes was introduced to make TokuDB successfully build in MySQL / Percona Server for MySQL 8.0. Bugs fixed #84, #85, #114, #115, #118, #128, #139, #141, and #172.</p> </li> <li> <p>Conditional compilation code dependent on version ID in the TokuDB tree was separated and arranged to specific version branches. Bugs fixed #133, #134, #135, and #136.</p> </li> <li> <p><code>ALTER TABLE ... COMMENT = ...</code> statement caused TokuDB to rebuild the whole table, which is not needed, as only FRM metadata should be changed. Bugs fixed #130 and #137.</p> </li> <li> <p>Data race on the cache table pair attributes was fixed. Bug fixed #109.</p> </li> </ul> <p>Other bugs fixed: #3793, #3812, #3813, #3815, #3818, #3835, #3875 (upstream #89916), #3843 (upstream #89822), #3848, #3856, #3887, MYR-160, MYR-245, #109, #111, #180, #181, #182, and #188.</p>"},{"location":"release-notes/Percona-Server-5.7.22-22.html","title":"Percona Server 5.7.22-22 (2018-05-31)","text":"<p>Percona is glad to announce the release of Percona Server 5.7.22-22 on May 31, 2018. Downloads are available here and from the Percona Software Repositories.</p> <p>This release is based on MySQL 5.7.22 and includes all the bug fixes in it. Percona Server for MySQL 5.7.22-22 is now the current GA (Generally Available) release in the 5.7 series.</p> <p>All software developed by Percona is open-source and free.</p>"},{"location":"release-notes/Percona-Server-5.7.22-22.html#new-features","title":"New Features","text":"<ul> <li>A new <code>--encrypt-tmp-files</code> option turns on encryption for the temporary files which  Percona Server for MySQL may create on disk for filesort, binary log transactional caches and Group Replication caches.</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.22-22.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>Executing the <code>SHOW GLOBAL STATUS</code> expression could cause \u201cdata drift\u201d on global status variables in case of a query rollback: the variable, being by its nature a counter and allowing only an increase, could return to its previous value. Bug fixed #3951 (upstream #90351).</p> </li> <li> <p>NUMA support was improved in Percona Server for MySQL, reverting upstream implementation back to the original one, due to the upstream variant being less effective in memory allocation. Now innodb_numa_interleave variable not only enables NUMA interleave memory policy for the InnoDB buffer pool allocation, but forces NUMA interleaved allocation at the buffer pool initialization time. Bug fixed #3967.</p> </li> <li> <p><code>audit_log_include_accounts</code> variable did not take effect if placed in <code>my.cnf</code> configuration file, while still working as intended if set dynamically. Bug fixed #3867.</p> </li> <li> <p>A <code>key_block_size</code> value was set automatically by the Improved MEMORY Storage Engine, which resulted in warnings when changing the engine type to InnoDB, and constantly growing <code>key_block_size</code> during alter operations. Bugs fixed #3936, #3940, and #3943.</p> </li> <li> <p>Fixes were introduced to remove GCC 8 compilation warnings for the Percona Server for MySQL build. Bug fixed #3950.</p> </li> <li> <p>An InnoDB Memcached Plugin code clean-up was backported from MySQL 8.0. Bug fixed #4506.</p> </li> <li> <p>Percona Server for MySQL could not be built with <code>-DWITH_LZ4=system</code> option on Ubuntu 14.04 (Trusty) because of too old LZ4 packages. Bug fixed #3842.</p> </li> <li> <p>A regression brought during TokuDB code clean-up in <code>5.7.21-21</code> was causing assertion in cases when the FT layer returns an error during an alter table operation. Bug fixed #4294.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.22-22.html#myrocks-changes-and-fixes","title":"MyRocks Changes and Fixes","text":"<ul> <li><code>UPDATE</code> statements were returning incorrect results because of not making a full table scan on tables with unique secondary index. Bug fixed #4495 (upstream Facebook/mysql-5.6#830).</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.22-22.html#other-bugs-fixed","title":"Other Bugs Fixed","text":"<ul> <li> <p>#4451 \u201cImplement better compression algo testing\u201d</p> </li> <li> <p>#4469 \u201cvariable use out of scope bug in get_last_key test detected by ASAN in clang 6\u201d</p> </li> <li> <p>#4470 \u201cthe cachetable-simple-pin-nonblocking-cheap test occasionally fails due to a locking conflict with the cachetable evictor\u201d</p> </li> <li> <p>#4488 \u201c`-Werror` is always disabled for `innodb_memcached`\u201c</p> </li> <li> <p>#1114 \u201cAssertion `inited == INDEX\u2019 failed\u201d</p> </li> <li> <p>#1130 \u201cRBR Replication with concurrent XA in READ-COMMITTED takes supremum pseudo-records and breaks replication\u201d</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.23-23.html","title":"Percona Server 5.7.23-23 (2018-09-12)","text":"<p>Percona is glad to announce the release of Percona Server 5.7.23-23 on September 12, 2018. Downloads are available here and from the Percona Software Repositories.</p> <p>This release is based on MySQL 5.7.23 and includes all the bug fixes in it. Percona Server for MySQL 5.7.23-23 is now the current GA (Generally Available) release in the 5.7 series.</p> <p>All software developed by Percona is open-source and free.</p>"},{"location":"release-notes/Percona-Server-5.7.23-23.html#new-features","title":"New Features","text":"<ul> <li> <p>The <code>max_binlog_files</code> variable is deprecated and replaced with the <code>binlog_space_limit</code> variable. The behavior of <code>binlog_space_limit</code> is consistent with the variable <code>relay-log-space-limit</code> used for relay logs; both variables have the same semantics. For more information, see #275.</p> </li> <li> <p>Starting with 5.7.23-23, it is possible to encrypt all data in the InnoDB system tablespace and in the parallel double write buffer. A new variable <code>innodb_sys_tablespace_encrypt</code> is introduced to encrypt the system tablespace. This feature is considered ALPHA quality. The encryption of the parallel double write buffer file is controlled by the variable <code>innodb_parallel_dblwr_encrypt</code>. Both variables are <code>OFF</code> by default. For more information, see #3822.</p> </li> <li> <p>Changing the <code>rocksdb_update_cf_options</code> value returns any warnings and errors to the client instead of printing them to the server error log. For more information, see #4258.</p> </li> <li> <p><code>rocksdb_number_stat_computers</code> and <code>rocksdb_rate_limit_delay_millis</code> variables have been removed. For more information, see #4780.</p> </li> <li> <p>A number of new variables were introduced for MyRocks: <code>rocksdb_rows_filtered</code> to show the number of rows filtered out for TTL in MyRocks tables, <code>rocksdb_bulk_load_allow_sk</code> to allow adding secondary keys using the bulk loading feature, <code>rocksdb_error_on_suboptimal_collation</code> toggling warning or error in case of an index creation on a char field where the table has a sub-optimal collation, <code>rocksdb_stats_recalc_rate</code> specifying the number of indexes to recalculate per second, <code>rocksdb_commit_time_batch_for_recovery</code> toggler of writing the commit time write batch into the database, and <code>rocksdb_write_policy</code> specifying when two-phase commit data are actually written into the database.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.23-23.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>The statement <code>SELECT...ORDER BY</code> produced inconsistent results with the <code>euckr</code> charset or <code>euckr_bin</code> collation. Bug fixed #4513 (upstream #91091).</p> </li> <li> <p>InnoDB statistics could incorrectly report zeros in the slow query log. Bug fixed #3828.</p> </li> <li> <p>With the FIPS mode enabled and performance_schema=off, the instance crashed when running the <code>CREATE VIEW</code> command. Bug fixed #3840.</p> </li> <li> <p>The soft limit of the core file size was set incorrectly starting with Percona Server for MySQL <code>5.7.21-20</code>. Bug fixed #4479.</p> </li> <li> <p>The option <code>innodb-optimize-keys</code> could fail when a dumped table has two columns such that the name of one of them contains the other as as a prefix and is defined with the AUTO_INCREMENT attribute. Bug fixed #4524.</p> </li> <li> <p>When <code>innodb_temp_tablespace_encrypt</code> was set to <code>ON</code> the <code>CREATE TABLE</code> command could ignore the value of the <code>ENCRYPTION</code> option. Bug fixed #4565.</p> </li> <li> <p>If <code>FLUSH STATUS</code> was run from a different session, a statement could be counted twice in <code>GLOBAL STATUS</code>. Bug fixed #4570 (upstream #91541).</p> </li> <li> <p>In some cases, it was not possible to set the <code>flush_caches</code> variable on systems that use systemd. Bug fixed #3796.</p> </li> <li> <p>A message in the MyRocks log file did not clearly inform whether fast CRC32 was supported. Bug fixed #3988.</p> </li> <li> <p><code>mysqld</code> could not be started on Ubuntu if the database recovery had taken longer than ten minutes. Bug fixed #4546 (upstream #91423).</p> </li> <li> <p>The ALTER TABLE command was slow when the number of dirty pages was high. Bug fixed #3702.</p> </li> <li> <p>Setting the global variable <code>version_suffix</code> to NULL could lead to a server crash. Bug fixed #4785.</p> </li> <li> <p>When more space was added to the data partition after the error that the disk partition was full, MyRocks could ignore data update commands. Bug fixed #4706.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.23-23.html#other-bugs-fixed","title":"Other Bugs Fixed","text":"<ul> <li> <p>#4620 \u201cEnable encryption of temporary tablespace from foreground thread\u201d</p> </li> <li> <p>#4727 \u201cintrinsic temp table behaviour shouldn\u2019t depend on innodb_encrypt_tables\u201d</p> </li> <li> <p>#4046 \u201cShip assert failure: \u2018res == 0\u2019 (bulk loader)\u201d</p> </li> <li> <p>#3851 \u201cPercona Ver 5.6.39-83.1 Failing assertion: sym_node-&gt;table != NULL\u201d</p> </li> <li> <p>#4533 \u201caudit_log MTR tests should refer to include files without parent directories\u201d</p> </li> <li> <p>#4619 \u201cmain.flush_read_lock fails with timeout in wait_condition.inc.\u201d</p> </li> <li> <p>#4561 \u201cRead after free at Binlog_crypt_data::load_latest_binlog_key()\u201d</p> </li> <li> <p>#4587 \u201cROCKSDB_INCLUDE_RFR macro in wrong file\u201d</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.23-24.html","title":"Percona Server 5.7.23-24 (2018-11-09)","text":"<p>Percona announces the release of Percona Server for MySQL 5.7.23-24 on November 9, 2018 (downloads are available here and from the Percona Software Repositories). This release merges changes of MySQL 5.7.23, including all the bug fixes in it. Percona Server for MySQL 5.7.23-24 is now the current GA release in the 5.7 series. All of Percona\u2019s software is open-source and free.</p> <p>This release introduces InnoDB encryption improvements and merges upstream MyRocks changes. Also, the usage of column families in MyRocks has been improved. The InnoDB encryption improvements are in Alpha quality and are not recommended to be used in production.</p>"},{"location":"release-notes/Percona-Server-5.7.23-24.html#new-features","title":"New Features","text":"<ul> <li> <p>#4905: Upstream MyRocks changes have been merged up to prod201810 tag</p> </li> <li> <p>#4976: InnoDB Undo Log Encryption has been implemented</p> </li> <li> <p>#4946: Add the <code>rocksdb_no_create_column_family</code> option to prevent the implicit creation of column families in MyRocks</p> </li> <li> <p>#4556: InnoDB Redo log has been implemented</p> </li> <li> <p>#3839: InnoDB Data Scrubbing has been implemented</p> </li> <li> <p>#3834: InnoDB Log Scrubbing has been implemented</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.23-24.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>#4723: <code>PURGE CHANGED_PAGE_BITMAPS</code> did not work when <code>innodb_data_home_dir</code> was used</p> </li> <li> <p>#4937: <code>rocksdb_update_cf_options</code> was ignored when specified in <code>my.cnf</code> or on command line</p> </li> <li> <p>#1107: The binlog could be corrupted when tmpdir got full</p> </li> <li> <p>#4834: The encrypted system tablespace could have an empty uuid</p> </li> <li> <p>#3906: The server instance could crash when running the <code>ALTER</code> statement</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.23-24.html#other-bugs-fixed","title":"Other bugs fixed","text":"<ul> <li> <p>#4106: \u201cAssertion <code>log.getting_synced</code> failed in <code>rocksdb::DBImpl::MarkLogsSynced(uint64_t, bool, const rocksdb::Status&amp;)</code>\u201d</p> </li> <li> <p>#4930: \u201cmain.percona_log_slow_innodb: Result content mismatch\u201d</p> </li> <li> <p>#4811: \u201c5.7 Merge and fixup for old DB-937 introduces possible regression\u201d</p> </li> <li> <p>#4705: \u201ccrash on snapshot size check in RocksDB\u201d</p> </li> </ul> <p>Find the release notes for Percona Server for MySQL 5.7.23-24 in our online documentation. Report bugs in the Jira bug tracker.</p>"},{"location":"release-notes/Percona-Server-5.7.23-25.html","title":"Percona Server 5.7.23-25 (2018-11-21)","text":"<p>Percona announces the release of Percona Server for MySQL 5.7.23-25 on November 21, 2018 (downloads are available here and from the Percona Software Repositories). This release merges changes of MySQL 5.7.23, including all the bug fixes in it. Percona Server for MySQL 5.7.23-25 is now the current GA release in the 5.7 series. All of Percona\u2019s software is open-source and free.</p> <p>This release fixes a critical bug in a RocksDB submodule.</p>"},{"location":"release-notes/Percona-Server-5.7.23-25.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li>#5049: A severe memory leak regression in the RocksDB Block Cache</li> </ul> <p>Find the release notes for Percona Server for MySQL 5.7.23-24 in our online documentation. Report bugs in the Jira bug tracker.</p>"},{"location":"release-notes/Percona-Server-5.7.24-26.html","title":"Percona Server 5.7.24-26 (2018-12-04)","text":"<p>Percona announces the release of Percona Server for MySQL 5.7.24-26 on December 4, 2018 (downloads are available here and from the Percona Software Repositories). This release merges changes of MySQL 5.7.24, including all the bug fixes in it. Percona Server for MySQL 5.7.24-26 is now the current GA release in the 5.7 series. All of Percona\u2019s software is open-source and free.</p> <p>This release includes fixes to the following upstream CVEs (Common Vulnerabilities and Exposures): CVE-2016-9843, CVE-2018-3155, CVE-2018-3143, CVE-2018-3156, CVE-2018-3251, CVE-2018-3133, CVE-2018-3144, CVE-2018-3185, CVE-2018-3247, CVE-2018-3187, CVE-2018-3174, CVE-2018-3171. For more information, see Oracle Critical Patch Update Advisory - October 2018 https://www.oracle.com/technetwork/security-advisory/cpuoct2018-4428296.html.</p>"},{"location":"release-notes/Percona-Server-5.7.24-26.html#improvements","title":"Improvements","text":"<ul> <li>#4790: Improve user statistics accuracy</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.24-26.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>Slave replication could break if upstream bug #74145 (FLUSH LOGS improperly disables the logging if the log file cannot be accessed) occurred in master. Bug fixed #1017 (Upstream #83232).</p> </li> <li> <p>Setting the <code>tokudb_last_lock_timeout</code> variable via the command line could cause the server to stop working when the actual timeout took place. Bug fixed #4943.</p> </li> <li> <p>Dropping TokuDB table with non-alphanumeric characters could lead to a crash. Bug fixed #4979.</p> </li> <li> <p>When using MyRocks storage engine, the server could crash after running <code>ALTER TABLE DROP INDEX</code> on a slave. Bug fixed #4744.</p> </li> <li> <p>The audit log could be corrupted when the <code>audit_log_rotations</code> variable was changed at runtime. Bug fixed #4950.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.24-26.html#other-bugs-fixed","title":"Other Bugs Fixed","text":"<ul> <li> <p>#4781: sql_yacc.yy uses SQLCOM_SELECT instead of SQLCOM_SHOW_XXXX_STATS</p> </li> <li> <p>#4881: Add LLVM/clang 7 to Travis-CI</p> </li> <li> <p>#4825: Backport MTR fixes from 8.0</p> </li> <li> <p>#4998: Valgrind: compilation fails with: writing to \u2018struct buf_buddy_free_t\u2019 with no trivial copy-assignment</p> </li> <li> <p>#4980: Valgrind: Syscall param write(buf) points to uninitialised byte(s): Event_encrypter::encrypt_and_write()</p> </li> <li> <p>#4982: Valgrind: Syscall param io_submit(PWRITE) points to uninitialised byte(s): buf_dblwr_write_block_to_datafile()</p> </li> <li> <p>#4983: Valgrind: Syscall param io_submit(PWRITE) points to uninitialised byte(s): buf_flush_write_block_low()</p> </li> <li> <p>#4951: Many libc-related Valgrind errors on CentOS7</p> </li> <li> <p>#5012: Valgrind: misused UNIV_MEM_ALLOC after ut_zalloc_nokey</p> </li> <li> <p>#4908: UBSan and valgrind errors with encrypted temporary files</p> </li> <li> <p>#4532: Replace obsolete HAVE_purify with HAVE_VALGRIND in ha_rocksdb.cc</p> </li> <li> <p>#4955: Backport mysqld fixes for valgrind warnings from 8.0</p> </li> <li> <p>#4529: MTR: index_merge_rocksdb2 inadvertently tests InnoDB instead of MyRocks</p> </li> <li> <p>#5056: handle_fatal_signal (sig=11) in ha_tokudb::write_row</p> </li> <li> <p>#5084: innodb_buffer_pool_size is an uninitialized variable</p> </li> <li> <p>#4836: Missing PFS signed variable aggregation</p> </li> <li> <p>#5033: rocksdb.show_engine: Result content mismatch</p> </li> <li> <p>#5034: rocksdb.rocksdb: Result content mismatch</p> </li> <li> <p>#5035: rocksdb.show_table_status: 1051: Unknown table \u2018db_new\u2019</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.24-27.html","title":"Percona Server for MySQL 5.7.24-27 (2018-12-18)","text":"<p>Percona is glad to announce the release of Percona Server for MySQL 5.7.24-27 on December 18, 2018. Downloads are available here and from the Percona Software Repositories.</p> <p>This release is based on MySQL 5.7.24 and includes all the bug fixes in it. Percona Server for MySQL 5.7.24-27 is now the current GA (Generally Available) release in the 5.7 series.</p> <p>All software developed by Percona is open-source and free.</p> <p>Note</p> <p>If you\u2019re currently using Percona Server for MySQL 5.7, Percona recommends upgrading to this version of 5.7 prior to upgrading to Percona Server for MySQL 8.0.</p>"},{"location":"release-notes/Percona-Server-5.7.24-27.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li>When uninstalling Percona Server for MySQL packages on CentOS 7 default configuration file <code>my.cnf</code> would get removed as well. This fix makes the backup of the configuration file instead of removing it. Bug fixed #5092.</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.25-28.html","title":"Percona Server for MySQL 5.7.25-28 (2019-02-18)","text":"<p>Percona is glad to announce the release of Percona Server for MySQL 5.7.25-28 on February 18, 2019. Downloads are available here and from the Percona Software Repositories.</p> <p>This release is based on MySQL 5.7.25 and includes all the bug fixes in it. Percona Server for MySQL 5.7.25-28 is now the current GA (Generally Available) release in the 5.7 series.</p> <p>All software developed by Percona is open-source and free.</p> <p>In this release, Percona Server for MySQL introduces the variable <code>binlog_skip_flush_commands</code>. This variable controls whether or not <code>FLUSH</code> commands are written to the binary log. Setting this variable to ON can help avoid problems in replication. For more information, see Writing FLUSH Commands to the Binary Log.</p> <p>Note</p> <p>If you\u2019re currently using Percona Server for MySQL 5.7, Percona recommends upgrading to this version of 5.7 prior to upgrading to Percona Server for MySQL 8.0.</p>"},{"location":"release-notes/Percona-Server-5.7.25-28.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>FLUSH commands written to the binary log could cause errors in case of replication. Bug fixed #1827: (upstream <code>88720</code>).</p> </li> <li> <p>Running LOCK TABLES FOR BACKUP followed by STOP SLAVE SQL_THREAD could block replication preventing it from being restarted normally. Bug fixed #4758.</p> </li> <li> <p>The <code>ACCESS_DENIED</code> field of the information_schema.user_statistics table was not updated correctly. Bug fixed #3956.</p> </li> <li> <p>MySQL could report that the maximum number of connections was exceeded with too many connections being in the CLOSE_WAIT state. Bug fixed #4716 (upstream #92108)</p> </li> <li> <p>Wrong query results could be received in semi-join sub queries with materialization-scan that allowed inner tables of different semi-join nests to interleave. Bug fixed #4907 (upstream bug #92809).</p> </li> <li> <p>In some cases, the server using the MyRocks storage engine could crash when TTL (Time to Live) was defined on a table. Bug fixed #4911.</p> </li> <li> <p>Running the SELECT statement with the ORDER BY and LIMIT clauses could result in a less than optimal performance. Bug fixed #4949 (upstream #92850)</p> </li> <li> <p>There was a typo in <code>mysqld_safe.sh</code>: trottling was replaced with throttling. Bug fixed #240. Thanks to Michael Coburn for the patch.</p> </li> <li> <p>MyRocks could crash while running <code>START TRANSACTION WITH CONSISTENT SNAPSHOT</code> if other transactions were in specific states. Bug fixed #4705.</p> </li> <li> <p>In some cases, <code>mysqld</code> could crash when inserting data into a database the name of which contained special characters (CVE-2018-20324). Bug fixed #5158.</p> </li> <li> <p>MyRocks incorrectly processed transactions in which multiple statements had to be rolled back.  Bug fixed #5219.</p> </li> <li> <p>In some cases, the MyRocks storage engine could crash without triggering the crash recovery. Bug fixed #5366.</p> </li> <li> <p>When bootstrapped with undo or redo log encryption enabled on a very fast storage, the server could fail to start. Bug fixed #4958.</p> </li> <li> <p>Some fields in the output of <code>SHOW USER_STATISTICS</code> command did not contain correct information. Bug fixed #4996.</p> </li> </ul> <p>Other bugs fixed: #2455, #4791, #4855, #5268.</p> <p>This release also contains fixes for the following CVE issues: CVE-2019-2534, CVE-2019-2529, CVE-2019-2482, CVE-2019-2434.</p>"},{"location":"release-notes/Percona-Server-5.7.26-29.html","title":"Percona Server for MySQL 5.7.26-29 (2019-05-27)","text":"<p>Percona is glad to announce the release of Percona Server for MySQL 5.7.26-29 on May 27, 2019. Downloads are available here and from the Percona Software Repositories.</p> <p>This release is based on MySQL 5.7.26 and includes all the bug fixes in it. Percona Server for MySQL 5.7.26-29 is now the current GA (Generally Available) release in the 5.7 series.</p>"},{"location":"release-notes/Percona-Server-5.7.26-29.html#new-features","title":"New Features","text":"<ul> <li>New variable <code>Audit_log_buffer_size_overflow</code> status variable has been implemented to track when an Audit Log Plugin entry was either dropped or written directly to the file due to its size being bigger than <code>audit_log_buffer_size</code> variable.</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.26-29.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>TokuDB storage engine would assert on load when used with jemalloc 5.x. Bug fixed #5406.</p> </li> <li> <p>A read-write workload on compressed InnoDB tables could cause an assertion error. Bug fixed #3581.</p> </li> <li> <p>Using TokuDB or MyRocks native partitioning and <code>index_merge</code> access method could lead to a server crash. Bugs fixed #5206, #5562.</p> </li> <li> <p>A stack buffer overrun could happen if the redo log encryption with key rotation was enabled. Bug fixed #5305.</p> </li> <li> <p>TokuDB and MyRocks native partitioning handler objects were allocated from a wrong memory allocator. Memory was released only on shutdown and concurrent access to global memory allocator caused memory corruptions and therefore crashes. Bugs fixed #5508, #5525.</p> </li> <li> <p>Enabling redo log encryption resulted in redo log being written unencrypted. Bug fixed #5547.</p> </li> <li> <p>If there are multiple row versions in InnoDB, reading one row from PK may have O(N) complexity and reading from secondary keys may have O(N^2) complexity. Bugs fixed #4712, #5450 (upstream #84958).</p> </li> <li> <p>Setting the <code>log_slow_verbosity</code> to include <code>innodb</code> value and enabling the <code>slow_query_log</code> could lead to a server crash. Bug fixed #4933.</p> </li> <li> <p>The page cleaner could sleep for long time when the system clock was adjusted to an earlier point in time. Bug fixed #5221 (upstream #93708).</p> </li> <li> <p>Executing <code>SHOW BINLOG EVENT</code> from an invalid position could result in a segmentation fault on 32bit machines. Bug fixed #5243.</p> </li> <li> <p><code>BLOB</code> entries in the binary log could become corrupted in case when a database with <code>Blackhole</code> tables served as an intermediate binary log server in a replication chain. Bug fixed #5353 (upstream #93917).</p> </li> <li> <p>When Audit Log Plugin was enabled, the server could use a lot of memory when handling large queries.  Bug fixed #5395.</p> </li> <li> <p>XtraDB changed page tracking was missing pages changed by the in-place DDL. Bug fixed #5447.</p> </li> <li> <p>The <code>innodb_encrypt_tables</code> variable accepted <code>FORCE</code> option only inside quotes as a string. Bug fixed #5538.</p> </li> <li> <p>Enabling redo log encryption and XtraDB changed page tracking together would result in the error log flooded with decryption errors. Bug fixed #5541.</p> </li> <li> <p>System keyring keys initialization wasn\u2019t thread safe. Bugs fixed #5554.</p> </li> <li> <p>when using the Docker image, if the root passwords set in the mounted <code>.cnf</code> file and the one specified with <code>MYSQL_ROOT_PASSWORD</code> are different, password from the <code>MYSQL_ROOT_PASSWORD</code> will be used. Bug fixed #5573.</p> </li> <li> <p>Long running <code>ALTER TABLE ADD INDEX</code> could cause a <code>semaphore wait &gt; 600</code> assertion. Bug fixed #3410 (upstream #82940).</p> </li> </ul> <p>Other bugs fixed: #5007 (upstream #93164), #5018, #5561, #5570, #5578, #5610, #5441, and #5442.</p> <p>This release also contains the fixes for the following security issues: CVE-2019-2632, CVE-2019-1559, CVE-2019-2628, CVE-2019-2581, CVE-2019-2683, CVE-2019-2592, CVE-2019-262, and CVE-2019-2614.</p>"},{"location":"release-notes/Percona-Server-5.7.27-30.html","title":"Percona Server for MySQL 5.7.27-30 (2019-08-22)","text":"<p>Percona is glad to announce the release of Percona Server for MySQL 5.7.27-30 on August 22, 2019. Downloads are available here and from the Percona Software Repositories.</p> <p>This release is based on MySQL 5.7.27 and includes all the bug fixes in it. Percona Server for MySQL 5.7.27-30 is now the current GA (Generally Available) release in the 5.7 series.</p> <p>All software developed by Percona is open-source and free.</p> <p>Note</p> <p>If you\u2019re currently using Percona Server for MySQL 5.7, Percona recommends upgrading to this version of 5.7 prior to upgrading to Percona Server for MySQL 8.0.</p>"},{"location":"release-notes/Percona-Server-5.7.27-30.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>Parallel doublewrite buffer writes must crash the server on an I/O error occurs. Bug fixed #5678.</p> </li> <li> <p>On a server with two million or more tables using foreign keys and AUTOINC columns, the shutdown may take a measurable length of time. Bug fixed #5639. (Upstream #95895)</p> </li> <li> <p>If large pages are enabled on the MySQL side, the maximum size for <code>innodb_buffer_pool_chunk_size</code> is effectively limited to 4GB. Bug fixed #5517. (Upstream #94747)</p> </li> <li> <p>The TokuDB hot backup library continually dumps TRACE information to the Server error log. The user cannot enable or disable the dump of this information. Bug fixed #4850.</p> </li> <li> <p>The TokuDBBackupPlugin is optional at cmake time. Bug fixed #5748.</p> </li> <li> <p>A multi-table <code>DELETE</code> with a foreign key breaks replication. Bug fixed #3845.</p> </li> <li> <p>A <code>TRUNCATE</code> with any table and interfacing with Adaptive Hash Index (AHI) can cause server stalls due to the interaction with AHI, whether the AHI is enabled or not. Bug fixed #5576. (Upstream #94610)</p> </li> <li> <p>In specific configurations and with <code>log_slow_verbosity</code> set to log InnoDB statistics, memory usage increases while running a stored procedure.  Bug fixed #5581.</p> </li> <li> <p>Thread Pool functionality to track network I/O was disabled.  Bug fixed #5723.</p> </li> <li> <p>When Adaptive Hash Index (AHI) is enabled or disabled, there is an AHI overhead during DDL operations. Bug fixed #5747.</p> </li> <li> <p>An instance started with the default values but setting the redo-log to encrypt without specifying the keyring plugin parameters does not fail or throw an error. Bug fixed #5476.</p> </li> <li> <p>Setting the encryption to <code>ON</code> for the system tablespace generates the encryption key and encrypts system temporary tablespace pages. Resetting encryption to <code>OFF</code> , all subsequent pages are written to the temporary tablespace without encryption. To allow any encrypted tables to be decrypted, the generated keys are not erased. Modifying the <code>innodb_temp_tablespace_encrypt</code> does not affect file-per-table temporary tables. This type of table is encrypted if <code>ENCRYPTION</code> =\u2019Y\u2019 is set during the table creation. Bug fixed #5736.</p> </li> <li> <p>After resetting the  <code>innodb_temp_tablespace_encrypt</code> to <code>OFF</code> during runtime, the subsequent file-per-table temporary tables continue to be encrypted. Bug fixed #5734.</p> </li> </ul> <p>Other bugs fixed: #5752, #5749, #5746, #5744, #5743, #5742, #5740, #5695, #5681, #5669, #5645, #5638, #5593, #5532, #5790, #5812, #3970, #5696, #5689, #5146, #5715, #5791, #5662, #5420, #5149, #5686, #5688, #5697, #5716, #5725, #5773, #5775, #5820, and #5839.</p>"},{"location":"release-notes/Percona-Server-5.7.28-31.html","title":"Percona Server for MySQL 5.7.28-31 (2019-11-13)","text":"<p>Percona is glad to announce the release of Percona Server for MySQL 5.7.28-31 on November 13, 2019. Downloads are available here and from the Percona Software Repositories.</p> <p>This release is based on MySQL 5.7.28 and includes all the bug fixes in it. Percona Server for MySQL 5.7.28-31 is now the current GA (Generally Available) release in the 5.7 series.</p> <p>All software developed by Percona is open-source and free.</p> <p>Note</p> <p>If you\u2019re currently using Percona Server for MySQL 5.7, Percona recommends upgrading to this version of 5.7 prior to upgrading to Percona Server for MySQL 8.0.</p>"},{"location":"release-notes/Percona-Server-5.7.28-31.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>When using skip-innodb_doublewrite in my.cnf, a parallel doublewrite buffer is still created. Bugs fixed #3411.</p> </li> <li> <p>During a binlogging replication event, if the master crashes after the multi-threaded slave has begun copying to the slave\u2019s relay log and before the process has completed, a STOP SLAVE on the slave takes longer than expected. Bug fixed #5824.</p> </li> <li> <p>If pam_krb5 is configured to allow the user to change their password, and the password expired, the server crashed after receiving the new password. Bug fixed #6023.</p> </li> </ul> <p>Other bugs fixed: #5859, #5910, #5966, #4784, #5216, #5327, #5584, #5642, #5659, #5754, #5761, #5797, #5875, #5933, #5941, #5997, #6050, #6052, #3345, and #5585</p>"},{"location":"release-notes/Percona-Server-5.7.28-31.html#known-issues","title":"Known Issues","text":"<ul> <li>#5783: The length of time and resources required for a MySQL query execution increased with a large number of table partitions. Limiting the Estimation of Records in a Query describes the experimental options added to prevent index scans on the partitions and return a specified number of values.</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.29-32.html","title":"Percona Server for MySQL 5.7.29-32 (2020-02-05)","text":"<ul> <li>Installation:  Installing Percona Server for MySQL</li> </ul> <p>Percona Server for MySQL 5.7.29-32 includes all the features and bug fixes available in MySQL 5.7.29 Community Edition in addition to enterprise-grade features developed by Percona.</p>"},{"location":"release-notes/Percona-Server-5.7.29-32.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>PS-1469: The Memory storage engine detected an incorrect \u201cis full\u201d condition when the space contained reusable memory chunks that could be reused.</p> </li> <li> <p>PS-6113: If <code>ANALYZE TABLE</code> with persistent statistics ran more than 600 seconds the execution of a diagnostic query may cause a server exit. (Upstream #97828)</p> </li> <li> <p>PS-5813: To set the <code>slow_query_log_use_global_control</code> to \u201cnone\u201d could cause an error.</p> </li> <li> <p>PS-6150: The execution of SHOW ENGINE INNODB STATUS to show locked mutexes could cause a server exit.</p> </li> <li> <p>PS-6750: The installation of client packages could cause a file conflict in Red Hat Enterprise Linux 8.</p> </li> <li> <p>PS-5940: When a temporary table was dropped, the server exited. (Upstream #96766)</p> </li> <li> <p>PS-5675: Concurrent INSERT \u2026 ON DUPLICATE KEY UPDATE statements could cause a failure with a unique index violation. (Upstream #96578)</p> </li> <li> <p>PS-5421: MyRocks: Corrected documentation for <code>rocksdb_db_write_buffer_size</code>.</p> </li> <li> <p>PS-4794: Documented that using ps-admin to enable MyRocks does not disable Transparent Huge Pages.</p> </li> <li> <p>PS-6093: The execution of SHOW ENGINE INNODB STATUS to show locked mutexes with simultaneous access to a compressed table could cause a server exit.</p> </li> <li> <p>PS-6148: If ANALYZE TABLE with transient statistics ran more than 600 seconds the execution of a diagnostic query may cause a server exit. (Upstream #97828)</p> </li> <li> <p>PS-6125: MyRocks: To set <code>rocksdb_update_cf_options</code> with a non-existant column family created a partially-defined column family which could cause a server exit.</p> </li> <li> <p>PS-6123: A Debian/Ubuntu init script used an incorrect comparison which could cause the service command to return before the server start.</p> </li> <li> <p>PS-5956: Root session could kill Utility user session.</p> </li> <li> <p>PS-5952: Utility user was visible in performance_schema.threads.</p> </li> <li> <p>PS-5843: A memory leak could occur after \u201cgroup_replication.gr_majority_loss_restart\u201d. (Upstream #96471)</p> </li> <li> <p>PS-5325: Conditional jump or move depended on uninitialized value on innodb_zip.wl5522_zip or innodb.alter_missing_tablespace.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.30-33.html","title":"Percona Server for MySQL 5.7.30-33 (2020-05-20)","text":"<ul> <li>Installation Installing Percona Server for MySQL</li> </ul> <p>Percona Server for MySQL 5.7.30-33 includes all the features and bug fixes available in MySQL 5.7.30 Community Edition in addition to enterprise-grade features developed by Percona.</p> <p>Merged MyRocks/RocksDB up to Facebook MySQL production tag fb-prod201907.</p>"},{"location":"release-notes/Percona-Server-5.7.30-33.html#new-features","title":"New Features","text":"<ul> <li> <p>PS-6951: Document new RocksDB variables: rocksdb_delete_cf, rocksdb_enable_iterate_bounds, and rocksdb_enable_remove_orphaned_dropped_cfs</p> </li> <li> <p>PS-4464: Expose the last global transaction identifier (GTID) executed for a CONSISTENT SNAPSHOT.</p> </li> <li> <p>PS-6926: Document RocksDB variables: rocksdb_table_stats_recalc_threshold_pct, rocksdb_table_stats_recalc_threshold_count, rocksdb_table_stats_background_thread_nice_value, rocksdb_table_stats_max_num_rows_scanned, rocksdb_table_stats_use_table_scan. rocksdb_table_stats_background_thread_nice_value,  rocksdb_table_stats_max_num_rows_scanned,  rocksdb_table_stats_use_table_scan, and rocksdb_trace_block_cache_access.</p> </li> <li> <p>PS-6901: Document RocksDB variable: rocksdb_read_free_rpl.</p> </li> <li> <p>PS-6890: Document RocksDB variable: rocksdb_blind_delete_primary_key.</p> </li> <li> <p>PS-6885: Document the new variable rocksdb_rollback_on_timeout which allows the rollback of an entire transaction on timeout.</p> </li> <li> <p>PS-6891: Document RocksDB variable: rocksdb_master_skip_tx_api.</p> </li> <li> <p>PS-6886: Document variable rocksdb_cache_dump which includes RocksDB block cache content in a core dump.</p> </li> <li> <p>PS-6910: Document RocksDB variable: rocksdb_stats_level.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.30-33.html#improvements","title":"Improvements","text":"<ul> <li>PS-6984: Update the zstd submodule to v1.4.4.</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.30-33.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>PS-6979: Modify the processing to call clean up functions to remove CREATE USER statement from the processlist after the statement has completed (Upstream #99200)</p> </li> <li> <p>PS-6860: Merge innodb_buffer_pool_pages_LRU_flushed into buf_get_total_stat()</p> </li> <li> <p>PS-6811: Correct service failure of asserting ACL_PROXY_USER when skip-name-resolve=1 and there is a Proxy user (Upstream #98908)</p> </li> <li> <p>PS-6112: Correct Binlog_snapshot_gtid inconsistency when mysqldump was used with \u2013single-transaction.</p> </li> <li> <p>PS-6945: Correct tokubackup plugin process exported API to allow large file backups.</p> </li> <li> <p>PS-6856: Correct binlogs corruptions in PS 5.7.28 and 5.7.29 (Upstream #97531)</p> </li> <li> <p>PS-6946: Correct tokubackup processing to free memory use from the address and thread sanitizers</p> </li> <li> <p>PS-5893: Add support for running multiple instances with systemD on Debian.</p> </li> <li> <p>PS-5620: Modify Docker image to support supplying custom TLS certificates</p> </li> <li> <p>PS-4573: Implement use of a single config file - mysqld.cnf file.</p> </li> <li> <p>PS-7041: Correct Compilation error when -DWITH_EDITLINE=bundled is used</p> </li> <li> <p>PS-7020: Modify MTR tests for Ubuntu 20.04 to include python2 (python 2.6 or higher) and python3</p> </li> <li> <p>PS-6974: Correct instability in the rocksdb.drop_cf_* tests</p> </li> <li> <p>PS-6969: Correct instability in the rocksdb.index_stats_large_table</p> </li> <li> <p>PS-6954: Correct tokudb-backup-plugin to avoid collision between -std=c++11 and -std=gnu++03.</p> </li> <li> <p>PS-6925: Correct mismatched default socket values for mysqld and mysqld_safe</p> </li> <li> <p>PS-6899: Correct main.events_bugs and main.events_1 to interpret date 01-01-2020 properly (Upstream #98860)</p> </li> <li> <p>PS-6796: Correct instability in percona_changed_page_bmp_shutdown_thread</p> </li> <li> <p>PS-6773: Initialize values in sha256_password_authenticate (Upstream #98223)</p> </li> <li> <p>PS-5844: Fix a memory leak after \u2018innodb.alter_crash\u2019 in \u2018prepare_inplace_alter_table_dict()\u2019 (Upstream #96472)</p> </li> <li> <p>PS-5735: Correct 5.7 package to install the charsets on CentOS 7</p> </li> <li> <p>PS-4757: Remove CHECK_IF_CURL_DEPENDS_ON_RTMP to build keyring_vault for unconditional test</p> </li> <li> <p>PS-4649: Document PerconaFT in TokuDB which is fractal tree indexing to enhance the B-tree data structure</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.31-34.html","title":"Percona Server for MySQL 5.7.31-34 (2020-08-24)","text":"<ul> <li>Installation Installing Percona Server for MySQL</li> </ul> <p>Percona Server for MySQL 5.7.31-34 includes all the features and bug fixes available in MySQL 5.7.31 Community Edition in addition to enterprise-grade features developed by Percona.</p>"},{"location":"release-notes/Percona-Server-5.7.31-34.html#new-features","title":"New Features","text":"<ul> <li>PS-7128: Document RocksDB variables: <code>rocksdb_max_background_compactions</code>, <code>rocksdb_max_background_flushes</code>, and <code>rocksdb_max_bottom_pri_background_compactions</code></li> </ul>"},{"location":"release-notes/Percona-Server-5.7.31-34.html#improvements","title":"Improvements","text":"<ul> <li> <p>PS-7132: Make default value of <code>rocksdb_wal_recovery_mode</code> compatible with InnoDB</p> </li> <li> <p>PS-7199: Add Coredumper functionality</p> </li> <li> <p>PS-7114: Enhance crash artifacts (core dumps and stack traces) to provide additional information to the operator</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.31-34.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>PS-7203: Fixed audit plugin memory leak on replicas when opening tables</p> </li> <li> <p>PS-7043: Correct constant equality expression is used in LEFT JOIN condition by setting the \u2018const_table\u2019 flag together with setting the row as a NULL-row. (Upstream #99499)</p> </li> <li> <p>PS-7212: Modify processing to binary compare in order to do native JSON comparison (Upstream #100307)</p> </li> <li> <p>PS-7076: Modify to not update Cardinality after setting <code>tokudb_cardinality_scale_percent</code></p> </li> <li> <p>PS-7025: Fix reading ahead of insert buffer pages by dispatching of buffered AIO transfers (Upstream #100086)</p> </li> <li> <p>PS-7010: Modify to Lock buffer blocks before sanity check in btr_cur_latch_leaves</p> </li> <li> <p>PS-6995: Introduce a new optimizer switch to allow the user to reduce the cost of a range scan to determine best execution plan for Primary Key lookup</p> </li> <li> <p>PS-5978: Remove unneeded check of variable to allow mysqld_safe support \u2013numa-interleave (Thanks to user springlin for reporting this issue)</p> </li> <li> <p>PS-7220: Fix activity counter update in purge coordinator and workers</p> </li> <li> <p>PS-7234: Modify PS minimal tarballs to remove COPYING.AGPLv3</p> </li> <li> <p>PS-7204: Add checks to linkingscript to correct failures in patchelf</p> </li> <li> <p>PS-7075: Provide binary tarball with shared libs and glibc suffix</p> </li> <li> <p>PS-7062: Modify ALTER INSTANCE ROTATE INNODB MASTER KEY to skip writing of redo for compressed encrypted temporary table.</p> </li> <li> <p>PS-5263: Update handle_binlog_flush_or_sync_error() to set my_ok(thd) after thd-&gt;clear_error() to correct assert in THD::send_statement_status (Upstream #93770)</p> </li> <li> <p>PS-4530: Add documentation that <code>ps-admin</code> removes jemalloc and THP settings on TokuDB uninstall</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.32-35.html","title":"Percona Server for MySQL 5.7.32-35 (2020-11-24)","text":"<ul> <li>Installation Installing Percona Server for MySQL</li> </ul> <p>Percona Server for MySQL 5.7.32-35 includes all the features and bug fixes available in MySQL 5.7.32 Community Edition in addition to enterprise-grade features developed by Percona.</p>"},{"location":"release-notes/Percona-Server-5.7.32-35.html#new-features","title":"New Features","text":"<ul> <li>PS-7238: Backport Data Masking plugin to 5.7 (Thanks to user Surenda Kumar Gupta for reporting this issue)</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.32-35.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>PS-7346: Correct the buffer calculation for the audit plugin used when large queries are executed(PS-5395).</p> </li> <li> <p>PS-7232: Modify Multithreaded Replica to correct the exhausted slave_transaction_retries when replica has slave_preserve_commit_order enabled (Upstream #99440)</p> </li> <li> <p>PS-7231: Modify Slave_transaction::retry_transaction() to call mysql_errno() only when thd-&gt;is_error() is true</p> </li> <li> <p>PS-7304: Correct package to include coredumper.a as a dependency of libperconaserverclient20-dev (Thanks to user Martin for reporting this issue)</p> </li> <li> <p>PS-7289: Restrict innodb encryption threads to 255 and add min/max values</p> </li> <li> <p>PS-7270: Fix admin_port to accept non-proxied connections when proxy_protocol_networks=\u2019*\u2019</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.33-36.html","title":"Percona Server for MySQL 5.7.33-36 (2021-03-02)","text":"<ul> <li>Installation Installing Percona Server for MySQL</li> </ul> <p>Percona Server for MySQL 5.7.33-36 includes all the features and bug fixes available in MySQL 5.7.33 Community Edition in addition to enterprise-grade features developed by Percona.</p>"},{"location":"release-notes/Percona-Server-5.7.33-36.html#new-features","title":"New Features","text":"<ul> <li> <p>PS-5364: Update keyring_vault plugin to support KV Secrets Engine Version 2 (kv-v2) (Thanks to user aprokofyev for reporting this issue)</p> </li> <li> <p>PS-7447: Backport variable innodb_buffer_pool_in_core_file and processing (Upstream #101825)</p> </li> <li> <p>PS-7459: Backport of InnoDB: Group purging of rows by table ID (WL#9387) to PS 5.7.33</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.33-36.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>PS-1956: Change data type for some microsecond times for the slow query log to 64-bit</p> </li> <li> <p>PS-7499: Improve error log when MyRocks fails with rocksdb_validate_tables=1</p> </li> <li> <p>PS-5112: Backport fix for PS-5027 from 8.0 to 5.7</p> </li> <li> <p>PS-7492: Update slow log formatting for tmp tables related stats</p> </li> <li> <p>PS-7498: Prevent the replication co-ordinator thread getting stuck due to MASTER_DELAY while handling partial relay log transactions. (Upstream #102647)</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.34-37.html","title":"Percona Server for MySQL 5.7.34-37 (2021-05-26)","text":"<ul> <li>Installation Installing Percona Server for MySQL</li> </ul> <p>Percona Server for MySQL 5.7.34-37 includes all the features and bug fixes available in MySQL 5.7.34 Community Edition in addition to enterprise-grade features developed by Percona.</p>"},{"location":"release-notes/Percona-Server-5.7.34-37.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>PS-4497: Incorrect option error message for mysqlbinlog</p> </li> <li> <p>PS-7498: Prevent the replication coordinator thread stuck in Waiting until MASTER_DELAY seconds after master executed event while handling partial relay log transactions. (Upstream #102647)</p> </li> <li> <p>PS-7578: Replication failure with UPDATE when replica server has a PK and the source does not. (Upstream #102628)</p> </li> <li> <p>PS-7593: When changing the tx-isolation on a session, after a transaction has executed, the change is not honored and may cause a service failure. (Upstream #102831)</p> </li> <li> <p>PS-7657: An update query executed against partition tables with compressed columns can cause an unexpected server exit.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.35-38.html","title":"Percona Server for MySQL 5.7.35-38 (2021-08-18)","text":"<ul> <li>Installation Installing Percona Server for MySQL</li> </ul> <p>Percona Server for MySQL 5.7.35-38 includes all the features and bug fixes available in MySQL 5.7.35 Community Edition in addition to enterprise-grade features developed by Percona.</p>"},{"location":"release-notes/Percona-Server-5.7.35-38.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>PS-1346: LP #1163232: Anomaly with <code>opt_log_slow_slave_statements</code>.</p> </li> <li> <p>PS-1344: LP #1160436: The <code>log_slow_statement</code> is called unconditionally</p> </li> <li> <p>PS-7582: Segmentation fault with the data masking plugin</p> </li> <li> <p>PS-1108: LP #1704163: Changing a column from uncompressed to compressed for JSON crashes the server</p> </li> <li> <p>PS-2433: LP #1234346: Include a timestamp in the slow query log file when initializing a new file</p> </li> <li> <p>PS-1955: LP #1088529: The <code>log_slow_verbosity</code> help text missing the \u201cminimal\u201d, \u201cstandard\u201d, and \u201cfull\u201d options</p> </li> <li> <p>PS-1116: LP #1719506: Audit plugin reports \u201ccommand_class=error\u201d for server-side prepared statements.</p> </li> <li> <p>PS-7755: InnoDB: tried to purge <code>non-delete-marked</code> record (Upstream #86485).</p> </li> <li> <p>PS-6802: Configure fails with make-4.3 with CMake Error at storage/rocksdb/CMakeLists.txt:152 (STRING) (Thanks to user whissi for reporting this issue)</p> </li> <li> <p>PS-1659: LP #1508909: Connect without proxy information hangs if <code>proxy_protocol_networks</code> is enabled</p> </li> <li> <p>PS-7746: Possible double call to free_share() in ha_innobase::open()</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.36-39.html","title":"Percona Server for MySQL 5.7.36-39 (2021-12-22)","text":"<ul> <li>Installation Installing Percona Server for MySQL</li> </ul> <p>Percona Server for MySQL 5.7.36-39 includes all the features and bug fixes available in the MySQL 5.7.36 Community Edition., in addition to enterprise-grade features developed by Percona.</p> <p>Percona Server for MySQL\u00ae is a free, fully compatible, enhanced, and open source drop-in replacement for any MySQL database. It provides superior performance, scalability, and instrumentation.</p> <p>Percona Server for MySQL is trusted by thousands of enterprises to provide better performance and concurrency for their most demanding workloads. It delivers more value to MySQL server users with optimized performance, greater performance scalability and availability, enhanced backups, and increased visibility. Commercial support contracts are available.</p>"},{"location":"release-notes/Percona-Server-5.7.36-39.html#release-highlights","title":"Release Highlights","text":"<p>The following lists some of the bug fixes for MySQL 5.7.36, provided by Oracle, and included in Percona Server for MySQL:</p> <ul> <li> <p>Fix for the possibility for a deadlock or failure when an undo log truncate operation is initiated after an upgrade from MySQL 5.6 to MySQL 5.7.</p> </li> <li> <p>Fix for when a parent table initiates a cascading <code>SET NULL</code> operation on the child table, the virtual column can be set to NULL instead of the value derived from the parent table.</p> </li> <li> <p>On a view, the query digest for each SELECT statement is now based on the SELECT statement and not the view definition, which was the case for earlier versions.</p> </li> </ul> <p>Find the complete list of bug fixes and changes in MySQL 5.7.36 Release Notes.</p>"},{"location":"release-notes/Percona-Server-5.7.36-39.html#improvements","title":"Improvements","text":"<ul> <li>PS-6730 The Last_errno field in the Slow Query Log only reports the errors.</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.36-39.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>PS-7868: Documentation - remove a reference to a 5.7 SELinux repository in the Yum installation document. (Thanks to user Simon Avery for reporting this issue)</p> </li> <li> <p>PS-7958: Fix for a MySQL exit when using a full-text search index with a special character.</p> </li> <li> <p>PS-1484: Fix for slow log rotation when the file name has an extension.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.37-40.html","title":"Percona Server for MySQL 5.7.37-40 (2022-03-31)","text":"<p>Percona Server for MySQL 5.7.37-40 includes all the features and bug fixes available in MySQL 5.7.37 Community Edition in addition to enterprise-grade features developed by Percona.</p> <p>Percona Server for MySQL is a free, fully compatible, enhanced, and open source drop-in replacement for any MySQL database. It delivers more value to MySQL server users with optimized performance, greater performance scalability and availability, enhanced backups, and increased visibility. Commercial support contracts are available.</p>"},{"location":"release-notes/Percona-Server-5.7.37-40.html#release-highlights","title":"Release Highlights","text":"<p>The following lists a number of the notable updates and fixes for MySQL 5.7.37, provided by Oracle, and included in Percona Server for MySQL:</p> <ul> <li> <p>The performance on debug builds has been improved by optimizing buf_validate() function in the InnoDB sources.</p> </li> <li> <p>Fix for when a query using an index that differs from the primary key of partitioned table results in excessive CPU load.</p> </li> </ul> <p>Find the complete list of bug fixes and changes in MySQL 5.7.37 Release Notes.</p>"},{"location":"release-notes/Percona-Server-5.7.37-40.html#improvements","title":"Improvements","text":"<ul> <li>PS-7792: Allows setting an empty MASTER_USER user name if you always provide user credentials when using the START_SLAVE statement. This method requires user intervention to restart the replica.</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.37-40.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>PS-7929: Fix for when the row locks were duplicated when inserting an existing row into a table within the same transaction.</p> </li> <li> <p>PS-8007: Percona Server for MySQL can fail to start if the server starts before the network mounts the datadir or a local mount of the datadir.</p> </li> <li> <p>PS-7856: A partition table update caused a server exit.</p> </li> <li> <p>PS-7890: When the server was started with the \u2013loose-rocksdb_persistent_cache_size_mb option, the RocksDB engine plugin installation failed.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.37-40.html#packaging-notes","title":"Packaging Notes","text":"<ul> <li> <p>Red Hat Enterprise Linux 6 (and derivative Linux distributions) are no longer supported.</p> </li> <li> <p>Debian 9 is no longer supported.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.37-40.html#known-issues","title":"Known issues","text":"<ul> <li>The RPM packages for Red Hat Enterprise Linux 7 (and compatible derivatives) do not support TLSv1.3, as it requires OpenSSL 1.1.1, which is currently not available on this platform.</li> </ul>"},{"location":"release-notes/Percona-Server-5.7.37-40.html#useful-links","title":"Useful links","text":"<ul> <li> <p>To install Percona Server for MySQL 5.7, follow the instructions in Installing Percona Server for MySQL .</p> </li> <li> <p>To upgrade Percona Server for MySQL from 5.6 to 5.7, follow the instructions in Percona Server In-Place Upgrading Guide: From 5.6 to 5.7.</p> </li> <li> <p>The GitHub location for Percona Server.</p> </li> <li> <p>To contribute to the Percona Server for MySQL documentation, review the Documentation Contribution Guide.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.38-41.html","title":"Percona Server for MySQL 5.7.38-41 (2022-06-02)","text":"<p>Percona Server for MySQL 5.7.39-42 includes all the features and bug fixes available in MySQL 5.7.39 Community Edition in addition to enterprise-grade features developed by Percona.</p> <p>Percona Server for MySQL is a free, fully compatible, enhanced, and open source drop-in replacement for any MySQL database. It provides superior performance, scalability, and instrumentation.</p> <p>Percona Server for MySQL is trusted by thousands of enterprises to provide better performance and concurrency for their most demanding workloads. It delivers more value to MySQL server users with optimized performance, greater performance scalability and availability, enhanced backups, and increased visibility. Commercial support contracts are available.</p>"},{"location":"release-notes/Percona-Server-5.7.38-41.html#release-highlights","title":"Release Highlights","text":"<p>Improvements and bug fixes provided by Oracle for MySQL 5.7.38 and included in Percona Server for MySQL are the following:</p> <ul> <li> <p>If a statement cannot be parsed, for example, if the statement contains syntax errors, that statement is not written to the slow query log.</p> </li> <li> <p>Loading an encrypted table failed if purge threads processed the undo records for that table.</p> </li> <li> <p>There was a memory leak when mysqldump was used on more than one table with the \u2013order-by-primary option. The memory allocated to sort each row in a table is now released after every table.</p> </li> </ul> <p>Find the full list of bug fixes and changes in the MySQL 5.7.38 Release Notes.</p>"},{"location":"release-notes/Percona-Server-5.7.38-41.html#deprecation-and-removal","title":"Deprecation and removal","text":"<ul> <li> <p>The <code>myisam_repair_threads</code> system variable is deprecated. Values other than 1 (the default) for <code>myisam_repair_threads</code> throw a warning. Support for this variable may be removed in future versions.</p> </li> <li> <p>myisamchk <code>--parallel-recover</code> option is deprecated. Support for this option may be removed in future versions.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.38-41.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>PS-6029: The data masking gen_rnd_us_phone() function had a different format compared to MySQL upstream version.</p> </li> <li> <p>PS-8129: A fix for when mutex hangs in thread_pool_unix.</p> </li> <li> <p>PS-8136: <code>LOCK TABLES FOR BACKUP</code> did not prevent InnoDB key rotation. Due to this behavior, Percona Xtrabackup couldn\u2019t fetch the key in case the key was rotated after starting the backup.</p> </li> <li> <p>PS-8143: Fixed the memory leak in <code>File_query_log::set_rotated_name()</code>.</p> </li> <li> <p>PS-8204: When the <code>audit_log_format</code> was set to XML, logged queries were truncated after a newline character.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.38-41.html#useful-links","title":"Useful links","text":"<ul> <li> <p>The Percona Server for MySQL installation instructions</p> </li> <li> <p>The Percona Server In-Place Upgrading Guide: From 5.6 to 5.7</p> </li> <li> <p>The Percona Software downloads</p> </li> <li> <p>The Percona Server for MySQL GitHub location</p> </li> <li> <p>To contribute to the documentation, review the Documentation Contribution Guide</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.39-42.html","title":"Percona Server for MySQL 5.7.39-42 (2022-08-15)","text":"<p>Percona Server for MySQL 5.7.39-42 includes all the features and bug fixes available in MySQL 5.7.39 Community Edition in addition to enterprise-grade features developed by Percona.</p> <p>Percona Server for MySQL is a free, fully compatible, enhanced, and open source drop-in replacement for any MySQL database. It provides superior performance, scalability, and instrumentation.</p> <p>Percona Server for MySQL is trusted by thousands of enterprises to provide better performance and concurrency for their most demanding workloads. It delivers more value to MySQL server users with optimized performance, greater performance scalability and availability, enhanced backups, and increased visibility. Commercial support contracts are available.</p>"},{"location":"release-notes/Percona-Server-5.7.39-42.html#release-highlights","title":"Release Highlights","text":"<p>Improvements and bug fixes provided by Oracle for MySQL 5.7.39 and included in Percona Server for MySQL are the following:</p> <ul> <li>To provide process information, the <code>SHOW PROCESSLIST</code> statement collects thread data from all active threads. Since the implementation iterates across active threads from within the thread manager while holding a global mutex, it has a negative impact on performance, particularly on busy systems.</li> </ul> <p>Now, an alternative <code>SHOW PROCESSLIST</code> implementation is available based on the new Performance Schema processlist table. This implementation queries active thread data from the Performance Schema rather than the thread manager and does not require a mutex:</p> <ul> <li> <p>To enable the alternative implementation, enable the <code>performance_schema_show_processlist</code> system variable.</p> <p>Note</p> <p>For new installations of MySQL 5.7.39, or higher, the processlist table is automatically created in the Performance Schema. It is not created automatically by an upgrade. If you are upgrading from an earlier version of MySQL 5.7, and want to use the Performance Schema implementation of processlist, create the table manually.</p> </li> </ul> <p>Find more information in the Creating the processlist table.</p> <ul> <li> <p>The alternative implementation of <code>SHOW PROCESSLIST</code> also applies to the mysqladmin processlist command.</p> </li> <li> <p>The alternative implementation does not apply to the <code>INFORMATION_SCHEMA PROCESSLIST</code> table or the <code>COM_PROCESS_INFO</code> command of the MySQL client/server protocol.</p> </li> <li> <p>To ensure that the default and alternative implementations give the same information, check the configuration requirements in The processlist Table.</p> </li> <li> <p>MySQL removes a 4GB tablespace file size limit on Windows 32-bit systems. The limit was set because of an incorrect calculation performed while extending the tablespace.</p> </li> <li> <p>When, during a session, an incorrect value for the <code>binlog_checksum</code> system variable is set, a <code>COM_BINLOG_DUMP</code> command ran in the same session to request a binary log stream from a source fails. Now, the server validates the specified checksum value before starting the checksum algorithm setup process.</p> </li> </ul> <p>Find the full list of bug fixes and changes in the MySQL 5.7.39 Release Notes.</p>"},{"location":"release-notes/Percona-Server-5.7.39-42.html#deprecation-and-removal","title":"Deprecation and removal","text":"<ul> <li> <p>The <code>myisam_repair_threads</code> system variable has been removed.</p> </li> <li> <p>myisamchk <code>--parallel-recover</code> option has been removed.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.39-42.html#improvements","title":"Improvements","text":"<p>The <code>SHOW PROCESSLIST</code> statement now displays an extra field <code>TIME_MS</code>. The <code>TIME_MS</code> field provides the information about the time in milliseconds that the thread has been in its current state.</p>"},{"location":"release-notes/Percona-Server-5.7.39-42.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>PS-8205: <code>DICT_TF2_FLAG_SET</code> was used instead of <code>DICT_TF2_FLAG_IS_SET</code>.</p> </li> <li> <p>PS-8174: MySQL crashed at shutdown with <code>buf0flu.cc:3567:UT_LIST_GET_LEN(buf_pool-&gt;flush_list) == 0</code> assertion.</p> </li> </ul>"},{"location":"release-notes/Percona-Server-5.7.39-42.html#useful-links","title":"Useful links","text":"<ul> <li> <p>The Percona Server for MySQL installation instructions</p> </li> <li> <p>The Percona Server In-Place Upgrading Guide: From 5.6 to 5.7</p> </li> <li> <p>The Percona Software downloads</p> </li> <li> <p>The Percona Server for MySQL GitHub location</p> </li> <li> <p>To contribute to the documentation, review the Documentation Contribution Guide</p> </li> </ul>"},{"location":"release-notes/release-notes_index.html","title":"Percona Server for MySQL 5.7 release notes index","text":"<ul> <li> <p>Percona Server for MySQL 5.7.43-47 (2023-08-17)</p> </li> <li> <p>Percona Server for MySQL 5.7.42-46 (2023-06-01)</p> </li> <li> <p>Percona Server for MySQL 5.7.42-45 (2023-05-23)</p> </li> <li> <p>Percona Server for MySQL 5.7.41-44 (2023-03-02)</p> </li> <li> <p>Percona Server for MySQL 5.7.40-43 (2022-11-28)</p> </li> <li> <p>Percona Server for MySQL 5.7.39-42 (2022-08-15)</p> </li> <li> <p>Percona Server for MySQL 5.7.38-41 (2022-06-02)</p> </li> <li> <p>Percona Server for MySQL 5.7.37-40 (2022-03-31)</p> </li> <li> <p>Percona Server for MySQL 5.7.36-39 (2021-12-22)</p> </li> <li> <p>Percona Server for MySQL 5.7.35-38 (2021-08-18)</p> </li> <li> <p>Percona Server for MySQL 5.7.34-37 (2021-05-26)</p> </li> <li> <p>Percona Server for MySQL 5.7.33-36 (2021-03-02)</p> </li> <li> <p>Percona Server for MySQL 5.7.32-35 (2020-11-24)</p> </li> <li> <p>Percona Server for MySQL 5.7.31-34 (2020-08-24)</p> </li> <li> <p>Percona Server for MySQL 5.7.30-33 (2020-05-20)</p> </li> <li> <p>Percona Server for MySQL 5.7.29-32 (2020-02-05)</p> </li> <li> <p>Percona Server for MySQL 5.7.28-31 (2019-11-13)</p> </li> <li> <p>Percona Server for MySQL 5.7.27-30 (2019-08-22)</p> </li> <li> <p>Percona Server for MySQL 5.7.26-29 (2019-05-27)</p> </li> <li> <p>Percona Server for MySQL 5.7.25-28 (2019-02-18)</p> </li> <li> <p>Percona Server for MySQL 5.7.24-27 (2018-12-18)</p> </li> <li> <p>Percona Server for MySQL 5.7.24-26 (2018-12-04)</p> </li> <li> <p>Percona Server for MySQL 5.7.23-25 (2018-11-21)</p> </li> <li> <p>Percona Server for MySQL 5.7.23-24 (2018-11-09)</p> </li> <li> <p>Percona Server for MySQL 5.7.23-23 (2018-09-12)</p> </li> <li> <p>Percona Server for MySQL 5.7.22-22 (2018-05-31)</p> </li> <li> <p>Percona Server for MySQL 5.7.21-21 (2018-04-24)</p> </li> <li> <p>Percona Server for MySQL 5.7.21-20 (2018-02-19)</p> </li> <li> <p>Percona Server for MySQL 5.7.20-19 (2018-01-03)</p> </li> <li> <p>Percona Server for MySQL 5.7.20-18 (2017-12-14)</p> </li> <li> <p>Percona Server for MySQL 5.7.19-17 (2017-08-31)</p> </li> <li> <p>Percona Server for MySQL 5.7.18-16 (2017-07-28)</p> </li> <li> <p>Percona Server for MySQL 5.7.18-15 (2017-05-26)</p> </li> <li> <p>Percona Server for MySQL 5.7.18-14 (2017-05-12)</p> </li> <li> <p>Percona Server for MySQL 5.7.17-13 (2017-04-05)</p> </li> <li> <p>Percona Server for MySQL 5.7.17-12 (2017-03-24)</p> </li> <li> <p>Percona Server for MySQL 5.7.17-11 (2017-02-03)</p> </li> <li> <p>Percona Server for MySQL 5.7.16-10 (2016-11-28)</p> </li> <li> <p>Percona Server for MySQL 5.7.15-9 (2016-10-21)</p> </li> <li> <p>Percona Server for MySQL 5.7.14-8 (2016-09-21)</p> </li> <li> <p>Percona Server for MySQL 5.7.14-7 (2016-08-23)</p> </li> <li> <p>Percona Server for MySQL 5.7.13-6 (2016-07-16)</p> </li> <li> <p>Percona Server for MySQL 5.7.12-5 (2016-06-06)</p> </li> <li> <p>Percona Server for MySQL 5.7.11-4 (2016-03-15)</p> </li> <li> <p>Percona Server for MySQL 5.7.10-3 (2016-02-23)</p> </li> <li> <p>Percona Server for MySQL 5.7.10-2 (2016-02-05)</p> </li> <li> <p>Percona Server for MySQL 5.7.10-1 (2015-12-14)</p> </li> </ul>"},{"location":"reliability/innodb_corrupt_table_action.html","title":"Handle Corrupted Tables","text":"<p>When a server subsystem tries to access a corrupted table, the server may crash. If this outcome is not desirable when a corrupted table is encountered, set the new system innodb_corrupt_table_action variable to a value which allows the ongoing operation to continue without crashing the server.</p> <p>The server error log registers attempts to access corrupted table pages.</p>"},{"location":"reliability/innodb_corrupt_table_action.html#interacting-with-the-innodb_force_recovery-variable","title":"Interacting with the innodb_force_recovery variable","text":"<p>The innodb_corrupt_table_action variable may work in conjunction with the innodb_force_recovery variable which considerably reduces the effect of InnoDB subsystems running in the background.</p> <p>If the innodb_force_recovery variable is set to a low value and you expect the server to crash, the server may continue to run due to a non-default value of the innodb_corrupt_table_action variable.</p> <p>For more information about the innodb_force_recovery variable, see Forcing InnoDB Recovery from the MySQL Reference Manual.</p> <p>This feature adds a new system variable.</p>"},{"location":"reliability/innodb_corrupt_table_action.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6 5.6</li> </ul>"},{"location":"reliability/innodb_corrupt_table_action.html#system-variables","title":"System Variables","text":""},{"location":"reliability/innodb_corrupt_table_action.html#innodb_corrupt_table_action","title":"<code>innodb_corrupt_table_action</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type ULONG Default assert Range assert, warn, salvage <ul> <li> <p>With the default value, <code>assert</code>, XtraDB will intentionally crash the server with an assertion failure as it would normally do when detecting corrupted data in a single-table tablespace.</p> </li> <li> <p>If the <code>warn</code> value is used it will pass corruption of the table as <code>corrupt table</code> instead of crashing itself. For this to work <code>innodb_file_per_table</code> should be enabled. All file I/O for the datafile after detected as corrupt is disabled, except for the deletion.</p> </li> <li> <p>When the option value is <code>salvage</code>, XtraDB allows read access to a corrupted tablespace, but ignores corrupted pages\u201d. You must enable the innodb_file_per_table option.</p> </li> </ul>"},{"location":"reliability/log_connection_error.html","title":"Too Many Connections Warning","text":"<p>This feature issues the warning <code>Too many connections</code> to the log, if log_error_verbosity is set to <code>2</code> or higher.</p>"},{"location":"reliability/log_connection_error.html#version-specific-information","title":"Version-Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6.</li> </ul>"},{"location":"scalability/innodb_io.html","title":"Improved InnoDB I/O Scalability","text":"<p>Because InnoDB is a complex storage engine it must be configured properly in order to perform at its best. Some points are not configurable in standard InnoDB. The goal of this feature is to provide a more exhaustive set of options for XtraDB.</p>"},{"location":"scalability/innodb_io.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"scalability/innodb_io.html#system-variables","title":"System Variables","text":""},{"location":"scalability/innodb_io.html#innodb_use_global_flush_log_at_trx_commit","title":"<code>innodb_use_global_flush_log_at_trx_commit</code>","text":"Option Description Command-line Yes Config File Yes Scope Global Dynamic Yes Data type Boolean Default True Range True/False <p>This variable enables or disables the effect of the per-session value of the innodb_flush_log_at_trx_commit variable.</p> <p>If the global variable  innodb_use_global_flush_log_at_trx_commit is set to <code>1</code>, the session uses the current global value of innodb_flush_log_at_trx_commit. This is the upstream-compatible mode. If the user attempts to change the innodb_flush_log_at_trx_commit value for a session, the session value is ignored.</p> <p>If the global variable innodb_use_global_flush_log_at_trx_commit is set to <code>0</code>, a user can modify the <code>innodb_flush_log_at_trx_commit</code> per-session using the following command:</p> <pre><code>SET SESSION innodb_flush_log_at_trx_commit=0\n</code></pre> <p>This modification only affects the transactions in that session. Other sessions, if they have not been individually modified, continue to use the global <code>innodb_use_flush_log_at_trx_commit</code> value.</p> <pre><code>SET innodb_use_global_flush_log_at_trx_commit=1\n</code></pre>"},{"location":"scalability/innodb_io.html#innodb_flush_method","title":"<code>innodb_flush_method</code>","text":"Option Description Command-line Yes Config File Yes Scope Global Dynamic No Data type Enumeration Default fdatasync Allowed values fdatasync, O_DSYNC, O_DIRECT, O_DIRECT_NO_FSYNC, ALL_O_DIRECT <p>The variable was ported from Percona Server for MySQL 5.6 in Percona Server for MySQL 5.7.10-3. This is an existing MySQL 5.7 system variable that has a new allowed value <code>ALL_O_DIRECT</code>. It determines the method InnoDB uses to flush its data and log files. (See innodb_flush_method in the MySQL 5.7 Reference Manual).</p> <p>The following values are allowed:</p> <ul> <li> <p><code>fdatasync</code>: use <code>fsync()</code> to flush data, log, and parallel doublewrite files.</p> </li> <li> <p><code>O_SYNC</code>: use <code>O_SYNC</code> to open and flush the log and parallel doublewrite files; use <code>fsync()</code> to flush the data files. Do not use <code>fsync()</code> to flush the parallel doublewrite file.</p> </li> <li> <p><code>O_DIRECT</code>: use O_DIRECT to open the data files and <code>fsync()</code> system call to flush data, log, and parallel doublewrite files.</p> </li> <li> <p><code>O_DIRECT_NO_FSYNC</code>: use O_DIRECT to open the data files and parallel doublewrite files, but does not use the <code>fsync()</code> system call to flush the data files, log files, and parallel doublewrite files. This option isn\u2019t suitable for XFS file system.</p> </li> <li> <p><code>ALL_O_DIRECT</code>: use O_DIRECT to open data files, log files, and parallel doublewrite files and use <code>fsync()</code> to flush the data files but not the log files or parallel doublewrite files. This option is recommended when InnoDB log files are big (more than 8GB), otherwise, there may be performance degradation. Note: On ext4 filesystem, set <code>innodb_log_write_ahead_size</code>. This variable should match the filesystem\u2019s write-ahead block size and avoids the <code>unaligned AIO/DIO</code> warnings.</p> </li> </ul>"},{"location":"scalability/innodb_io.html#status-variables","title":"Status Variables","text":"<p>The following information has been added to <code>SHOW ENGINE INNODB STATUS</code> to confirm the checkpoint activity:</p> <pre><code>The max checkpoint age\nThe current checkpoint age target\nThe current age of the oldest page modification which has not been flushed to disk yet.\nThe current age of the last checkpoint\n...\n---\nLOG\n---\nLog sequence number 0 1059494372\nLog flushed up to   0 1059494372\nLast checkpoint at  0 1055251010\nMax checkpoint age  162361775\nCheckpoint age target 104630090\nModified age        4092465\nCheckpoint age      4243362\n0 pending log writes, 0 pending chkp writes\n...\n</code></pre>"},{"location":"scalability/innodb_split_buf_pool_mutex.html","title":"Improved Buffer Pool Scalability","text":"<p>The InnoDB buffer pool is a well known point of contention when many queries are executed concurrently. In XtraDB, the global mutex protecting the buffer pool has been split into several mutexes to decrease contention.</p> <p>This feature splits the single global InnoDB buffer pool mutex into several mutexes:</p> Name Protects flush_state_mutex flushing state of dirty blocks LRU_list_mutex LRU lists of blocks in buffer pool flush_list_mutex flush list of dirty blocks to flush free_list_mutex list of free blocks in buffer pool zip_free_mutex lists of free area to treat compressed pages zip_hash_mutex hash table to search compressed pages <p>The goal of this change is to reduce mutex contention, which can be very impacting when the working set does not fit in memory.</p>"},{"location":"scalability/innodb_split_buf_pool_mutex.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</li> </ul>"},{"location":"scalability/innodb_split_buf_pool_mutex.html#other-information","title":"Other Information","text":""},{"location":"scalability/innodb_split_buf_pool_mutex.html#detecting-mutex-contention","title":"Detecting Mutex Contention","text":"<p>You can detect when you suffer from mutex contention in the buffer pool by reading the information provided in the SEMAPHORES section of the output of SHOW ENGINE INNODB STATUS:</p> <p>Under normal circumstances this section should look like this:</p> <pre><code>SEMAPHORES\n----------\nOS WAIT ARRAY INFO: reservation count 50238, signal count 17465\nMutex spin waits 0, rounds 628280, OS waits 31338\nRW-shared spins 38074, OS waits 18900; RW-excl spins 0, OS waits 0\n</code></pre> <p>If you have a high-concurrency workload this section may look like this:</p> <pre><code>1 ----------\n2 SEMAPHORES\n3 ----------\n4 OS WAIT ARRAY INFO: reservation count 36255, signal count 12675\n5 --Thread 10607472 has waited at buf/buf0rea.c line 420 for 0.00 seconds the semaphore:\n6 Mutex at 0x358068 created file buf/buf0buf.c line 597, lock var 0\n7 waiters flag 0\n8 --Thread 3488624 has waited at buf/buf0buf.c line 1177 for 0.00 seconds the semaphore:\n9 Mutex at 0x358068 created file buf/buf0buf.c line 597, lock var 0\n10 waiters flag 0\n11 --Thread 6896496 has waited at btr/btr0cur.c line 442 for 0.00 seconds the semaphore:\n12 S-lock on RW-latch at 0x8800244 created in file buf/buf0buf.c line 547\n13 a writer (thread id 14879600) has reserved it in mode  exclusive\n14 number of readers 0, waiters flag 1\n15 Last time read locked in file btr/btr0cur.c line 442\n16 Last time write locked in file buf/buf0buf.c line 1797\n[...]\n17 Mutex spin waits 0, rounds 452650, OS waits 22573\n18 RW-shared spins 27550, OS waits 13682; RW-excl spins 0, OS waits 0\n</code></pre> <p>Note that in the second case you will see indications that threads are waiting for a mutex created in the file <code>buf/buf0buf.c</code> (lines 5 to 7 or 8 to 10). Such an indication is a sign of buffer pool contention.</p>"},{"location":"security/data-at-rest-encryption.html","title":"Data at Rest Encryption","text":"<p>Percona Server for MySQL  enables data at rest encryption of the InnoDB (file-per-table) tablespace by encrypting the physical database files. The data is automatically encrypted prior to writing to storage and automatically decrypted when read. If unauthorized users access the data files, they cannot read the contents. Percona Server for MySQL 5.7 data at rest encryption is similar to the MySQL 5.7 data-at-rest encryption. Percona Server for MySQL 8.0 provides more encryption features and options which are not available in this version.</p> <p>The following table lists the various features that are considered Generally Available (GA) or in tech preview. The tech preview features and variables are not recommended to be used in production. Features and variables marked as deprecated perform no action.</p> Feature Status GA Version Tech Preview Version Deprecated Version Vault Keyring Plugin Generally Available, supported Percona Server 5.7.21-21 Encrypting a File-Per-Table Tablespace Generally Available, supported Percona Server 5.7.21-21 Encrypting a General Tablespace Generally Available, supported Percona Server 5.7.21-21 Temporary file encryption Generally Available, supported Percona Server 5.7.22-22 binlog_encrypt Generally Available, supported Percona Server 5.7.21-21 InnoDB System Tablespace Encryption Deprecated Percona Server 5.7.23-24 Percona Server for MySQL 5.7.32-35 Doublewrite buffer Deprecated Percona Server 5.7.23-24 Percona Server for MySQL 5.7.32-35 InnoDB Undo Tablespace Encryption Deprecated Percona Server 5.7.23-24 Percona Server for MySQL 5.7.32-35 Redo Log Encryption Deprecated Percona Server 5.7.23-24 Percona Server for MySQL 5.7.32-35 Data Scrubbing Deprecated Percona Server 5.7.23-24 Percona Server for MySQL 5.7.32-35"},{"location":"security/data-at-rest-encryption.html#architecture","title":"Architecture","text":"<p>The data at rest encryption uses a two-tier architecture with the following components:</p> Type Description Master key The Master key is used to encrypt or decrypt the tablespace keys. Tablespace key for each tablespace The tablespace key encrypts the data pages and is written in the tablespace header. <p>When the server must access the data, the master key decrypts the tablespace key, the tablespace is decrypted and available for read or write operations.</p> <p>The two separate keys architecture allows the master key to be rotated in a minimal operation. During the master key rotation, each tablespace key is re-encrypted with the new master key. Only the first page of the tablespace file (.ibd) is read and written during the rotation. An encrypted page is decrypted at the I/O layer, added to the buffer pool, and used to read and write the data. A buffer pool page is not encrypted. The I/O layer encrypts the page before the page is flushed to disk.</p> <p>An encryption key in the tablespace header is required to encrypt or decrypt the tablespace. The Master key is stored in the keyring plugin.</p> <p>Note</p> <p>Percona XtraBackup version 2.4 supports the backup of encrypted general tablespaces.</p>"},{"location":"security/data-at-rest-encryption.html#vault-keyring-plugin","title":"Vault Keyring Plugin","text":"<p>To enable encryption, use either of the following plugins:</p> <ul> <li> <p>keyring_file stores the keyring data locally in a flat file</p> </li> <li> <p>keyring_vault provides an interface for the database with a HashiCorp Vault server to store key and secure encryption keys.</p> </li> </ul> <p>Enable only one keyring plugin at a time. Enabling multiple keyring plugins is not supported and may result in data loss.</p> <p>Note</p> <p>The keyring_file plugin should not be used for regulatory compliance.</p> <p>To install the selected plugin, follow the installing and uninstalling plugins instructions.</p>"},{"location":"security/data-at-rest-encryption.html#loading-the-keyring-plugin","title":"Loading the Keyring Plugin","text":"<p>Load the plugin at server startup with the early-plugin-load Option to enable the keyring. To make encrypted table recovery more efficient,load the plugin with the configuration file.</p> <p>Run the following command to load the keyring_file plugin:</p> <pre><code>$ mysqld --early-plugin-load=\"keyring_file=keyring_file.so\"\n</code></pre> <p>Note</p> <p>To start a server with different early plugins to be loaded, the <code>--early-plugin-load</code> option can contain the plugin names in a double-quoted list with each plugin name separated by a semicolon. The use of double quotes ensures the semicolons do not create issues when the list is executed in a script.</p> <p>To enable Master key vault encryption, the user must have SUPER privileges.</p> <p>The following statements loads the keyring_vault plugin and the keyring_vault_config. The second statement provides the location to the keyring_vault configuration file.</p> <pre><code>[mysqld]\nearly-plugin-load=\"keyring_vault=keyring_vault.so\"\nloose-keyring_vault_config=\"/home/mysql/keyring_vault.conf\"\n</code></pre> <p>Add the following statements to my.cnf:</p> <pre><code>[mysqld]\nearly-plugin-load=\"keyring_vault=keyring_vault.so\"\nloose-keyring_value_config=\"/home/mysql/keyring_vault.conf\"\n</code></pre> <p>Restart the server.</p> <p>Note</p> <p>The keyring_vault extension, \u201c.so\u201d, and the file location for the vault configuration should be changed to match your operating system\u2019s extension and operating system location.</p>"},{"location":"security/data-at-rest-encryption.html#describing-the-keyring_vault_config-file","title":"Describing the keyring_vault_config file","text":"<p>The keyring_vault_config file has the following information:</p> <ul> <li> <p><code>vault_url</code> - the Vault server address</p> </li> <li> <p><code>secret_mount_point</code> - where the keyring_vault stores the keys</p> </li> <li> <p><code>secret_mount_point_version</code> - the <code>KV Secrets Engine version (kv or kv-v2)</code> used. Implemented in Percona Server for MySQL 5.7.33-36.</p> </li> <li> <p><code>token</code> - a token generated by the Vault server</p> </li> <li> <p><code>vault_ca [optional]</code> - if the machine does not trust the Vault\u2019s CA certificate, this variable points to the CA certificate used to sign the Vault\u2019s certificates.</p> </li> </ul> <p>The following is a configuration file example:</p> <pre><code>vault_url = https://vault.public.com:8202\nsecret_mount_point = secret\nsecret_mount_point_version = AUTO\ntoken = 58a20c08-8001-fd5f-5192-7498a48eaf20\nvault_ca = /data/keyring_vault_confs/vault_ca.crt\n</code></pre> <p>Warning</p> <p>Each <code>secret_mount_point</code> must be used by only one server. Multiple servers using the same secret_mount_point may cause unpredictable behavior.</p> <p>Create a backup of the keyring configuration file or data file immediately after creating the encrypted tablespace. If you are using Master key encryption, backup before master key rotation and after master key rotation.</p> <p>The first time a key is retrieved from a keyring, the keyring_vault communicates with the Vault server to retrieve the key type and data.</p> Variables keyring_vault_config keyring_vault_timeout"},{"location":"security/data-at-rest-encryption.html#verifying-the-keyring-plugin-is-active","title":"Verifying the Keyring Plugin is Active","text":"<p>To verify the keyring plugin is active, run the SHOW PLUGINS statement or run a query on the INFORMATION_SCHEMA.PLUGINS table. You can also query the PLUGINS view.</p> <pre><code>SELECT plugin_name, plugin_status FROM INFORMATION_SCHEMA.PLUGINS WHERE plugin_name LIKE 'keyring%';\n</code></pre> <p>The output could be the following:</p> <pre><code>+---------------+----------------+\n| plugin_name   | plugin_status  |\n+===============+================+\n| keyring_file  | ACTIVE         |\n+---------------+----------------+\n</code></pre>"},{"location":"security/data-at-rest-encryption.html#encrypting-a-file-per-table-tablespace","title":"Encrypting a File-Per-Table Tablespace","text":"<p>The CREATE TABLESPACE statement is extended to allow the <code>ENCRYPTION=['Y/N']</code> option to encrypt a File-per-Table tablespace.</p> <pre><code>CREATE TABLE myexample (id INT mytext varchar(255)) ENCRYPTION='Y';\n</code></pre> <p>To enable encryption to an existing tablespace, add the <code>ENCRYPTION</code> option to the <code>ALTER TABLE</code> statement.</p> <pre><code>CREATE TABLE myexample ENCRYPTION='Y';\n</code></pre> <p>You must add the <code>ENCRYPTION</code> option to ALTER TABLE to change the table encryption state. Without the <code>ENCRYPTION</code> option, an encrypted table remains encrypted or an unencrypted table remains unencrypted.</p> <p>To change the tablespace key, run the optimize table command.</p> <pre><code>mysql&gt; optimize table t1;\n</code></pre>"},{"location":"security/data-at-rest-encryption.html#encrypting-a-general-tablespace","title":"Encrypting a General Tablespace","text":"<p>As of Percona Server 5.7.20-18, Percona Server for MySQL supports general tablespace encryption. You cannot partially encrypt the tables in a general tablespace. All of the tables must be encrypted or none of the tables are encrypted.</p>"},{"location":"security/data-at-rest-encryption.html#automatically-encrypting-tablespaces","title":"Automatically Encrypting Tablespaces","text":"<p>Add the <code>innodb_encrypt_tables</code> variable to my.cnf to automatically encrypt general tablespaces. The possible values for the variable are:</p> Value Description OFF The default value which disables automatic encryption of new tables ON Enables automatic encryption for new tables FORCE New tables are automatically created with encryption. Adding ENCRYPTION=NO to either a CREATE TABLE or ALTER TABLE statement results in a warning. <p>The CREATE TABLESPACE statement is extended to allow the <code>ENCRYPTION=[' Y/N']</code> option.</p> <pre><code>CREATE TABLE t1 (id INT) ENCRYPTION='Y';\n</code></pre> <p>To encrypt an existing table, add the ENCRYPTION option in the <code>ALTER TABLE</code> statement.</p> <pre><code>ALTER TABLE t1 ENCRYPTION='Y';\n</code></pre> <p>You can also disable encryption for a table, set the encryption to N.</p> <pre><code>ALTER TABLE t1 ENCRYPTION='N';\n</code></pre> <p>Note</p> <p>The <code>ALTER TABLE</code> statement modifies the current encryption mode only if the <code>ENCRYPTION</code> clause is explicitly added.</p>"},{"location":"security/data-at-rest-encryption.html#system-variables","title":"System Variables","text":"<p>Note</p> <p>You cannot change the tablespace key for tables in a general tablespace.</p>"},{"location":"security/data-at-rest-encryption.html#encrypting-binary-logs","title":"Encrypting Binary Logs","text":"<p>To start binlog encryption, start the server with <code>-encrypt-binlog=1</code>. This state requires <code>-master_verify_checksum</code> and <code>-binlog_checksum</code> to be <code>ON</code> and one of the keyring plugins loaded.</p> <p>NOTE: These actions do not encrypt all binlogs in a replication schema. You must enable <code>encrypt-binlog</code> on each of the replica servers, even if they do not produce binlog files. Enabling encryption on replica servers enable relay log encryption.</p> <p>You can rotate the encryption key used by Percona Server for MySQL by running the following statement:</p> <pre><code>SELECT rotate_system_key(\"percona_binlog\");\n</code></pre> <p>Note</p> <p>The <code>rotate_system_key(\"percona_binlog\")</code> command is Experimental quality.</p> <p>This command creates a new binlog encryption key in the keyring. The new key encrypts the next binlog file.</p>"},{"location":"security/data-at-rest-encryption.html#temporary-file-encryption","title":"Temporary file encryption","text":"<p>Percona Server for MySQL supports the encryption of temporary file storage. Users enable the encryption with <code>encrypt-tmp_files</code>.</p> <p>Enable the variable in the following command:</p> <pre><code>[mysqld]\n...\nencrypt-tmp-files=ON\n...\n</code></pre>"},{"location":"security/data-at-rest-encryption.html#verifying-the-encryption-setting","title":"Verifying the Encryption Setting","text":"<p>For single tablespaces, verify the ENCRYPTION option using INFORMATION_SCHEMA.TABLES and the CREATE OPTIONS settings.</p> <pre><code>SELECT TABLE_SCHEMA, TABLE_NAME, CREATE_OPTIONS FROM\nINFORMATION_SCHEMA.TABLES WHERE CREATE_OPTIONS LIKE '%ENCRYPTION%';\n</code></pre> <p>The output could be the following:</p> <pre><code>+----------------------+-------------------+------------------------------+\n| TABLE_SCHEMA         | TABLE_NAME        | CREATE_OPTIONS               |\n+----------------------+-------------------+------------------------------+\n|sample                | t1                | ENCRYPTION=\"Y\"               |\n+----------------------+-------------------+------------------------------+\n</code></pre> <p>A <code>flag</code> field in the <code>INFORMATION_SCHEMA.INNODB_TABLESPACES</code> has the bit number 13 set if the tablespace is encrypted. This bit can be checked with the <code>flag &amp; 8192</code> expression with the following method:</p> <pre><code>SELECT space, name, flag, (flag &amp; 8192) != 0 AS encrypted FROM\nINFORMATION_SCHEMA.INNODB_TABLESPACES WHERE name in ('foo', 'test/t2', 'bar',\n'noencrypt');\n</code></pre> <p>The output could be the following:</p> <pre><code>  +-------+-----------+-------+-----------+\n  | space | name      | flag  | encrypted |\n  +-------+-----------+-------+-----------+\n  |    29 | foo       | 10240 |      8192 |\n  |    30 | test/t2   |  8225 |      8192 |\n  |    31 | bar       | 10240 |      8192 |\n  |    32 | noencrypt |  2048 |         0 |\n  +-------+-----------+-------+-----------+\n  4 rows in set (0.01 sec)\n</code></pre> <p>To allow for master Key rotation, you can encrypt an already encrypted InnoDB system tablespace with a new master key by running the following <code>ALTER INSTANCE</code> statement:</p> <pre><code>ALTER INSTANCE ROTATE INNODB MASTER KEY;\n</code></pre>"},{"location":"security/data-at-rest-encryption.html#rotating-the-master-key","title":"Rotating the Master Key","text":"<p>For security, you should rotate the Master key in a timely manner. Use the <code>ALTER INSTANCE</code> statement. To rotate the key, you must have <code>SUPER</code> privilege.</p> <pre><code>ALTER INSTANCE ROTATE INNODB MASTER KEY;\n</code></pre> <p>The statement cannot be run at the same time you run <code>CREATE TABLE ... ENCRYPTION</code> or <code>ALTER TABLE ENCRYPTION</code> statements. The <code>ALTER INSTANCE</code> statement uses locks to prevent conflicts. If a DML statement is running, that statement must complete before the <code>ALTER INSTANCE</code> statement begins.</p> <p>When the Master key is rotated, the tablespace keys in that instance are re-encrypted. The operation does not re-encrypt the tablespace data.</p> <p>The re-encryption for the tablespace keys must succeed for the key rotation to be successful. If the rotation is interrupted, for example, if there is a server failure, the operation rolls forward when the server restarts.</p>"},{"location":"security/data-at-rest-encryption.html#innodb-system-tablespace-encryption","title":"InnoDB System Tablespace Encryption","text":"<p>This feature was in tech preview from version 5.7.23-24 but is deprecated from version 5.7.32-35. This feature is not recommended to be used in production.</p> <p>The InnoDB system tablespace is encrypted by using master key encryption. The server must be started with the <code>--bootstrap</code> option.</p> <p>If the variable innodb_sys_tablespace_encrypt is set to ON and the server has been started in the bootstrap mode, you may create an encrypted table as follows:</p> <pre><code>mysql&gt; CREATE TABLE ... TABLESPACE=innodb_system ENCRYPTION='Y'\n</code></pre> <p>Note</p> <p>You cannot encrypt existing tables in the System tablespace.</p> <p>It is not possible to convert the system tablespace from encrypted to unencrypted or vice versa. A new instance should be created and user tables must be transferred to the desired instance.</p> <p>You can encrypt the already encrypted InnoDB system tablespace (key rotation) with a new master key by running the following <code>ALTER INSTANCE</code> statement:</p> <pre><code>mysql&gt; ALTER INSTANCE ROTATE INNODB MASTER KEY\n</code></pre>"},{"location":"security/data-at-rest-encryption.html#innodb_sys_tablespace_encrypt","title":"<code>innodb_sys_tablespace_encrypt</code>","text":"Option Description Command-line \u2013innodb-sys-tablespace-encrypt Scope Global Dynamic No Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.23-24 and deprecated in Percona Server for MySQL 5.7.32-35. Enables the encryption of the InnoDB System tablespace. It is essential that the server is started with the <code>--bootstrap</code> option.</p>"},{"location":"security/data-at-rest-encryption.html#doublewrite-buffer","title":"Doublewrite buffer","text":"<p>This feature was in tech preview from version Percona Server 5.7.23-24 but is deprecated from version Percona Server for MySQL 5.7.32-35. This feature is not recommended to be used in production.</p> <p>The two types of doublewrite buffers used in Percona Server for MySQL are encrypted differently.</p> <p>When the InnoDB system tablespace is encrypted, the <code>doublewrite buffer</code> pages are encrypted as well. The key which was used to encrypt the InnoDB system tablespace is also used to encrypt the doublewrite buffer.</p> <p>Percona Server for MySQL encrypts the <code>parallel doublewrite buffer</code> with the respective tablespace keys. Only encrypted tablespace pages are written as encrypted in the parallel doublewrite buffer. Unencrypted tablespace pages will be written as unencrypted.</p>"},{"location":"security/data-at-rest-encryption.html#innodb_parallel_dblwr_encrypt","title":"<code>innodb_parallel_dblwr_encrypt</code>","text":"Option Description Command-line \u2013innodb-parallel-dblwr-encrypt Scope Global Dynamic Yes Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.23-24 and deprecated in Percona Server for MySQL 5.7.32-35. Enables the encryption of the parallel doublewrite buffer. For encryption, uses the key of the tablespace where the parallel doublewrite buffer is used.</p>"},{"location":"security/data-at-rest-encryption.html#innodb-undo-tablespace-encryption","title":"InnoDB Undo Tablespace Encryption","text":"<p>This feature was in tech preview from version Percona Server 5.7.23-24 but is deprecated from version Percona Server for MySQL 5.7.32-35. This feature is not recommended to be used in production.</p> <p>The encryption of InnoDB Undo tablespaces is only available when using separate undo tablespaces. Otherwise, the InnoDB undo log is part of the InnoDB system tablespace.</p>"},{"location":"security/data-at-rest-encryption.html#system-variables_1","title":"System variables","text":""},{"location":"security/data-at-rest-encryption.html#innodb_undo_log_encrypt","title":"<code>innodb_undo_log_encrypt</code>","text":"Option Description Command-line \u2013innodb-undo-log-encrypt Scope Global Dynamic Yes Data type Boolean Default Off <p>The variable has been implemented in Percona Server 5.7.23-24 and deprecated in Percona Server for MySQL 5.7.32-35. Enables the encryption of InnoDB Undo tablespaces. You can enable encryption and disable encryption while the server is running.</p> <p>Note</p> <p>If you enable undo log encryption, the server writes encryption information into the header. That information stays in the header during the life of the undo log. If you restart the server, the server will try to load the encryption key from the keyring during startup. If the keyring is not available, the server cannot start.</p>"},{"location":"security/data-at-rest-encryption.html#redo-log-encryption","title":"Redo Log Encryption","text":"<p>This feature was in tech preview from version 5.7.23-24 but is deprecated from version 5.7.32-35. This feature is not recommended to be used in production.</p> <p>InnoDB redo log encryption is enabled by setting the variable innodb_redo_log_encrypt. This variable has three values: <code>MASTER_KEY</code>, <code>KEYRING_KEY</code> and <code>OFF</code> (set by default).</p> <p><code>MASTER_KEY</code> uses the InnoDB master key to encrypt with unique keys for each log file in the redo log header.</p> <p><code>KEYRING_KEY</code> uses the <code>percona_redo</code> versioned key from the keyring. When innodb_redo_log_encrypt is set to <code>KEYRING_KEY</code>, each new redo log file is encrypted with the latest <code>percona_redo</code> key from the keyring.</p>"},{"location":"security/data-at-rest-encryption.html#system-variables_2","title":"System variables","text":"<p>Implemented in version Percona Server for MySQL 5.7.27-30, the key rotation is redesigned to allow <code>SELECT rotate_system_key(\"percona_redo)</code>. The currently used key version is available in the innodb_redo_key_version status. The feature is Experimental.</p>"},{"location":"security/data-at-rest-encryption.html#data-scrubbing","title":"Data Scrubbing","text":"<p>This feature was in tech preview from version Percona Server 5.7.23-24 but is deprecated from version Percona Server for MySQL 5.7.32-35. This feature is not recommended to be used in production.</p> <p>While data encryption ensures that the existing data are not stored in plain form, the data scrubbing literally removes the data once the user decides they should be deleted. Compare this behavior with how the <code>DELETE</code> statement works which only marks the affected data as deleted - the space claimed by this data is overwritten with new data later.</p> <p>Once enabled, data scrubbing works automatically on each tablespace separately. To enable data scrubbing, you need to set the following variables:</p> <ul> <li> <p>innodb-background-scrub-data-uncompressed</p> </li> <li> <p>innodb-background-scrub-data-compressed</p> </li> </ul> <p>Uncompressed tables can also be scrubbed immediately, independently of key rotation or background threads. This can be enabled by setting the variable innodb-immediate-scrub-data-uncompressed. This option is not supported for compressed tables.</p> <p>Note that data scrubbing is made effective by setting the innodb_online_encryption_threads variable to a value greater than zero.</p>"},{"location":"security/data-at-rest-encryption.html#system-variables_3","title":"System Variables","text":""},{"location":"security/data-at-rest-encryption.html#innodb_background_scrub_data_compressed","title":"<code>innodb_background_scrub_data_compressed</code>","text":"Option Description Command-line \u2013innodb-background-scrub-data-compressed Scope Global Dynamic Yes Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.23-24 and deprecated in Percona Server for MySQL 5.7.32-35.</p>"},{"location":"security/data-at-rest-encryption.html#innodb_background_scrub_data_uncompressed","title":"<code>innodb_background_scrub_data_uncompressed</code>","text":"Option Description Command-line \u2013innodb-background-scrub-data-uncompressed Scope Global Dynamic Yes Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.23-24 and deprecated in Percona Server for MySQL 5.7.32-35.</p>"},{"location":"security/data-at-rest-encryption.html#variables","title":"Variables","text":"<p>keyring_vault_config - Defines the location of the Loading the Keyring Plugin configuration file.</p> Option Description Command line \u2013keyring-vault-config Dynamic Yes Scope Global Variable Type Text Default <p>keyring_vault_timeout - Set the duration in seconds for the Vault server connection timeout. The default value is <code>15</code>. The allowed range is from <code>0</code> to <code>86400</code>. To wait an infinite amount of time set the variable to <code>0</code>.</p> Option Description Command line \u2013keyring-vault-timeout Dynamic Yes Scope Global Variable Type Numeric Default 15"},{"location":"security/data-at-rest-encryption.html#innodb_encrypt_tables","title":"<code>innodb_encrypt_tables</code>","text":"Option Description Command-line \u2013innodb-encrypt-tables Scope Global Dynamic Yes Data type Text Default OFF <p>The variable has been implemented in Percona Server 5.7.21-21.</p> <p>Note</p> <p>This variable is Experimental quality.</p>"},{"location":"security/data-at-rest-encryption.html#innodb_redo_log_encrypt","title":"<code>innodb_redo_log_encrypt</code>","text":"Option Description Command-line \u2013innodb-redo-log-encrypt Scope Global Dynamic Yes Data type Text Default OFF <p>The variable has been implemented in Percona Server 5.7.23-24. Enables the encryption of the redo log.</p>"},{"location":"security/data-at-rest-encryption.html#innodb_scrub_log","title":"<code>innodb_scrub_log</code>","text":"Option Description Command-line \u2013innodb-scrub-log Scope Global Dynamic Yes Data type Boolean Default OFF <p>The variable has been implemented in Percona Server 5.7.23-24. Specifies if data scrubbing should be automatically applied to the redo log.</p>"},{"location":"security/data-at-rest-encryption.html#innodb_scrub_log_speed","title":"<code>innodb_scrub_log_speed</code>","text":"Option Description Command-line \u2013innodb-scrub-log-speed Scope Global Dynamic Yes Data type Text Default <p>The variable has been implemented in Percona Server 5.7.23-24. Specifies the velocity of data scrubbing (writing dummy redo log records) in bytes per second.</p>"},{"location":"security/data-masking.html","title":"Data Masking","text":"<p>This feature was implemented in Percona Server for MySQL version Percona Server for MySQL 5.7.32-35.</p> <p>The Percona Data Masking plugin is a free and Open Source implementation of the MySQL\u2019s data masking plugin. Data Masking provides a set of functions to hide sensitive data with modified content.</p> <p>Data masking can have either of the characteristics:</p> <ul> <li> <p>Generation of random data, such as an email address</p> </li> <li> <p>De-identify data by transforming the data to hide content</p> </li> </ul>"},{"location":"security/data-masking.html#installing-the-plugin","title":"Installing the plugin","text":"<p>The following command installs the plugin:</p> <pre><code>mysql&gt; INSTALL PLUGIN data_masking SONAME 'data_masking.so';\n</code></pre>"},{"location":"security/data-masking.html#data-masking-functions","title":"Data Masking functions","text":"<p>The data masking functions have the following categories:</p> <ul> <li> <p>General purpose</p> </li> <li> <p>Special purpose</p> </li> <li> <p>Generating Random Data with Defined characteristics</p> </li> <li> <p>Using Dictionaries to Generate Random Data</p> </li> </ul>"},{"location":"security/data-masking.html#general-purpose","title":"General Purpose","text":"<p>The general purpose data masking functions are the following:</p> <p>Parameter</p> <p>Description</p> <p>Sample</p> <p>mask_inner(string, margin1, margin2 [, character])</p> <p>Returns a result where only the inner part of a string is masked. An optional masking character can be specified.</p> <pre>mysql&gt; SELECT mask_inner('123456789', 1, 2);\n\n+-----------------------------------+\n| mask_inner(\"123456789\", 1, 2)     |\n+-----------------------------------+\n|1XXXXXX89                          |\n+-----------------------------------+\n</pre> <p>mask_outer(string, margin1, margin2 [, character])</p> <p>Masks the outer part of the string. The inner section is not masked.</p> <pre>mysql&gt; SELECT mask_outer('123456789', 2, 2);\n\n+------------------------------------+\n| mask_outer(\"123456789\", 2, 2).     |\n+------------------------------------+\n| XX34567XX                          |\n+------------------------------------+\n</pre>"},{"location":"security/data-masking.html#special-purpose","title":"Special Purpose","text":"<p>The special purpose data masking functions are as follows:</p>"},{"location":"security/data-masking.html#generating-random-data-for-specific-requirements","title":"Generating Random Data for Specific Requirements","text":"<p>The following functions generate random values for specific requirements:</p> <p>Parameter</p> <p>Description</p> <p>Sample</p> <p>gen_range(lower, upper)</p> <p>Generates a random number based on a selected range and supports negative numbers.</p> <pre>mysql&gt; SELECT gen_range(10, 100) AS result;\n\n+--------------------------------------+\n| result                               |\n+--------------------------------------+\n| 56                                   |\n+--------------------------------------+\n\nmysql&gt; SELECT gen_range(100,80);\n\n+--------------------------------------+\n| gen_range(100,80)                    |\n+--------------------------------------+\n| 91                                   |\n+--------------------------------------+\n</pre> <p>gen_rnd_email()</p> <p>Generates a random email address. The domain is <code>example.com</code>.</p> <pre>mysql&gt; SELECT gen_rnd_email();\n\n+---------------------------------------+\n| gen_rnd_email()                       |\n+---------------------------------------+\n| sma.jrts@example.com                  |\n+---------------------------------------+\n</pre> <p>gen_rnd_pan([size in integer])</p> <p>Generates a random primary account number. This function should only be used for test purposes.</p> <pre>mysql&gt; SELECT mask_pan(gen_rnd_pan());\n\n+-------------------------------------+\n| mask_pan(gen_rnd_pan())             |\n+-------------------------------------+\n| XXXXXXXXXXXX4444                    |\n+-------------------------------------+\n</pre> <p>gen_rnd_us_phone()</p> <p>Generates a random U.S. phone number. The generated number adds the 1 dialing code and is in the 555 area code. The 555 area code is not valid for any U.S. phone number.</p> <pre>mysql&gt; SELECT gen_rnd_us_phone();\n\n+-------------------------------+\n| gen_rnd_us_phone()            |\n+-------------------------------+\n| 1-555-635-5709                |\n+-------------------------------+\n</pre> <p>gen_rnd_ssn()</p> <p>Generates a random, non-legitimate US Social Security Number in an <code>AAA-BBB-CCCC</code> format. This function should only be used for test purposes.</p> <pre>mysql&gt; SELECT gen_rnd_ssn()\n\n+-----------------------------+\n| gen_rnd_ssn()               |\n+-----------------------------+\n| 995-33-5656                 |\n+-----------------------------+\n</pre>"},{"location":"security/data-masking.html#using-dictionaries-to-generate-random-terms","title":"Using Dictionaries to Generate Random Terms","text":"<p>Data masking returns a value from a range. To use a predefined file as the range to select a string value, load and use a dictionary. A dictionary supports only strings and is loaded from a file with the following characteristics:</p> <ul> <li> <p>Plain text</p> </li> <li> <p>One term per line</p> </li> <li> <p>Must contain at least one entry</p> </li> </ul> <p>An example of a dictionary, which is a list of trees, located in /usr/local/mysql/dict-files/testdict</p> <ul> <li> <p>Black Ash</p> </li> <li> <p>White Ash</p> </li> <li> <p>Bigtooth Aspen</p> </li> <li> <p>Quaking Aspen</p> </li> </ul> <p>The following table displays the commands for using dictionaries to generate random terms:</p> <p>Parameter</p> <p>Description</p> <p>Sample</p> <p>gen_range(lower, upper)</p> <p>Generates a random number based on a selected range and supports negative numbers.</p> <pre>mysql&gt; SELECT gen_range(10, 100) AS result;\n\n+--------------------------------------+\n| result                               |\n+--------------------------------------+\n| 56                                   |\n+--------------------------------------+\n\nmysql&gt; SELECT gen_range(100,80);\n\n+--------------------------------------+\n| gen_range(100,80)                    |\n+--------------------------------------+\n| 91                                   |\n+--------------------------------------+\n</pre> <p>gen_rnd_email()</p> <p>Generates a random email address. The domain is <code>example.com</code>.</p> <pre>mysql&gt; SELECT gen_rnd_email();\n\n+---------------------------------------+\n| gen_rnd_email()                       |\n+---------------------------------------+\n| sma.jrts@example.com                  |\n+---------------------------------------+\n</pre> <p>gen_rnd_pan([size in integer])</p> <p>Generates a random primary account number. This function should only be used for test purposes.</p> <pre>mysql&gt; SELECT mask_pan(gen_rnd_pan());\n\n+-------------------------------------+\n| mask_pan(gen_rnd_pan())             |\n+-------------------------------------+\n| XXXXXXXXXXXX4444                    |\n+-------------------------------------+\n</pre> <p>gen_rnd_us_phone()</p> <p>Generates a random U.S. phone number. The generated number adds the 1 dialing code and is in the 555 area code. The 555 area code is not valid for any U.S. phone number.</p> <pre>mysql&gt; SELECT gen_rnd_us_phone();\n\n+-------------------------------+\n| gen_rnd_us_phone()            |\n+-------------------------------+\n| 1-555-635-5709                |\n+-------------------------------+\n</pre> <p>gen_rnd_ssn()</p> <p>Generates a random, non-legitimate US Social Security Number in an <code>AAA-BBB-CCCC</code> format. This function should only be used for test purposes.</p> <pre>mysql&gt; SELECT gen_rnd_ssn()\n\n+-----------------------------+\n| gen_rnd_ssn()               |\n+-----------------------------+\n| 995-33-5656                 |\n+-----------------------------+\n</pre>"},{"location":"security/data-masking.html#uninstalling-the-plugin","title":"Uninstalling the plugin","text":"<p>The UNINSTALL PLUGIN statement disables and uninstalls the plugin.</p>"},{"location":"security/pam_plugin.html","title":"PAM Authentication Plugin","text":"<p>Percona PAM Authentication Plugin is a free and Open Source implementation of the MySQL\u2019s authentication plugin. This plugin acts as a mediator between the MySQL server, the MySQL client, and the PAM stack. The server plugin requests authentication from the PAM stack, forwards any requests and messages from the PAM stack over the wire to the client (in cleartext) and reads back any replies for the PAM stack.</p> <p>PAM plugin uses dialog as its client side plugin. Dialog plugin can be loaded to any client application that uses <code>libperconaserverclient</code>/<code>libmysqlclient</code> library.</p> <p>Here are some of the benefits that Percona dialog plugin offers over the default one:</p> <ul> <li> <p>It correctly recognizes whether PAM wants input to be echoed or not, while the default one always echoes the input on the user\u2019s console.</p> </li> <li> <p>It can use the password which is passed to MySQL client via \u201c-p\u201d parameter.</p> </li> <li> <p>Dialog client installation bug has been fixed.</p> </li> <li> <p>This plugin works on MySQL and Percona Server for MySQL.</p> </li> </ul> <p>Percona offers two versions of this plugin:</p> <ul> <li> <p>Full PAM plugin called auth_pam. This plugin uses dialog.so. It fully supports the PAM protocol with arbitrary communication between client and server.</p> </li> <li> <p>Oracle-compatible PAM called auth_pam_compat. This plugin uses mysql_clear_password which is a part of Oracle MySQL client. It also has some limitations, such as, it supports only one password input. You must use <code>-p</code> option in order to pass the password to auth_pam_compat.</p> </li> </ul> <p>These two versions of plugins are physically different. To choose which one you want used, you must use IDENTIFIED WITH \u2018auth_pam\u2019 for auth_pam, and IDENTIFIED WITH \u2018auth_pam_compat\u2019 for auth_pam_compat.</p>"},{"location":"security/pam_plugin.html#installation","title":"Installation","text":"<p>This plugin requires manual installation because it isn\u2019t installed by default.</p> <pre><code>mysqlINSTALL PLUGIN auth_pam SONAME 'auth_pam.so';\n</code></pre> <p>After the plugin has been installed it should be present in the plugins list. To check if the plugin has been correctly installed and active</p> <pre><code>mysqlSHOW PLUGINS;\n</code></pre> <p>The output should be include the following:</p> <pre><code>...\n...\n| auth_pam                       | ACTIVE   | AUTHENTICATION     | auth_pam.so | GPL     |\n</code></pre>"},{"location":"security/pam_plugin.html#configuration","title":"Configuration","text":"<p>In order to use the plugin, authentication method should be configured. Simple setup can be to use the standard UNIX authentication method (<code>pam_unix</code>).</p> <p>Note</p> <p>To use <code>pam_unix</code>, mysql will need to be added to the shadow group in order to have enough privileges to read the /etc/shadow.</p> <p>A sample /etc/pam.d/mysqld file:</p> <pre><code>auth       required     pam_unix.so\naccount    required     pam_unix.so\n</code></pre> <p>For added information in the system log, you can expand it to be:</p> <pre><code>auth       required     pam_warn.so\nauth       required     pam_unix.so audit\naccount    required     pam_unix.so audit\n</code></pre>"},{"location":"security/pam_plugin.html#creating-a-user","title":"Creating a user","text":"<p>After the PAM plugin has been configured, users can be created with the PAM plugin as authentication method</p> <pre><code>mysql&gt; CREATE USER 'newuser'@'localhost' IDENTIFIED WITH auth_pam;\n</code></pre> <p>This will create a user <code>newuser</code> that can connect from <code>localhost</code> who will be authenticated using the PAM plugin. If the <code>pam_unix</code> method is being used user will need to exist on the system.</p>"},{"location":"security/pam_plugin.html#supplementary-groups-support","title":"Supplementary groups support","text":"<p>Percona Server for MySQL has implemented PAM plugin support for supplementary groups. Supplementary or secondary groups are extra groups a specific user is member of. For example user <code>joe</code> might be a member of groups: <code>joe</code> (his primary group) and secondary groups <code>developers</code> and <code>dba</code>. A complete list of groups and users belonging to them can be checked with <code>cat /etc/group</code> command.</p> <p>This feature enables using secondary groups in the mapping part of the authentication string, like \u201c<code>mysql, developers=joe, dba=mark</code>\u201d. Previously only primary groups could have been specified there. If user is a member of both <code>developers</code> and <code>dba</code>, PAM plugin will map it to the <code>joe</code> because <code>developers</code> matches first.</p>"},{"location":"security/pam_plugin.html#known-issues","title":"Known issues","text":"<p>Default mysql stack size is not enough to handle <code>pam_ecryptfs</code> module. Workaround is to increase the MySQL stack size by setting the thread-stack variable to at least <code>512KB</code> or by increasing the old value by <code>256KB</code>.</p> <p>PAM authentication can fail with <code>mysqld: pam_unix(mysqld:account): Fork failed: Cannot allocate memory</code> error in the <code>/var/log/secure</code> even when there is enough memory available. Current workaround is to set vm.overcommit_memory to <code>1</code>:</p> <pre><code>echo 1 /proc/sys/vm/overcommit_memory\n</code></pre> <p>and by adding the <code>vm.overcommit_memory = 1</code> to <code>/etc/sysctl.conf</code> to make the change permanent after reboot. Authentication of internal (i.e. non PAM) accounts continues to work fine when <code>mysqld</code> reaches this memory utilization level. NOTE: Setting the <code>vm.overcommit_memory</code> to <code>1</code> will cause kernel to perform no memory overcommit handling which could increase the potential for memory overload and invoking of OOM killer.</p>"},{"location":"security/pam_plugin.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li>8.0.12-1 Feature ported from Percona Server for MySQL 5.7.</li> </ul>"},{"location":"security/ssl-improvement.html","title":"SSL Improvements","text":"<p>By default, Percona Server for MySQL passes elliptic-curve crypto-based ciphers to OpenSSL, such as ECDHE-RSA-AES128-GCM-SHA256.</p> <p>Note</p> <p>Although documented as supported, elliptic-curve crypto-based ciphers do not work with MySQL.</p>"},{"location":"tokudb/fast_updates.html","title":"Fast Updates with TokuDB","text":""},{"location":"tokudb/fast_updates.html#introduction","title":"Introduction","text":"<p>Update intensive applications can have their throughput limited by the random read capacity of the storage system. The cause of the throughput limit is the read-modify-write algorithm that MySQL uses to process update statements (read a row from the storage engine, apply the updates to it, write the new row back to the storage engine).</p> <p>To address this throughput limit, TokuDB provides an experimental fast update feature, which uses a different update algorithm. Update expressions of the SQL statement are encoded into tiny programs that are stored in an update Fractal Tree message. This update message is injected into the root of the Fractal Tree index. Eventually, these update  messages reach a leaf node, where the update programs are applied to the row. Since messages are moved between Fractal Tree levels in batches, the cost of reading in the leaf node is amortized over many update messages.</p> <p>This feature is available for <code>UPDATE</code> and <code>INSERT</code> statements, and can be turned ON/OFF separately for them with use of two variables. Variable tokudb_enable_fast_update variable toggles fast updates for the <code>UPDATE</code>, and  tokudb_enable_fast_upsert does the same  for <code>INSERT</code>.</p>"},{"location":"tokudb/fast_updates.html#limitations","title":"Limitations","text":"<p>Fast updates are activated instead of normal MySQL read-modify-write updates if the executed expression meets the number of conditions.</p> <ul> <li> <p>fast updates can be activated for a statement or a mixed replication,</p> </li> <li> <p>a primary key must be defined for the involved table,</p> </li> <li> <p>both simple and compound primary keys are supported, and <code>int</code>, <code>char</code> or <code>varchar</code> are the allowed data types for them,</p> </li> <li> <p>updated fields should have <code>Integer</code> or <code>char</code> data type,</p> </li> <li> <p>fields that are part of any key should be not updated,</p> </li> <li> <p>clustering keys are not allowed,</p> </li> <li> <p>triggers should be not involved,</p> </li> <li> <p>supported update expressions should belong to one of the following types:</p> <ul> <li> <p><code>x = constant</code></p> </li> <li> <p><code>x = x + constant</code></p> </li> <li> <p><code>x = x - constant</code></p> </li> <li> <p><code>x = if (x=0,0,x-1)</code></p> </li> <li> <p><code>x = x + values</code></p> </li> </ul> </li> </ul>"},{"location":"tokudb/fast_updates.html#usage-specifics-and-examples","title":"Usage Specifics and Examples","text":"<p>Following example creates a table that associates event identifiers with their count:</p> <pre><code>CREATE TABLE t (\nevent_id bigint unsigned NOT NULL PRIMARY KEY,\nevent_count bigint unsigned NOT NULL\n);\n</code></pre> <p>Many graph applications that map onto relational tables can use duplicate key inserts and updates to maintain the graph. For example, one can update the meta-data associated with a link in the graph using duplicate key insertions. If the affected rows is not used by the application, then the insertion or update can be marked and executed as a fast insertion or a fast update.</p>"},{"location":"tokudb/fast_updates.html#insertion-example","title":"Insertion example","text":"<p>If it is not known if the event identifier (represented by event_id) already exists in the table, then <code>INSERT ... ON DUPLICATE KEY UPDATE ...</code> statement can insert it if not existing, or increment its event_count otherwise. Here is an example with duplicate key insertion statement, where <code>%id</code> is some specific event_id value:</p> <pre><code>INSERT INTO t VALUES (%id, 1)\nON DUPLICATE KEY UPDATE event_count=event_count+1;\n</code></pre>"},{"location":"tokudb/fast_updates.html#explanation","title":"Explanation","text":"<p>If the event id\u2019s are random, then the throughput of this application would be limited by the random read capacity of the storage system since each <code>INSERT</code> statement has to determine if this event_id exists in the table.</p> <p>TokuDB replaces the primary key existence check with an insertion of an \u201cupsert\u201d message into the Fractal Tree index. This \u201cupsert\u201d message contains a copy of the row and a program that increments event_count. As the Fractal Tree buffer\u2019s get filled, this \u201cupsert\u201d message is flushed down the tree. Eventually, the message reaches a leaf node and gets executed there. If the key exists in the leaf node, then the event_count is incremented. Otherwise, the new row is inserted into the leaf node.</p>"},{"location":"tokudb/fast_updates.html#update-example","title":"Update example","text":"<p>If event_id is known to exist in the table, then <code>UPDATE</code> statement can be used to increment its event_count (once again, specific event_id value is written here as <code>%id</code>):</p> <pre><code>UPDATE t SET event_count=event_count+1\nWHERE event_id=%id;\n</code></pre>"},{"location":"tokudb/fast_updates.html#explanation_1","title":"Explanation","text":"<p>TokuDB generates an \u201cupdate\u201d message from the <code>UPDATE</code> statement and its update expression trees, and inserts this message into the Fractal Tree index. When the message eventually reaches the leaf node, the increment program is extracted from the message and executed.</p>"},{"location":"tokudb/removing_tokudb.html","title":"Removing TokuDB storage engine","text":"<p>In case you want remove the TokuDB storage engine from Percona Server for MySQL without causing any errors, the following is the recommended procedure:</p>"},{"location":"tokudb/removing_tokudb.html#change-the-tables-from-tokudb-to-innodb","title":"Change the tables from TokuDB to InnoDB","text":"<p>If you still need the data in the TokuDB tables you must alter the tables to another supported storage engine, i.e., InnoDB:</p> <pre><code>mysql&gt; ALTER TABLE City ENGINE=InnoDB;\n</code></pre> <p>Note</p> <p>Removing the TokuDB storage engine before you have changed your tables to another supported storage engine closes access to that data. You must re-install the TokuDB storage engine to regain access.</p>"},{"location":"tokudb/removing_tokudb.html#removing-the-plugins","title":"Removing the plugins","text":"<p>One option is to remove the TokuDB storage engine with all installed plugins by running the <code>ps-admin</code> script:</p> <pre><code>ps-admin --disable-tokudb -uroot -pPassw0rd\n</code></pre> <p>The script output should look like this:</p> <pre><code>Checking if Percona server is running with jemalloc enabled...\n&gt;&gt; Percona server is running with jemalloc enabled.\n\nChecking transparent huge pages status on the system...\n&gt;&gt; Transparent huge pages are currently disabled on the system.\n\nChecking if thp-setting=never option is already set in config file...\n&gt;&gt; Option thp-setting=never is set in the config file.\n\nChecking TokuDB plugin status...\n&gt;&gt; TokuDB plugin is installed.\n\nRemoving thp-setting=never option from /etc/mysql/my.cnf\n&gt;&gt; Successfully removed thp-setting=never option from /etc/mysql/my.cnf\n\nUninstalling TokuDB plugin...\n&gt;&gt; Successfully uninstalled TokuDB plugin.\n</code></pre> <p>Note</p> <p>The ps-admin removal may not restore the Transparent Huge Pages (THP) to the original operating system default state. You may have the following result:</p> <pre><code>$ cat /sys/kernel/mm/transparent_hugepage/enabled\n</code></pre> <p>The output could be the following:</p> <pre><code>always madvise [never]\n</code></pre> <p>On many operating systems, the default state is <code>[always]</code>. To enable transparent huge pages, run the following:</p> <pre><code>echo always &gt; /sys/kernel/mm/transparent_hugepage/enabled\n</code></pre> <p>Another option is to manually remove the TokuDB storage engine with all installed plugins:</p> <pre><code>UNINSTALL PLUGIN tokudb;\nUNINSTALL PLUGIN tokudb_file_map;\nUNINSTALL PLUGIN tokudb_fractal_tree_info;\nUNINSTALL PLUGIN tokudb_fractal_tree_block_map;\nUNINSTALL PLUGIN tokudb_trx;\nUNINSTALL PLUGIN tokudb_locks;\nUNINSTALL PLUGIN tokudb_lock_waits;\nUNINSTALL PLUGIN tokudb_background_job_status;\n</code></pre> <p>After the engine and the plugins have been uninstalled you can remove the TokuDB package by using the apt/yum commands:</p> <pre><code>[root@centos ~]# yum remove Percona-Server-tokudb-57.x86_64\n</code></pre> <p>or</p> <pre><code>root@wheezy:~# apt remove percona-server-tokudb-5.7\n</code></pre> <p>Note</p> <p>Make sure you\u2019ve removed all the TokuDB specific variables from your configuration file (<code>my.cnf</code>) before you restart the server, otherwise server could show errors or warnings and will not start.</p>"},{"location":"tokudb/toku_backup.html","title":"Percona TokuBackup","text":"<p>Percona TokuBackup is an open-source hot backup utility for MySQL servers running the TokuDB storage engine (including Percona Server for MySQL and MariaDB). It does not lock your database during backup. The TokuBackup library intercepts system calls that write files and duplicates the writes to the backup directory.</p> <p>Note</p> <p>This feature is currently considered Experimental</p>"},{"location":"tokudb/toku_backup.html#installing-from-binaries","title":"Installing From Binaries","text":"<p>TokuBackup is included with Percona Server for MySQL Percona Server for MySQL 5.7.10-1 and later versions. Installation can be performed with the <code>ps-admin</code> script.</p> <p>To install Percona TokuBackup:</p> <ol> <li> <p>Run <code>ps-admin --enable-tokubackup</code> to add the <code>preload-hotbackup</code> option into [mysqld_safe] section of <code>my.cnf</code>.</p> <pre><code>$ sudo ps-admin --enable-tokubackup\n</code></pre> <p>The output should be the following:</p> <pre><code>Checking SELinux status...\nINFO: SELinux is disabled.\n\nChecking if preload-hotbackup option is already set in config file...\nINFO: Option preload-hotbackup is not set in the config file.\n\nChecking TokuBackup plugin status...\nINFO: TokuBackup plugin is not installed.\n\nAdding preload-hotbackup option into /etc/my.cnf\nINFO: Successfully added preload-hotbackup option into /etc/my.cnf\nPLEASE RESTART MYSQL SERVICE AND RUN THIS SCRIPT AGAIN TO FINISH INSTALLATION!\n</code></pre> </li> <li> <p>Restart mysql service</p> <pre><code>$ sudo service mysql restart\n</code></pre> </li> <li> <p>Run <code>ps-admin --enable-tokubackup</code> again to finish installation of TokuBackup plugin</p> <pre><code>$ sudo ps-admin --enable-tokubackup\n</code></pre> <p>The output should be the following:</p> <pre><code>Checking SELinux status...\nINFO: SELinux is disabled.\n\nChecking if preload-hotbackup option is already set in config file...\nINFO: Option preload-hotbackup is set in the config file.\n\nChecking TokuBackup plugin status...\nINFO: TokuBackup plugin is not installed.\n\nChecking if Percona Server is running with libHotBackup.so preloaded...\nINFO: Percona Server is running with libHotBackup.so preloaded.\n\nInstalling TokuBackup plugin...\nINFO: Successfully installed TokuBackup plugin.\n</code></pre> </li> </ol>"},{"location":"tokudb/toku_backup.html#making-a-backup","title":"Making a Backup","text":"<p>To run Percona TokuBackup, the backup destination directory must exist, be writable and owned by the same user under which MySQL server is running (usually <code>mysql</code>) and empty. Once this directory is created, the backup can be run using the following command:</p> <pre><code>mysqlset tokudb_backup_dir='/path_to_empty_directory';\n</code></pre> <p>Note</p> <p>Setting the tokudb_backup_dir variable automatically starts the backup process to the specified directory. Percona TokuBackup will take full backup each time, currently there is no incremental backup option</p> <p>If you get any error on this step (for example, caused by some misconfiguration), the Reporting Errors section explains how to find out the reason.</p>"},{"location":"tokudb/toku_backup.html#restoring-from-backup","title":"Restoring From Backup","text":"<p>Percona TokuBackup does not have any functionality for restoring a backup. You can use rsync or cp to restore the files. You should check that the restored files have the correct ownership and permissions.</p> <p>Note</p> <p>Make sure that the datadir is empty and that MySQL server is shut down before restoring from backup. You can\u2019t restore to a datadir of a running mysqld instance (except when importing a partial backup).</p> <p>The following example shows how you might use the rsync command to restore the backup:</p> <pre><code>$ rsync -avrP /data/backup/ /var/lib/mysql/\n</code></pre> <p>Since attributes of files are preserved, in most cases you will need to change their ownership to mysql before starting the database server. Otherwise, the files will be owned by the user who created the backup.</p> <pre><code>$ chown -R mysql:mysql /var/lib/mysql\n</code></pre> <p>If you have changed default TokuDB data directory (tokudb_data_dir) or TokuDB log directory (tokudb_log_dir) or both of them, you will see separate folders for each setting in backup directory after taking backup. You\u2019ll need to restore each folder separately:</p> <pre><code>$ rsync -avrP /data/backup/mysql_data_dir/ /var/lib/mysql/\n$ rsync -avrP /data/backup/tokudb_data_dir/ /path/to/original/tokudb_data_dir/\n$ rsync -avrP /data/backup/tokudb_log_dir/ /path/to/original/tokudb_log_dir/\n$ chown -R mysql:mysql /var/lib/mysql\n$ chown -R mysql:mysql /path/to/original/tokudb_data_dir\n$ chown -R mysql:mysql /path/to/original/tokudb_log_dir\n</code></pre>"},{"location":"tokudb/toku_backup.html#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"tokudb/toku_backup.html#monitoring-progress","title":"Monitoring Progress","text":"<p>TokuBackup updates the PROCESSLIST state while the backup is in progress. You can see the output by running <code>SHOW PROCESSLIST</code> or <code>SHOW FULL PROCESSLIST</code>.</p>"},{"location":"tokudb/toku_backup.html#excluding-source-files","title":"Excluding Source Files","text":"<p>You can exclude certain files and directories based on a regular expression set in the tokudb_backup_exclude session variable. If the source file name matches the excluded regular expression, then the source file is excluded from backup.</p> <p>For example, to exclude all <code>lost+found</code> directories from backup, use the following command:</p> <pre><code>mysqlSET tokudb_backup_exclude='/lost\\\\+found($|/)';\n</code></pre> <p>Note</p> <p>In Percona Server for MySQL Percona Server for MySQL 5.7.10-3 to address bug #125, server <code>pid</code> file is excluded by default. If you\u2019re providing your own additions to the exclusions and have the <code>pid</code> file in the default location, you will need to add the mysqld_safe.pid entry.</p>"},{"location":"tokudb/toku_backup.html#throttling-backup-rate","title":"Throttling Backup Rate","text":"<p>You can throttle the backup rate using the tokudb_backup_throttle session-level variable. This variable throttles the write rate in bytes per second of the backup to prevent TokuBackup from crowding out other jobs in the system. The default and max value is 18446744073709551615.</p> <pre><code>mysqlSET tokudb_backup_throttle=1000000;\n</code></pre>"},{"location":"tokudb/toku_backup.html#restricting-backup-target","title":"Restricting Backup Target","text":"<p>You can restrict the location of the destination directory where the backups can be located using the tokudb_backup_allowed_prefix system-level variable. Attempts to backup to a location outside of the specified directory or its children will result in an error.</p> <p>The default is <code>null</code>, backups have no restricted locations. This read-only variable can be set in the <code>my.cnf</code> configuration file and displayed with the <code>SHOW VARIABLES</code> command:</p> <pre><code>mysqlSHOW VARIABLES LIKE 'tokudb_backup_allowed_prefix';\n</code></pre> <p>The output should be the following:</p> <pre><code>+------------------------------+-----------+\n| Variable_name                | Value     |\n+------------------------------+-----------+\n| tokudb_backup_allowed_prefix | /dumpdir  |\n+------------------------------+-----------+\n</code></pre>"},{"location":"tokudb/toku_backup.html#reporting-errors","title":"Reporting Errors","text":"<p>Percona TokuBackup uses two variables to capture errors. They are tokudb_backup_last_error and tokudb_backup_last_error_string. When TokuBackup encounters an error, these will report on the error number and the error string respectively. For example, the following output shows these parameters following an attempted backup to a directory that was not empty:</p> <pre><code>mysqlSET tokudb_backup_dir='/tmp/backupdir';\n</code></pre> <p>The output could be the following:</p> <p><pre><code>ERROR 1231 (42000): Variable 'tokudb_backup_dir' can't be set to the value of '/tmp/backupdir'\n</code></pre> <pre><code>mysqlSELECT @@tokudb_backup_last_error;\n</code></pre></p> <p>The output should be the following:</p> <p><pre><code>+----------------------------+\n| @@tokudb_backup_last_error |\n+----------------------------+\n|                         17 |\n+----------------------------+\n</code></pre> <pre><code>mysqlSELECT @@tokudb_backup_last_error_string;\n</code></pre></p> <p>The output should be the following:</p> <pre><code>+---------------------------------------------------+\n| @@tokudb_backup_last_error_string                 |\n+---------------------------------------------------+\n| tokudb backup couldn't create needed directories. |\n+---------------------------------------------------+\n</code></pre>"},{"location":"tokudb/toku_backup.html#create-a-backup-with-a-timestamp","title":"Create a Backup with a Timestamp","text":"<p>If you plan to store more than one backup in a location, you should add a timestamp to the backup directory name.</p> <p>A sample Bash script has this information:</p> <pre><code>#!/bin/bash\ntm=$(date \"+%Y-%m-%d-%H-%M-%S\");\nbackup_dir=$PWD/backup/$tm;\nmkdir -p $backup_dir;\nbin/mysql -uroot -e \"set tokudb_backup_dir='$backup_dir'\"\n</code></pre>"},{"location":"tokudb/toku_backup.html#using-tokudb-hot-backup-for-replication","title":"Using TokuDB Hot Backup for Replication","text":"<p>TokuDB Hot Backup makes a transactionally consistent copy of the TokuDB files while applications read and write to these files. The TokuDB hot backup library intercepts certain system calls that writes files and duplicates the writes on backup files while copying files to the backup directory. The copied files contain the same content as the original files.</p> <p>TokuDB Hot Backup also has an API. This API includes the <code>start capturing</code> and <code>stop capturing</code> commands. The \u201ccapturing\u201d command starts the process, when a portion of a file is copied to the backup location, and this portion is changed, these changes are also applied to the backup location.</p> <p>Replication often uses backup replication to create replicas. You must know the last executed global transaction identifier (GTID) or binary log position both for the replica and source configuration.</p> <p>To lock tables, use <code>FLUSH TABLE WITH READ LOCK</code> or use the smart locks like <code>LOCK TABLES FOR BACKUP</code> or <code>LOCK BINLOG FOR BACKUP</code>.</p> <p>During the copy process, the binlog is flushed, and the changes are copied to backup by the \u201ccapturing\u201d mechanism. After everything has been copied, and the \u201ccapturing\u201d mechanism is still running, use the <code>LOCK BINLOG FOR BACKUP</code>. After this statement is executed, the binlog is flushed, the changes are captured, and any queries that could change the binlog position or executed GTID are blocked.</p> <p>After this command, we can stop capturing and retrieve the last executed GTID or binlog log position and unlock the binlog.</p> <p>After a backup is taken, there are the following files in the backup directory:</p> <ul> <li> <p>tokubackup_slave_info</p> </li> <li> <p>tokubackup_binlog_info</p> </li> </ul> <p>These files contain information for replica and source. You can use this information to start a new replica from the source or replica.</p> <p>The <code>SHOW MASTER STATUS</code> and <code>SHOW SLAVE STATUS</code> commands provide the information.</p> <p>In specific binlog formats, a binary log event can contain statements that produce temporary tables on the replica side, and the result of further statements may depend on the temporary table content. Typically, temporary tables are not selected for backup because they are created in a separate directory. A backup created with temporary tables created by binlog events can cause issues when restored because the temporary tables are not restored. The data may be inconsistent.</p> <p>The following system variables \u2013tokudb-backup-safe-slave, which enables or disables the safe-slave mode, and \u2013tokudb-backup-safe-slave-timeout, which defines the maximum amount of time in seconds to wait until temporary tables disappear.  The <code>safe-slave</code> mode, when used with <code>LOCK BINLOG FOR BACKUP</code>, the replica SQL thread is stopped and checked to see if temporary tables produced by the replica exist or do not exist. If temporary tables exist, the replica SQL thread is restarted until there are no temporary tables or a defined timeout is reached.</p> <p>You should not use this option for group-replication.</p>"},{"location":"tokudb/toku_backup.html#limitations-and-known-issues","title":"Limitations and known issues","text":"<ul> <li> <p>You must disable InnoDB asynchronous IO if backing up InnoDB tables with TokuBackup. Otherwise you will have inconsistent, unrecoverable backups. The appropriate setting is <code>innodb_use_native_aio=0</code>.</p> </li> <li> <p>To be able to run Point-In-Time-Recovery you\u2019ll need to manually get the binary log position.</p> </li> <li> <p>Transactional storage engines (TokuDB and InnoDB) will perform recovery on the backup copy of the database when it is first started.</p> </li> <li> <p>Tables using non-transactional storage engines (MyISAM) are not locked during the copy and may report issues when starting up the backup. It is best to avoid operations that modify these tables at the end of a hot backup operation (adding/changing users, stored procedures, etc.).</p> </li> <li> <p>The database is copied locally to the path specified in <code>/path/to/backup</code>. This folder must exist, be writable, be empty, and contain enough space for a full copy of the database.</p> </li> <li> <p>TokuBackup always makes a backup of the MySQL datadir and optionally the tokudb_data_dir, tokudb_log_dir, and the binary log folder. The latter three are only backed up separately if they are not the same as or contained in the MySQL datadir. None of these three folders can be a parent of the MySQL datadir.</p> </li> <li> <p>No other directory structures are supported. All InnoDB, MyISAM, and other storage engine files must be within the MySQL datadir.</p> </li> <li> <p>TokuBackup does not follow symbolic links.</p> </li> <li> <p>TokuBackup does not backup MySQL configuration file(s).</p> </li> <li> <p>TokuBackup does not backup tablespaces if they are out of datadir.</p> </li> <li> <p>Due to upstream bug #80183, TokuBackup can\u2019t recover backed-up table data if backup was taken while running <code>OPTIMIZE TABLE</code> or <code>ALTER TABLE ... TABLESPACE</code>.</p> </li> <li> <p>TokuBackup doesn\u2019t support incremental backups.</p> </li> </ul>"},{"location":"tokudb/tokudb_background_analyze_table.html","title":"TokuDB Background ANALYZE TABLE","text":"<p>Percona Server for MySQL has an option to automatically analyze tables in the background based on a measured change in data. This has been done by implementing the background job manager that can perform operations on a background thread.</p>"},{"location":"tokudb/tokudb_background_analyze_table.html#background-jobs","title":"Background Jobs","text":"<p>Background jobs and schedule are transient in nature and are not persisted anywhere. Any currently running job will be terminated on shutdown and all scheduled jobs will be forgotten about on server restart. There can\u2019t be two jobs on the same table scheduled or running at any one point in time. If you manually invoke an <code>ANALYZE TABLE</code> that conflicts with either a pending or running job, the running job will be canceled and the users task will run immediately in the foreground. All the scheduled and running background jobs can be viewed by querying the TOKUDB_BACKGROUND_JOB_STATUS table.</p> <p>New tokudb_analyze_in_background variable has been implemented in order to control if the <code>ANALYZE TABLE</code> will be dispatched to the background process or if it will be running in the foreground. To control the function of <code>ANALYZE TABLE</code> a new tokudb_analyze_mode variable has been implemented. This variable offers options to cancel any running or scheduled job on the specified table (<code>TOKUDB_ANALYZE_CANCEL</code>), use existing analysis algorithm (<code>TOKUDB_ANALYZE_STANDARD</code>), or to recount the logical rows in table and update persistent count (<code>TOKUDB_ANALYZE_RECOUNT_ROWS</code>).</p> <p><code>TOKUDB_ANALYZE_RECOUNT_ROWS</code> is a new mechanism that is used to perform a logical recount of all rows in a table and persist that as the basis value for the table row estimate. This mode was added for tables that have been upgraded from an older version of TokuDB that only reported physical row counts and never had a proper logical row count. Newly created tables/partitions will begin counting logical rows correctly from their creation and should not need to be recounted unless some odd edge condition causes the logical count to become inaccurate over time. This analysis mode has no effect on the table cardinality counts. It will take the currently set session values for tokudb_analyze_in_background, and tokudb_analyze_throttle. Changing the global or session instances of these values after scheduling will have no effect on the job.</p> <p>Any background job, both pending and running, can be canceled by setting the tokudb_analyze_mode to <code>TOKUDB_ANALYZE_CANCEL</code> and issuing the <code>ANALYZE TABLE</code> on the table for which you want to cancel all the jobs for.</p>"},{"location":"tokudb/tokudb_background_analyze_table.html#auto-analysis","title":"Auto analysis","text":"<p>To implement the background analysis and gathering of cardinality statistics on a TokuDB tables new <code>delta</code> value is now maintained in memory for each TokuDB table. This value is not persisted anywhere and it is reset to <code>0</code> on a server start. It is incremented for each <code>INSERT/UPDATE/DELETE</code> command and ignores the impact of transactions (rollback specifically). When this delta value exceeds the tokudb_auto_analyze percentage of rows in the table an analysis is performed according to the current session\u2019s settings. Other analysis for this table will be disabled until this analysis completes. When this analysis completes, the delta is reset to <code>0</code> to begin recalculating table changes for the next potential analysis.</p> <p>Status values are now reported to server immediately upon completion of any analysis (previously new status values were not used until the table has been closed and re-opened). Half-time direction reversal of analysis has been implemented, meaning that if a tokudb_analyze_time is in effect and the analysis has not reached the half way point of the index by the time tokudb_analyze_time/2 has been reached: it will stop the forward progress and restart the analysis from the last/rightmost row in the table, progressing leftwards and keeping/adding to the status information accumulated from the first half of the scan.</p> <p>For small ratios of <code>table_rows</code> / tokudb_auto_analyze, auto analysis will be run for almost every change. The trigger formula is: <code>if (table_delta &gt;= ((table_rows \\* tokudb_auto_analyze) / 100))</code> then run <code>ANALYZE TABLE</code>. If a user manually invokes an <code>ANALYZE TABLE</code> and tokudb_auto_analyze is enabled and there are no conflicting background jobs, the users <code>ANALYZE TABLE</code> will behave exactly as if the delta level has been exceeded in that the analysis is executed and delta reset to <code>0</code> upon completion.</p>"},{"location":"tokudb/tokudb_background_analyze_table.html#system-variables","title":"System Variables","text":""},{"location":"tokudb/tokudb_background_analyze_table.html#tokudb_analyze_in_background","title":"<code>tokudb_analyze_in_background</code>","text":"Option Description Command-line Yes Config file Yes Scope Global/Session Dynamic Yes Data type Boolean Default ON <p>When this variable is set to <code>ON</code>  it will dispatch any <code>ANALYZE TABLE</code> job to a background process and return immediately, otherwise <code>ANALYZE TABLE</code> will run in foreground/client context.</p>"},{"location":"tokudb/tokudb_background_analyze_table.html#tokudb_analyze_mode","title":"<code>tokudb_analyze_mode</code>","text":"Option Description Command-line Yes Config file Yes Scope Global/Session Dynamic Yes Data type ENUM Default TOKUDB_ANALYZE_STANDARD Range TOKUDB_ANALYZE_CANCEL, TOKUDB_ANALYZE_STANDARD, TOKUDB_ANALYZE_RECOUNT_ROWS <p>This variable is used to control the function of <code>ANALYZE TABLE</code>. Possible values are:</p> <ul> <li> <p><code>TOKUDB_ANALYZE_CANCEL</code> - Cancel any running or scheduled job on the specified table.</p> </li> <li> <p><code>TOKUDB_ANALYZE_STANDARD</code> - Use existing analysis algorithm. This is the standard table cardinality analysis mode used to obtain cardinality statistics for a tables and its indexes. It will take the currently set session values for tokudb_analyze_time, tokudb_analyze_in_background, and tokudb_analyze_throttle at the time of its scheduling, either via a user invoked <code>ANALYZE TABLE</code> or an auto schedule as a result of tokudb_auto_analyze threshold being hit. Changing the global or session instances of these values after scheduling will have no effect on the scheduled job.</p> </li> <li> <p><code>TOKUDB_ANALYZE_RECOUNT_ROWS</code> - Recount logical rows in table and update persistent count. This is a new mechanism that is used to perform a logical recount of all rows in a table and persist that as the basis value for the table row estimate. This mode was added for tables that have been upgraded from an older version of TokuDB/PerconaFT that only reported physical row counts and never had a proper logical row count. Newly created tables/partitions will begin counting logical rows correctly from their creation and should not need to be recounted unless some odd edge condition causes the logical count to become inaccurate over time. This analysis mode has no effect on the table cardinality counts. It will take the currently set session values for tokudb_analyze_in_background, and tokudb_analyze_throttle. Changing the global or session instances of these values after scheduling will have no effect on the job.</p> </li> </ul>"},{"location":"tokudb/tokudb_background_analyze_table.html#tokudb_analyze_throttle","title":"<code>tokudb_analyze_throttle</code>","text":"Option Description Command-line Yes Config file Yes Scope Global/Session Dynamic Yes Data type Numeric Default 0 <p>This variable is used to define maximum number of keys to visit per second when performing <code>ANALYZE TABLE</code> with either a <code>TOKUDB_ANALYZE_STANDARD</code> or <code>TOKUDB_ANALYZE_RECOUNT_ROWS</code>.</p>"},{"location":"tokudb/tokudb_background_analyze_table.html#tokudb_analyze_time","title":"<code>tokudb_analyze_time</code>","text":"Option Description Command-line Yes Config file Yes Scope Global/Session Dynamic Yes Data type Numeric Default 5 <p>This session variable controls the number of seconds an analyze operation will spend on each index when calculating cardinality. Cardinality is shown by executing the following command:</p> <pre><code>SHOW INDEXES FROM table_name;\n</code></pre> <p>If an analyze is never performed on a table then the cardinality is <code>1</code> for primary key indexes and unique secondary indexes, and <code>NULL</code> (unknown) for all other indexes. Proper cardinality can lead to improved performance of complex SQL statements.</p>"},{"location":"tokudb/tokudb_background_analyze_table.html#tokudb_auto_analyze","title":"<code>tokudb_auto_analyze</code>","text":"Option Description Command-line Yes Config file Yes Scope Global/Session Dynamic Yes Data type Numeric Default 30 <p>Percentage of table change as <code>INSERT/UPDATE/DELETE</code> commands to trigger an <code>ANALYZE TABLE</code> using the current session tokudb_analyze_in_background, tokudb_analyze_mode, tokudb_analyze_throttle, and tokudb_analyze_time settings. If this variable is enabled and tokudb_analyze_in_background variable is set to <code>OFF</code>, analysis will be performed directly within the client thread context that triggered the analysis. </p> <p>Note</p> <p>InnoDB enabled this functionality by default when they introduced it. Due to the potential unexpected new load it might place on a server, it is disabled by default in TokuDB.</p>"},{"location":"tokudb/tokudb_background_analyze_table.html#tokudb_cardinality_scale_percent","title":"<code>tokudb_cardinality_scale_percent</code>","text":"Option Description Command-line Yes Config file Yes Scope Global/Session Dynamic Yes Data type Numeric Default 100 Range 0-100 <p>Percentage to scale table/index statistics when sending to the server to make an index appear to be either more or less unique than it actually is. InnoDB has a hard coded scaling factor of 50%. So if a table of 200 rows had an index with 40 unique values, InnoDB would return 200/40/2 or 2 for the index. The new TokuDB formula is the same but factored differently to use percent, for the same table.index (200/40 * tokudb_cardinality_scale) / 100, for a scale of 50% the result would also be 2 for the index.</p>"},{"location":"tokudb/tokudb_background_analyze_table.html#information_schema-tables","title":"INFORMATION_SCHEMA Tables","text":"<p><code>INFORMATION_SCHEMA.TOKUDB_BACKGROUND_JOB_STATUS</code></p> Column Name Description \u2018id\u2019 \u2018Simple monotonically incrementing job id, resets to 0 on server start.\u2019 \u2018database_name\u2019 \u2018Database name\u2019 \u2018table_name\u2019 \u2018Table name\u2019 \u2018job_type\u2019 \u2018Type of job, either TOKUDB_ANALYZE_STANDARD or TOKUDB_ANALYZE_RECOUNT_ROWS\u2019 \u2018job_params\u2019 \u2018Param values used by this job in string format. For example: TOKUDB_ANALYZE_DELETE_TIME=1.0; TOKUDB_ANALYZE_TIME=5; TOKUDB_ANALYZE_THROTTLE=2048;\u2019 \u2018scheduler\u2019 \u2018Either USER or AUTO to indicate if the job was explicitly scheduled by a user or if it was scheduled as an automatic trigger\u2019 \u2018scheduled_time\u2019 \u2018The time the job was scheduled\u2019 \u2018started_time\u2019 \u2018The time the job was started\u2019 \u2018status\u2019 \u2018Current job status if running. For example: ANALYZE TABLE standard db.tbl.idx 3 of 5 50% rows 10% time scanning forward\u2019 <p>This table holds the information on scheduled and running background <code>ANALYZE TABLE</code> jobs for TokuDB tables.</p>"},{"location":"tokudb/tokudb_background_analyze_table.html#version-specific-information","title":"Version Specific Information","text":"<ul> <li> <p>Percona Server for MySQL 5.7.10-1: Feature ported from Percona Server for MySQL 5.6</p> </li> <li> <p>Percona Server for MySQL 5.7.11-4: tokudb_analyze_in_background is now set to <code>ON</code> by default and tokudb_auto_analyze is set to <code>30</code></p> </li> </ul>"},{"location":"tokudb/tokudb_faq.html","title":"Frequently Asked Questions","text":"<p>This section contains frequently asked questions regarding TokuDB and related software.</p>"},{"location":"tokudb/tokudb_faq.html#transactional-operations","title":"Transactional Operations","text":"<p>What transactional operations does TokuDB support?</p> <p>TokuDB supports <code>BEGIN TRANSACTION</code>, <code>END TRANSACTION</code>, <code>COMMIT</code>, <code>ROLLBACK</code>, <code>SAVEPOINT</code>, and <code>RELEASE SAVEPOINT</code>.</p>"},{"location":"tokudb/tokudb_faq.html#tokudb-and-the-file-system","title":"TokuDB and the File System","text":"<p>How can I determine which files belong to the various tables and indexes in my schemas?</p> <p>The tokudb_file_map plugin lists all Fractal Tree Indexes and their corresponding data files. The <code>internal_file_name</code> is the actual file name (in the data folder).</p> <pre><code>mysqlSELECT * FROM information_schema.tokudb_file_map;\n</code></pre> <p>The output should be similar to the </p> <pre><code>+--------------------------+---------------------------------------+---------------+-------------+------------------------+\n| dictionary_name          | internal_file_name                    | table_schema  | table_name  | table_dictionary_name  |\n+--------------------------+---------------------------------------+---------------+-------------+------------------------+\n| ./test/tmc-key-idx_col2  | ./_test_tmc_key_idx_col2_a_14.tokudb  | test          | tmc         | key_idx_col2           |\n| ./test/tmc-main          | ./_test_tmc_main_9_14.tokudb          | test          | tmc         | main                   |\n| ./test/tmc-status        | ./_test_tmc_status_8_14.tokudb        | test          | tmc         | status                 |\n+--------------------------+---------------------------------------+---------------+-------------+------------------------+\n</code></pre>"},{"location":"tokudb/tokudb_faq.html#full-disks","title":"Full Disks","text":"<p>What happens when the disk system fills up?</p> <p>The disk system may fill up during bulk load operations, such as <code>LOAD DATA IN FILE</code> or <code>CREATE INDEX</code>, or during incremental operations like <code>INSERT</code>.</p> <p>In the bulk case, running out of disk space will cause the statement to fail with <code>ERROR 1030 (HY000): Got error 1 from storage engine</code>. The temporary space used by the bulk loader will be released. If this happens, you can use a separate physical disk for the temporary files (for more information, see tokudb_tmp_dir). If server runs out of free space TokuDB will assert the server to prevent data corruption to existing data files.</p> <p>Otherwise, disk space can run low during non-bulk operations. When available space is below a user-configurable reserve (5% by default) inserts are prevented and transactions that perform inserts are aborted. If the disk becomes completely full then TokuDB will freeze until some disk space is made available.</p> <p>Details about the disk system:</p> <ul> <li>There is a free-space reserve requirement, which is a user-configurable parameter given as a percentage of the total space in the file system. The default reserve is five percent. This value is available in the global variable tokudb_fs_reserve_percent. We recommend that this reserve be at least half the size of your physical memory.</li> </ul> <p>TokuDB polls the file system every five seconds to determine how much free space is available. If the free space dips below the reserve, then further table inserts are prohibited. Any transaction that attempts to insert rows will be aborted. Inserts are re-enabled when twice the reserve is available in the file system (so freeing a small amount of disk storage will not be sufficient to resume inserts). Warning messages are sent to the system error log when free space dips below twice the reserve and again when free space dips below the reserve.</p> <p>Even with inserts prohibited it is still possible for the file system to become completely full. For example this can happen because another storage engine or another application consumes disk space.</p> <ul> <li>If the file system becomes completely full, then TokuDB will freeze. It will not crash, but it will not respond to most SQL commands until some disk space is made available. When TokuDB is frozen in this state, it will still respond to the following command:</li> </ul> <pre><code>SHOW ENGINE TokuDB STATUS;\n</code></pre> <p>Make disk space available will allow the storage engine to continue running, but inserts will still be prohibited until twice the reserve is free.</p> <p>Note</p> <p>Engine status displays a field indicating if disk free space is above twice the reserve, below twice the reserve, or below the reserve. It will also display a special warning if the disk is completely full.</p> <ul> <li> <p>In order to make space available on this system you can:</p> </li> <li> <p>Add some disk space to the filesystem.</p> </li> <li> <p>Delete some non-TokuDB files manually.</p> </li> <li> <p>If the disk is not completely full, you may be able to reclaim space by aborting any transactions that are very old. Old transactions can consume large volumes of disk space in the recovery log.</p> </li> <li> <p>If the disk is not completely full, you can drop indexes or drop tables from your TokuDB databases.</p> </li> <li> <p>Deleting large numbers of rows from an existing table and then closing the table may free some space, but it may not.  Deleting rows may simply leave unused space (available for new inserts) inside TokuDB data files rather than shrink the files (internal fragmentation).</p> </li> </ul> <p>The fine print:</p> <ul> <li> <p>The TokuDB storage engine can use up to three separate file systems simultaneously, one each for the data, the recovery log, and the error log. All three are monitored, and if any one of the three falls below the relevant threshold then a warning message will be issued and inserts may be prohibited.</p> </li> <li> <p>Warning messages to the error log are not repeated unless available disk space has been above the relevant threshold for at least one minute. This prevents excess messages in the error log if the disk free space is fluctuating around the limit.</p> </li> <li> <p>Even if there are no other storage engines or other applications running, it is still possible for TokuDB to consume more disk space when operations such as row delete and query are performed, or when checkpoints are taken. This can happen because TokuDB can write cached information when it is time-efficient rather than when inserts are issued by the application, because operations in addition to insert (such as delete) create log entries, and also because of internal fragmentation of TokuDB data files.</p> </li> <li> <p>The tokudb_fs_reserve_percent variable can not be changed once the system has started. It can only be set in <code>my.cnf</code> or on the mysqld command line.</p> </li> </ul>"},{"location":"tokudb/tokudb_faq.html#backup","title":"Backup","text":"<p>How do I back up a system with TokuDB tables?</p>"},{"location":"tokudb/tokudb_faq.html#taking-backups-with-percona-tokubackup","title":"Taking backups with Percona TokuBackup","text":"<p>TokuDB is capable of performing online backups with Percona TokuBackup. To perform a backup, execute <code>backup to '/path/to/backup';</code>. This will create backup of the server and return when complete. The backup can be used by another server using a copy of the binaries on the source server. You can view the progress of the backup by executing <code>SHOW PROCESSLIST;</code>. TokuBackup produces a copy of your running MySQL server that is consistent at the end time of the backup process. The thread copying files from source to destination can be throttled by setting the tokudb_backup_throttle server variable. For more information check Percona TokuBackup.</p> <p>The following conditions apply:</p> <ul> <li>Currently, TokuBackup only supports tables using the TokuDB storage engine and the MyISAM tables in the <code>mysql</code> database.</li> </ul> <p>Warning</p> <p>You must disable InnoDB asynchronous IO if backing up InnoDB tables via TokuBackup utility. Otherwise you will have inconsistent, unrecoverable backups. The appropriate setting is innodb_use_native_aio to <code>0</code>.</p> <ul> <li> <p>Transactional storage engines (TokuDB and InnoDB) will perform recovery on the backup copy of the database when it is first started.</p> </li> <li> <p>Tables using non-transactional storage engines (MyISAM) are not locked during the copy and may report issues when starting up the backup. It is best to avoid operations that modify these tables at the end of a hot backup operation (adding/changing users, stored procedures, etc.).</p> </li> <li> <p>The database is copied locally to the path specified in <code>/path/to/backup</code>. This folder must exist, be writable, be empty, and contain enough space for a full copy of the database.</p> </li> <li> <p>TokuBackup always makes a backup of the MySQL <code>datadir</code> and optionally the tokudb_data_dir, tokudb_log_dir, and the binary log folder. The latter three are only backed up separately if they are not the same as or contained in the MySQL <code>datadir</code>. None of these three folders can be a parent of the MySQL <code>datadir</code>.</p> </li> <li> <p>A folder is created in the given backup destination for each of the source folders.</p> </li> <li> <p>No other directory structures are supported. All InnoDB, MyISAM, and other storage engine files must be within the MySQL <code>datadir</code>.</p> </li> <li> <p>TokuBackup does not follow symbolic links.</p> </li> </ul>"},{"location":"tokudb/tokudb_faq.html#other-options-for-taking-backups","title":"Other options for taking backups","text":"<p>TokuDB tables are represented in the file system with dictionary files, log files, and metadata files. A consistent copy of all of these files must be made during a backup. Copying the files while they may be modified by a running MySQL may result in an inconsistent copy of the database.</p> <p>LVM snapshots may be used to get a consistent snapshot of all of the TokuDB files. The LVM snapshot may then be backed up at leisure.</p> <p>The <code>SELECT INTO OUTFILE</code> statement or mysqldump application may also be used to get a logical backup of the database.</p>"},{"location":"tokudb/tokudb_faq.html#references","title":"References","text":"<p>The MySQL 5.5 reference manual describes several backup methods and strategies. In addition, we recommend reading the backup and recovery chapter in the following book:</p> <p>High Performance MySQL, 3<sup>rd</sup> Edition, by Baron Schwartz, Peter Zaitsev, and Vadim Tkachenko, Copyright 2012, O\u2019Reilly Media.</p>"},{"location":"tokudb/tokudb_faq.html#cold-backup","title":"Cold Backup","text":"<p>When MySQL is shut down, a copy of the MySQL data directory, the TokuDB data directory, and the TokuDB log directory can be made. In the simplest configuration, the TokuDB files are stored in the MySQL data directory with all of other MySQL files. One merely has to back up this directory.</p>"},{"location":"tokudb/tokudb_faq.html#hot-backup-using-mylvmbackup","title":"Hot Backup using mylvmbackup","text":"<p>The mylvmbackup utility, located on Launchpad, works with TokuDB. It does all of the magic required to get consistent copies of all of the MySQL tables, including MyISAM tables, InnoDB tables, etc., creates the LVM snapshots, and backs up the snapshots.</p>"},{"location":"tokudb/tokudb_faq.html#logical-snapshots","title":"Logical Snapshots","text":"<p>A logical snapshot of the databases uses a SQL statements to retrieve table rows and restore them. When used within a transaction, a consistent snapshot of the database can be taken. This method can be used to export tables from one database server and import them into another server.</p> <p>The <code>SELECT INTO OUTFILE</code> statement is used to take a logical snapshot of a database. The <code>LOAD DATA INFILE</code> statement is used to load the table data. Please see the MySQL 5.6 reference manual for details.</p> <p>Note</p> <p>Please do not use the :program`mysqlhotcopy` to back up TokuDB tables. This script is incompatible with TokuDB.</p>"},{"location":"tokudb/tokudb_faq.html#missing-log-files","title":"Missing Log Files","text":"<p>What do I do if I delete my logs files or they are otherwise missing?</p> <p>You\u2019ll need to recover from a backup. It is essential that the log files be present in order to restart the database.</p>"},{"location":"tokudb/tokudb_faq.html#isolation-levels","title":"Isolation Levels","text":"<p>What is the default isolation level for TokuDB?</p> <p>It is repeatable-read (MVCC).</p> <p>How can I change the isolation level?</p> <p>TokuDB supports repeatable-read, serializable, read-uncommitted and read-committed isolation levels (other levels are not supported). TokuDB employs pessimistic locking, and aborts a transaction when a lock conflict is detected.</p> <p>To guarantee that lock conflicts do not occur, use repeatable-read, read-uncommitted or read- committed isolation level.</p>"},{"location":"tokudb/tokudb_faq.html#lock-wait-timeout-exceeded","title":"Lock Wait Timeout Exceeded","text":"<p>Why do my MySQL clients get lock timeout errors for my update queries? And what should my application do when it gets these errors?</p> <p>Updates can get lock timeouts if some other transaction is holding a lock on the rows being updated for longer than the TokuDB lock timeout. You may want to increase the this timeout.</p> <p>If an update deadlocks, then the transaction should abort and retry.</p> <p>For more information on diagnosing locking issues, see Lock Visualization in TokuDB.</p>"},{"location":"tokudb/tokudb_faq.html#query-cache","title":"Query Cache","text":"<p>Does TokuDB support the query cache?</p> <p>Yes, you can enable the query cache in the <code>my.cnf</code> file. Please make sure that the size of the cache is set to something larger than <code>0</code>, as this, in effect, disables the cache.</p>"},{"location":"tokudb/tokudb_faq.html#row-size","title":"Row Size","text":"<p>What is the maximum row size?</p> <p>The maximum row size is 32 MiB.</p>"},{"location":"tokudb/tokudb_faq.html#nfs-cifs","title":"NFS &amp; CIFS","text":"<p>Can the data directories reside on a disk that is NFS or CIFS mounted?</p> <p>Yes, we do have customers in production with NFS &amp; CIFS volumes today. However, both of these disk types can pose a challenge to performance and data integrity due to their complexity. If you\u2019re seeking performance, the switching infrastructure and protocols of a traditional network were not conceptualized for low response times and can be very difficult to troubleshoot. If you\u2019re concerned with data integrity, the possible data caching at the NFS level can cause inconsistencies between the logs and data files that may never be detected in the event of a crash. If you are thinking of using a NFS or CIFS mount, we would recommend that you use synchronous mount options, which are available from the NFS mount man page, but these settings may decrease performance. For further discussion please look here.</p>"},{"location":"tokudb/tokudb_faq.html#using-other-storage-engines","title":"Using Other Storage Engines","text":"<p>Can the MyISAM and InnoDB Storage Engines be used?</p> <p>MyISAM and InnoDB can be used directly in conjunction with TokuDB. Please note that you should not overcommit memory between InnoDB and TokuDB. The total memory assigned to both caches must be less than physical memory.</p> <p>Can the Federated Storage Engines be used?</p> <p>The Federated Storage Engine can also be used, however it is disabled by default in MySQL. It can be enabled by either running mysqld with <code>--federated</code> as a command line parameter, or by putting <code>federated</code> in the <code>[mysqld]</code> section of the <code>my.cnf</code> file.</p> <p>For more information see the MySQL 5.6 Reference Manual: FEDERATED Storage Engine.</p>"},{"location":"tokudb/tokudb_faq.html#using-mysql-patches-with-tokudb","title":"Using MySQL Patches with TokuDB","text":"<p>Can I use MySQL source code patches with TokuDB?</p> <p>Yes, but you need to apply Percona patches as well as your patches to MySQL to build a binary that works with the Percona Fractal Tree library.</p>"},{"location":"tokudb/tokudb_faq.html#truncate-table-vs-delete-from-table","title":"Truncate Table vs Delete from Table","text":"<p>Which is faster, TRUNCATE TABLE or DELETE FROM TABLE?</p> <p>Please use <code>TRUNCATE TABLE</code> whenever possible. A table truncation runs in constant time, whereas a <code>DELETE FROM TABLE</code> requires a row-by-row deletion and thus runs in time linear to the table size.</p>"},{"location":"tokudb/tokudb_faq.html#foreign-keys","title":"Foreign Keys","text":"<p>Does TokuDB enforce foreign key constraints?</p> <p>No, TokuDB ignores foreign key declarations.</p>"},{"location":"tokudb/tokudb_faq.html#dropping-indexes","title":"Dropping Indexes","text":"<p>Is dropping an index in TokuDB hot?</p> <p>No, the table is locked for the amount of time it takes the file system to delete the file associated with the index.</p>"},{"location":"tokudb/tokudb_file_management.html","title":"TokuDB file management","text":"<p>As mentioned in the TokuDB files and file types Percona FT is extremely pedantic about validating its data set. If a file goes missing or can\u2019t be accessed, or seems to contain some nonsensical data, it will assert, abort or fail to start. It does this not to annoy you, but to try to protect you from doing any further damage to your data.</p> <p>This document contains examples of common file maintenance operations and instructions on how to safely execute these operations.</p> <p>Beginning in Percona Server Percona Server for MySQL 5.7.15-9 a new server option was introduced called tokudb_dir_per_db. This feature addressed two shortcomings the renaming of data files on table/index rename, and the ability to group data files together within a directory that represents a single database. This feature is enabled by default.</p> <p>In Percona Server for MySQL Percona Server for MySQL 5.7.18-14 new tokudb_dir_cmd variable has been implemented that can be used to edit the contents of the TokuDB/PerconaFT directory map.</p>"},{"location":"tokudb/tokudb_file_management.html#moving-tokudb-data-files-to-a-location-outside-of-the-default-mysql-datadir","title":"Moving TokuDB data files to a location outside of the default MySQL datadir","text":"<p>TokuDB uses the location specified by the tokudb_data_dir variable for all of its data files. If the tokudb_data_dir variable is not explicitly set, TokuDB will use the location specified by the servers source/glossary.rst`datadir` for these files.</p> <p>The TokuDB data files are protected from concurrent process access by the <code>__tokudb_lock_dont_delete_me_data</code> file that is located in the same directory as the TokuDB data files.</p> <p>TokuDB data files may be moved to other locations with symlinks left behind in their place. If those symlinks refer to files on other physical data volumes, the tokudb_fs_reserve_percent monitor will not traverse the symlink and monitor the real location for adequate space in the file system.</p> <p>To safely move your TokuDB data files:</p> <ol> <li> <p>Shut the server down cleanly.</p> </li> <li> <p>Change the tokudb_data_dir in your <code>my.cnf</code> configuration file to the location where you wish to store your TokuDB data files.</p> </li> <li> <p>Create your new target directory.</p> </li> <li> <p>Move your <code>\\*.tokudb</code> files and your <code>__tokudb_lock_dont_delete_me_data</code> from the current location to the new location.</p> </li> <li> <p>Restart your server.</p> </li> </ol>"},{"location":"tokudb/tokudb_file_management.html#moving-tokudb-temporary-files-to-a-location-outside-of-the-default-mysql-datadir","title":"Moving TokuDB temporary files to a location outside of the default MySQL datadir","text":"<p>TokuDB will use the location specified by the tokudb_tmp_dir variable for all of its temporary files. If tokudb_tmp_dir variable is not explicitly set, TokuDB will use the location specified by the tokudb_data_dir variable. If the tokudb_data_dir variable is also not explicitly set, TokuDB will use the location specified by the servers source/glossary.rst`datadir` for these files.</p> <p>TokuDB temporary files are protected from concurrent process access by the <code>__tokudb_lock_dont_delete_me_temp</code> file that is located in the same directory as the TokuDB temporary files.</p> <p>If you locate your TokuDB temporary files on a physical volume that is different from where your TokuDB data files or recovery log files are located, the tokudb_fs_reserve_percent monitor will not monitor their location for adequate space in the file system.</p> <p>To safely move your TokuDB temporary files:</p> <ol> <li> <p>Shut the server down cleanly. A clean shutdown will ensure that there are no temporary files that need to be relocated.</p> </li> <li> <p>Change the tokudb_tmp_dir variable in your <code>my.cnf</code> configuration file to the location where you wish to store your new TokuDB temporary files.</p> </li> <li> <p>Create your new target directory.</p> </li> <li> <p>Move your <code>__tokudb_lock_dont_delete_me_temp</code> file from the current location to the new location.</p> </li> <li> <p>Restart your server.</p> </li> </ol>"},{"location":"tokudb/tokudb_file_management.html#moving-tokudb-recovery-log-files-to-a-location-outside-of-the-default-mysql-datadir","title":"Moving TokuDB recovery log files to a location outside of the default MySQL datadir","text":"<p>TokuDB will use the location specified by the tokudb_log_dir variable for all of its recovery log files. If the tokudb_log_dir variable is not explicitly set, TokuDB will use the location specified by the servers source/glossary.rst`datadir` for these files.</p> <p>The TokuDB recovery log files are protected from concurrent process access by the <code>__tokudb_lock_dont_delete_me_logs</code> file that is located in the same directory as the TokuDB recovery log files.</p> <p>TokuDB recovery log files may be moved to another location with symlinks left behind in place of the tokudb_log_dir. If that symlink refers to a directory on another physical data volume, the tokudb_fs_reserve_percent monitor will not traverse the symlink and monitor the real location for adequate space in the file system.</p> <p>To safely move your TokuDB recovery log files:</p> <ol> <li> <p>Shut the server down cleanly.</p> </li> <li> <p>Change the tokudb_log_dir in your <code>my.cnf</code> configuration file to the location where you wish to store your TokuDB recovery log files.</p> </li> <li> <p>Create your new target directory.</p> </li> <li> <p>Move your <code>log\\*.tokulog\\*</code> files and your <code>__tokudb_lock_dont_delete_me_logs</code> file from the current location to the new location.</p> </li> <li> <p>Restart your server.</p> </li> </ol>"},{"location":"tokudb/tokudb_file_management.html#improved-table-renaming-functionality","title":"Improved table renaming functionality","text":"<p>When you rename a TokuDB table via SQL, the data files on disk keep their original names and only the mapping in the Percona FT directory file is changed to map the new dictionary name to the original internal file names. This makes it difficult to quickly match database/table/index names to their actual files on disk, requiring you to use the refTOKUDB_FILE_MAP table to cross reference.</p> <p>Beginning with Percona Server for MySQL Percona Server for MySQL 5.7.15-9 a new server option was introduced called tokudb_dir_per_db to address this issue.</p> <p>When tokudb_dir_per_db is enabled (<code>ON</code> by default), this is no longer the case. When you rename a table, the mapping in the Percona FT directory file will be updated and the files will be renamed on disk to reflect the new table name.</p>"},{"location":"tokudb/tokudb_file_management.html#improved-directory-layout-functionality","title":"Improved directory layout functionality","text":"<p>Many users have had issues with managing the huge volume of individual files that TokuDB and Percona FT use.</p> <p>Beginning with Percona Server for MySQL Percona Server for MySQL 5.7.15-9 a new server option was introduced called tokudb_dir_per_db to address this issue.</p> <p>When tokudb_dir_per_db variable is enabled (<code>ON</code> by default), all new tables and indices will be placed within their corresponding database directory within the <code>tokudb_data_dir</code> or server source/glossary.rst`datadir`.</p> <p>If you have tokudb_data_dir variable set to something other than the server source/glossary.rst`datadir`, TokuDB will create a directory matching the name of the database, but upon dropping of the database, this directory will remain behind.</p> <p>Existing table files will not be automatically relocated to their corresponding database directory.</p> <p>You can easily move a tables data files into the new scheme and proper database directory with a few steps:</p> <pre><code>mysql&gt; SET GLOBAL tokudb_dir_per_db=true;\nmysql&gt; RENAME TABLE &lt;table&gt; TO &lt;tmp_table&gt;;\nmysql&gt; RENAME TABLE &lt;tmp_table&gt; TO &lt;table&gt;;\n</code></pre> <p>Note</p> <p>Two renames are needed because MySQL doesn\u2019t allow you to rename a table to itself. The first rename, renames the table to the temporary name and moves the table files into the owning database directory. The second rename sets the table name back to the original name. Tables can also be renamed/moved across databases and will be placed correctly into the corresponding database directory.</p> <p>Warning</p> <p>You must be careful with renaming tables in case you have used any tricks to create symlinks of the database directories on different storage volumes, the move is not a simple directory move on the same volume but a physical copy across volumes. This can take quite some time and prevent access to the table being moved during the copy.</p>"},{"location":"tokudb/tokudb_file_management.html#editing-tokudb-directory-map-with-tokudb_dir_cmd","title":"Editing TokuDB directory map with tokudb_dir_cmd","text":"<p>Note</p> <p>This feature is currently considered Experimental.</p> <p>In Percona Server for MySQL Percona Server for MySQL 5.7.18-14 new tokudb_dir_cmd variable has been implemented that can be used to edit the TokuDB directory map.</p> <p>Warning</p> <p>Use this variable only if you know what you\u2019re doing otherwise it will have data loss.</p> <p>This method can be used if any kind of system issue causes the loss of specific <code>.tokudb</code> files for a given table, because the TokuDB tablespace file mapping will then contain invalid (nonexistent) entries, visible in TokuDB_file_map table.</p> <p>This variable is used to send commands to edit directory file. The format of the command line is the following:</p> <pre><code>command arg1 arg2 .. argn\n</code></pre> <p>I.e, if we want to execute some command the following statement can be used:</p> <pre><code>SET tokudb_dir_cmd = \"command arg1 ... argn\"\n</code></pre> <p>Currently the following commands are available:</p> <ul> <li> <p><code>attach dictionary_name internal_file_name</code> - attach internal_file_name to a dictionary_name, if the dictionary_name exists override the previous value, add new record otherwise</p> </li> <li> <p><code>detach dictionary_name</code> - remove record with corresponding dictionary_name, the corresponding internal_file_name file stays untouched</p> </li> <li> <p><code>move old_dictionary_name new_dictionary_name</code> - rename (only) dictionary_name from old_dictionary_name to new_dictionary_name</p> </li> </ul> <p>Information about the dictionary_name and internal_file_name can be found in the TokuDB_file_map table:</p> <pre><code>mysql&gt; SELECT dictionary_name, internal_file_name FROM INFORMATION_SCHEMA.TokuDB_file_map;\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>+------------------------------+---------------------------------------------------------+\n| dictionary_name              | internal_file_name                                      |\n+------------------------------+---------------------------------------------------------+\n| ./world/City-key-CountryCode | ./_world_sql_340a_39_key_CountryCode_12_1_1d_B_1.tokudb |\n| ./world/City-main            | ./_world_sql_340a_39_main_12_1_1d_B_0.tokudb            |\n| ./world/City-status          | ./_world_sql_340a_39_status_f_1_1d.tokudb               |\n+------------------------------+---------------------------------------------------------+\n</code></pre>"},{"location":"tokudb/tokudb_file_management.html#system-variables","title":"System Variables","text":""},{"location":"tokudb/tokudb_file_management.html#tokudb_dir_cmd","title":"<code>tokudb_dir_cmd</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type String <p>The variable has been implemented in Percona Server for MySQL 5.7.18-14. This variable is used to send commands to edit TokuDB directory map.</p> <p>Warning</p> <p>Use this variable only if you know what you\u2019re doing otherwise it WILL lead to data loss.</p>"},{"location":"tokudb/tokudb_file_management.html#status-variables","title":"Status Variables","text":""},{"location":"tokudb/tokudb_file_management.html#tokudb_dir_cmd_last_error","title":"<code>tokudb_dir_cmd_last_error</code>","text":"Option Description Scope Global Data type Numeric <p>This variable contains the error number of the last executed command by using the tokudb_dir_cmd variable.</p>"},{"location":"tokudb/tokudb_file_management.html#tokudb_dir_cmd_last_error_string","title":"<code>tokudb_dir_cmd_last_error_string</code>","text":"Option Description Scope Global Data type Numeric <p>This variable contains the error string of the last executed command by using the tokudb_dir_cmd variable.</p>"},{"location":"tokudb/tokudb_files_and_file_types.html","title":"TokuDB files and file types","text":"<p>The TokuDB file set consists of many different files that all serve various purposes.</p> <p>If you have any TokuDB data your data directory should look similar to this:</p> <pre><code>root@server:/var/lib/mysql# ls -lah\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>...\n-rw-rw----  1 mysql mysql  76M Oct 13 18:45 ibdata1\n...\n-rw-rw----  1 mysql mysql  16K Oct 13 15:52 tokudb.directory\n-rw-rw----  1 mysql mysql  16K Oct 13 15:52 tokudb.environment\n-rw-------  1 mysql mysql    0 Oct 13 15:52 __tokudb_lock_dont_delete_me_data\n-rw-------  1 mysql mysql    0 Oct 13 15:52 __tokudb_lock_dont_delete_me_environment\n-rw-------  1 mysql mysql    0 Oct 13 15:52 __tokudb_lock_dont_delete_me_logs\n-rw-------  1 mysql mysql    0 Oct 13 15:52 __tokudb_lock_dont_delete_me_recovery\n-rw-------  1 mysql mysql    0 Oct 13 15:52 __tokudb_lock_dont_delete_me_temp\n-rw-rw----  1 mysql mysql  16K Oct 13 15:52 tokudb.rollback\n...\n</code></pre> <p>This document lists the different types of TokuDB and Percona Fractal Tree files, explains their purpose, shows their location and how to move them around.</p>"},{"location":"tokudb/tokudb_files_and_file_types.html#tokudbenvironment","title":"tokudb.environment","text":"<p>This file is the root of the Percona FT file set and contains various bits of metadata about the system, such as creation times, current file format versions, etc.</p> <p>Percona FT will create/expect this file in the directory specified by the MySQL source/glossary.rst`datadir`.</p>"},{"location":"tokudb/tokudb_files_and_file_types.html#tokudbrollback","title":"tokudb.rollback","text":"<p>Every transaction within Percona FT maintains its own transaction rollback log. These logs are stored together within a single Percona FT dictionary file and take up space within the Percona FT cachetable (just like any other Percona FT dictionary).</p> <p>The transaction rollback logs will <code>undo</code> any changes made by a transaction if the transaction is explicitly rolled back, or rolled back via recovery as a result of an uncommitted transaction when a crash occurs.</p> <p>Percona FT will create/expect this file in the directory specified by the MySQL source/glossary.rst`datadir`.</p>"},{"location":"tokudb/tokudb_files_and_file_types.html#tokudbdirectory","title":"tokudb.directory","text":"<p>Percona FT maintains a mapping of a dictionary name (example: <code>sbtest.sbtest1.main</code>) to an internal file name (example: <code>_sbtest_sbtest1_main_xx_x_xx.tokudb</code>). This mapping is stored within this single Percona FT dictionary file and takes up space within the Percona FT cachetable just like any other Percona FT dictionary.</p> <p>Percona FT will create/expect this file in the directory specified by the MySQL source/glossary.rst`datadir`.</p>"},{"location":"tokudb/tokudb_files_and_file_types.html#dictionary-files","title":"Dictionary files","text":"<p>TokuDB dictionary (data) files store actual user data. For each MySQL table there will be:</p> <ul> <li> <p>One <code>status</code> dictionary that contains metadata about the table.</p> </li> <li> <p>One <code>main</code> dictionary that stores the full primary key (an imaginary key is used if one was not explicitly specified) and full row data.</p> </li> <li> <p>One <code>key</code> dictionary for each additional key/index on the table.</p> </li> </ul> <p>These are typically named: <code>_&lt;database&gt;_&lt;table&gt;_&lt;key&gt;_&lt;internal_txn_id&gt;.tokudb</code></p> <p>Percona FT creates/expects these files in the directory specified by tokudb_data_dir if set, otherwise the MySQL <code>datadir</code> is used.</p>"},{"location":"tokudb/tokudb_files_and_file_types.html#recovery-log-files","title":"Recovery log files","text":"<p>The Percona FT recovery log records every operation that modifies a Percona FT dictionary. Periodically, the system will take a snapshot of the system called a checkpoint. This checkpoint ensures that the modifications recorded within the Percona FT recovery logs have been applied to the appropriate dictionary files up to a known point in time and synced to disk.</p> <p>These files have a rolling naming convention, but use: <code>log&lt;log_file_number&gt;.tokulog&lt;log_file_format_version&gt;</code>.</p> <p>Percona FT creates/expects these files in the directory specified by tokudb_log_dir if set, otherwise the MySQL source/glossary.rst`datadir` is used.</p> <p>Percona FT does not track what log files should or shouldn\u2019t be present. Upon startup, it discovers the logs in the log directory, and replays them in order. If the wrong logs are present, the recovery aborts and possibly damages the dictionaries.</p>"},{"location":"tokudb/tokudb_files_and_file_types.html#temporary-files","title":"Temporary files","text":"<p>Percona FT might need to create some temporary files in order to perform some operations. When the bulk loader is active, these temporary files might grow to be quite large.</p> <p>As different operations start and finish, the files will come and go.</p> <p>There are no temporary files left behind upon a clean shutdown,</p> <p>Percona FT creates/expects these files in the directory specified by tokudb_tmp_dir if set. If not, the tokudb_data_dir is used if set, otherwise the MySQL source/glossary.rst`datadir` is used.</p>"},{"location":"tokudb/tokudb_files_and_file_types.html#lock-files","title":"Lock files","text":"<p>Percona FT uses lock files to prevent multiple processes from accessing and writing to the files in the assorted Percona FT functionality areas. Each lock file will be in the same directory as the file(s) that it is protecting.</p> <p>These empty files are only used as semaphores across processes. They are safe to delete/ignore as long as no server instances are currently running and using the data set.</p> <p><code>__tokudb_lock_dont_delete_me_environment</code></p> <p><code>__tokudb_lock_dont_delete_me_recovery</code></p> <p><code>__tokudb_lock_dont_delete_me_logs</code></p> <p><code>__tokudb_lock_dont_delete_me_data</code></p> <p><code>__tokudb_lock_dont_delete_me_temp</code></p> <p>Percona FT is extremely pedantic about validating its data set. If a file goes missing or unfound, or seems to contain some nonsensical data, it will assert, abort or fail to start. It does this not to annoy you, but to try to protect you from doing any further damage to your data.</p>"},{"location":"tokudb/tokudb_fractal_tree_indexing.html","title":"TokuDB Fractal Tree Indexing","text":"<p>Fractal Tree indexing is the technology behind TokuDB and is protected by multiple patents. This type of index enhances the tradional B-tree data structure used in other database engines, and optimizes performance for modern hardware and data sets.</p>"},{"location":"tokudb/tokudb_fractal_tree_indexing.html#background","title":"Background","text":"<p>The B-tree data structure was optimized for large blocks of data but the performance is limited by I/O bandwidth. The size of a production database generally exceeds available main memory. Most leaves in a tree are stored on disk, not in RAM. If a leaf is not in main memory inserting information requires a disk I/O operation. Continually adding RAM to keep pace with data\u2019s growth is too expensive.</p>"},{"location":"tokudb/tokudb_fractal_tree_indexing.html#buffers","title":"Buffers","text":"<p>Like a B-tree structure, a fractal tree index is a tree data structure, but each node has buffers that allow messages to be stored. Insertions, deletions, and updates are inserted into the buffers as messages. Buffers let each disk operation be more efficient by writing large amounts of data. Buffers also avoid the common B-tree scenario when disk writes change only a small amount of data.</p> <p>In fractal tree indexes, non-leaf (internal) nodes have child nodes. The number of child nodes is variable and based on a pre-defined range. When data is inserted or deleted from a node, the number of child nodes changes. Internal nodes may join or split to maintain the defined range. When the buffer is full, the mesages are flushed to children nodes.</p> <p>Fractal tree index data structure involves the same algorithmic complexity as B-tree queries. There is no data loss because the queries follow the path from the root to leaf and pass through all messages. A query knows the current state of data even if changes have not been propagated to the corresponding leaves.</p> <p>Each message is stamped with a unique message sequence number (MSN) when the message is stored in a non-leaf node message buffer. The MSN maintains the order of messages and ensures the messages are only applied once to leaf nodes when the leaf node is updated by messages.</p> <p>Buffers are also serialized to disk, messages in internal nodes are not lost in the case of a crash or outage. If a write happened after a checkpoint, but before a crash, recovery replays the operation from the log.</p>"},{"location":"tokudb/tokudb_installation.html","title":"TokuDB Installation","text":"<p>Percona Server for MySQL is compatible with the separately available TokuDB storage engine package. The TokuDB engine must be separately downloaded and then enabled as a plug-in component. This package can be installed alongside with standard Percona Server for MySQL 5.7 releases and does not require any specially adapted version of Percona Server for MySQL.</p> <p>The TokuDB storage engine is a scalable, ACID and MVCC compliant storage engine that provides indexing-based query improvements, offers online schema modifications, and reduces replica lag for both hard disk drives and flash memory. This storage engine is specifically designed for high performance on write-intensive workloads which is achieved with Fractal Tree indexing. To learn more about Fractal Tree indexing, you can visit the following Wikipedia page.</p> <p>Warning</p> <p>Only the Percona supplied TokuDB engine should be used with Percona Server for MySQL 5.7. A TokuDB engine downloaded from other sources is not compatible. TokuDB file formats are not the same across MySQL variants. Migrating from one variant to any other variant requires a logical data dump and reload.</p>"},{"location":"tokudb/tokudb_installation.html#prerequisites","title":"Prerequisites","text":""},{"location":"tokudb/tokudb_installation.html#libjemalloc-library","title":"<code>libjemalloc</code> library","text":"<p>TokuDB storage engine requires <code>libjemalloc</code> library 3.3.0 or greater. If the version in the distribution repository is lower than that you can use one from Percona Software Repositories or download it from somewhere else.</p> <p>If the <code>libjemalloc</code> wasn\u2019t installed and enabled before it will be automatically installed when installing the TokuDB storage engine package by using the apt` or yum package manager, but Percona Server for MySQL instance should be restarted for <code>libjemalloc</code> to be loaded. This way <code>libjemalloc</code> will be loaded with <code>LD_PRELOAD</code>. You can also enable <code>libjemalloc</code> by specifying malloc-lib variable in the <code>[mysqld_safe]</code> section of the <code>my.cnf</code> file:</p> <pre><code>[mysqld_safe]\nmalloc-lib= /path/to/jemalloc\n</code></pre>"},{"location":"tokudb/tokudb_installation.html#transparent-huge-pages","title":"Transparent huge pages","text":"<p>TokuDB won\u2019t be able to start if the transparent huge pages are enabled. Transparent huge pages is feature available in the newer kernel versions. You can check if the Transparent huge pages are enabled with:</p> <pre><code>$ cat /sys/kernel/mm/transparent_hugepage/enabled\n</code></pre> <p>The output could be the following:</p> <pre><code> [always] madvise never\n</code></pre> <p>If transparent huge pages are enabled and you try to start the TokuDB engine you\u2019ll get the following message in you <code>error.log</code>:</p> <pre><code>Transparent huge pages are enabled, according to /sys/kernel/mm/redhat_transparent_hugepage/enabled\nTransparent huge pages are enabled, according to /sys/kernel/mm/transparent_hugepage/enabled\n</code></pre> <p>You can disable transparent huge pages permanently by passing <code>transparent_hugepage=never</code> to the kernel in your bootloader (NOTE: For this change to take an effect you\u2019ll need to reboot your server).</p> <p>You can disable the transparent huge pages by running the following command as root.</p> <p>Note</p> <p>This setting lasts until the server is rebooted.</p> <pre><code>echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled\necho never &gt; /sys/kernel/mm/transparent_hugepage/defrag\n</code></pre>"},{"location":"tokudb/tokudb_installation.html#installation","title":"Installation","text":"<p>TokuDB storage engine for Percona Server for MySQL is currently available in our apt and yum repositories.</p> <p>You can install the Percona Server for MySQL with TokuDB engine by using the apt/yum commands:</p> <pre><code>[root@centos ~]# yum install Percona-Server-tokudb-57.x86_64\n</code></pre> <p>or</p> <pre><code>root@wheezy:~# apt install percona-server-tokudb-5.7\n</code></pre>"},{"location":"tokudb/tokudb_installation.html#enabling-the-tokudb-storage-engine","title":"Enabling the TokuDB Storage Engine","text":"<p>Once the TokuDB server package has been installed following output will be shown:</p> <pre><code>* This release of Percona Server is distributed with TokuDB storage engine.\n   * Run the following script to enable the TokuDB storage engine in Percona Server:\n\n    ps-admin --enable-tokudb -u &lt;mysql_admin_user&gt; -p[mysql_admin_pass] [-S &lt;socket&gt;] [-h &lt;host&gt; -P &lt;port&gt;]\n\n   * See http://www.percona.com/doc/percona-server/5.7/tokudb/tokudb_installation.html for more installation details\n\n   * See http://www.percona.com/doc/percona-server/5.7/tokudb/tokudb_intro.html for an introduction to TokuDB\n</code></pre> <p>Percona Server for MySQL has implemented <code>ps_tokudb_admin</code> script to make the enabling the TokuDB storage engine easier. This script will automatically disable Transparent huge pages, if they\u2019re enabled, and install and enable the TokuDB storage engine with all the required plugins. You need to run this script as root or with sudo. The script should only be used for local installations and should not be used to install TokuDB to a remote server. After you run the script with required parameters:</p> <pre><code>ps-admin --enable-tokudb -uroot -pPassw0rd\n</code></pre> <p>Following output will be displayed:</p> <pre><code>Checking if Percona server is running with jemalloc enabled...\n&gt;&gt; Percona server is running with jemalloc enabled.\n\nChecking transparent huge pages status on the system...\n&gt;&gt; Transparent huge pages are currently disabled on the system.\n\nChecking if thp-setting=never option is already set in config file...\n&gt;&gt; Option thp-setting=never is not set in the config file.\n&gt;&gt; (needed only if THP is not disabled permanently on the system)\n\nChecking TokuDB plugin status...\n&gt;&gt; TokuDB plugin is not installed.\n\nAdding thp-setting=never option into /etc/mysql/my.cnf\n&gt;&gt; Successfuly added thp-setting=never option into /etc/mysql/my.cnf\n\nInstalling TokuDB engine...\n&gt;&gt; Successfuly installed TokuDB plugin.\n</code></pre> <p>If the script returns no errors, TokuDB storage engine should be successfully enabled on your server. You can check it out by running:</p> <pre><code>mysql&gt; SHOW ENGINES;\n</code></pre> <p>The output could be the following:</p> <pre><code>...\n | TokuDB | YES | Tokutek TokuDB Storage Engine with Fractal Tree(tm) Technology | YES | YES | YES |\n...\n</code></pre>"},{"location":"tokudb/tokudb_installation.html#enabling-the-tokudb-storage-engine-manually","title":"Enabling the TokuDB Storage Engine Manually","text":"<p>If you don\u2019t want to use <code>ps-admin</code> script you\u2019ll need to manually install the storage engine ad required plugins.</p> <pre><code>INSTALL PLUGIN tokudb SONAME 'ha_tokudb.so';\nINSTALL PLUGIN tokudb_file_map SONAME 'ha_tokudb.so';\nINSTALL PLUGIN tokudb_fractal_tree_info SONAME 'ha_tokudb.so';\nINSTALL PLUGIN tokudb_fractal_tree_block_map SONAME 'ha_tokudb.so';\nINSTALL PLUGIN tokudb_trx SONAME 'ha_tokudb.so';\nINSTALL PLUGIN tokudb_locks SONAME 'ha_tokudb.so';\nINSTALL PLUGIN tokudb_lock_waits SONAME 'ha_tokudb.so';\nINSTALL PLUGIN tokudb_background_job_status SONAME 'ha_tokudb.so';\n</code></pre> <p>After the engine has been installed it should be present in the engines list. To check if the engine has been correctly installed and active:</p> <pre><code>mysql&gt; SHOW ENGINES;\n</code></pre> <p>The output could be the following:</p> <pre><code>...\n| TokuDB | YES | Tokutek TokuDB Storage Engine with Fractal Tree(tm) Technology | YES | YES | YES |\n...\n</code></pre> <p>To check if all the TokuDB plugins have been installed correctly you should run:</p> <pre><code>mysql&gt; SHOW PLUGINS;\n</code></pre> <p>The output could be the following:</p> <pre><code>...\n| TokuDB                        | ACTIVE   | STORAGE ENGINE     | ha_tokudb.so | GPL     |\n| TokuDB_file_map               | ACTIVE   | INFORMATION SCHEMA | ha_tokudb.so | GPL     |\n| TokuDB_fractal_tree_info      | ACTIVE   | INFORMATION SCHEMA | ha_tokudb.so | GPL     |\n| TokuDB_fractal_tree_block_map | ACTIVE   | INFORMATION SCHEMA | ha_tokudb.so | GPL     |\n| TokuDB_trx                    | ACTIVE   | INFORMATION SCHEMA | ha_tokudb.so | GPL     |\n| TokuDB_locks                  | ACTIVE   | INFORMATION SCHEMA | ha_tokudb.so | GPL     |\n| TokuDB_lock_waits             | ACTIVE   | INFORMATION SCHEMA | ha_tokudb.so | GPL     |\n| TokuDB_background_job_status  | ACTIVE   | INFORMATION SCHEMA | ha_tokudb.so | GPL     |\n...\n</code></pre>"},{"location":"tokudb/tokudb_installation.html#tokudb-version","title":"TokuDB Version","text":"<p>TokuDB storage engine version can be checked with:</p> <pre><code>mysql&gt; SELECT @@tokudb_version;\n</code></pre> <p>The output could be the following:</p> <pre><code>+------------------+\n| @@tokudb_version |\n+------------------+\n| 5.7.10-1rc1      |\n+------------------+\n1 row in set (0.00 sec)\n</code></pre>"},{"location":"tokudb/tokudb_installation.html#upgrade","title":"Upgrade","text":"<p>Installing the TokuDB package is compatible with existing server setup and databases.</p>"},{"location":"tokudb/tokudb_intro.html","title":"TokuDB Introduction","text":"<p>TokuDB is a highly scalable, zero-maintenance downtime MySQL storage engine that delivers indexing-based query acceleration, improved replication performance, unparalleled compression, and live schema modification. The TokuDB storage engine is a scalable, ACID and MVCC compliant storage engine that provides indexing-based query improvements, offers online schema modifications, and reduces replica lag for both hard disk drives and flash memory. This storage engine is specifically designed for high performance on write-intensive workloads which is achieved with Fractal Tree indexing.</p> <p>Percona Server for MySQL is compatible with the separately available TokuDB storage engine package. The TokuDB engine must be separately downloaded and then enabled as a plug-in component. This package can be installed alongside with standard Percona Server for MySQL 5.7 releases and does not require any specially adapted version of Percona Server for MySQL.</p> <p>Warning</p> <p>Only the Percona supplied TokuDB engine should be used with Percona Server for MySQL 5.7. A TokuDB engine downloaded from other sources is not compatible. TokuDB file formats are not the same across MySQL variants. Migrating from one variant to any other variant requires a logical data dump and reload.</p> <p>Additional features unique to TokuDB include:</p> <ul> <li> <p>Up to 25x Data Compression</p> </li> <li> <p>Fast Inserts</p> </li> <li> <p>Eliminates Replica Lag with Read Free Replication</p> </li> <li> <p>Hot Schema Changes</p> </li> <li> <p>Hot Index Creation - TokuDB tables support insertions, deletions and queries with no down time while indexes are being added to that table</p> </li> <li> <p>Hot column addition, deletion, expansion, and rename - TokuDB tables support insertions, deletions and queries without down-time when an alter table adds, deletes, expands, or renames columns</p> </li> <li> <p>On-line Backup</p> </li> </ul> <p>For more information on installing and using TokuDB click on the following links:</p> <ul> <li> <p>TokuDB Installation</p> </li> <li> <p>Using TokuDB</p> </li> <li> <p>Getting Started with TokuDB</p> </li> <li> <p>TokuDB Variables</p> </li> <li> <p>Percona TokuBackup</p> </li> <li> <p>TokuDB Troubleshooting</p> </li> <li> <p>Frequently Asked Questions</p> </li> <li> <p>Removing TokuDB storage engine</p> </li> </ul>"},{"location":"tokudb/tokudb_intro.html#getting-the-most-from-tokudb","title":"Getting the Most from TokuDB","text":"<p>Compression: TokuDB compresses all data on disk, including indexes. Compression lowers cost by reducing the amount of storage required and frees up disk space for additional indexes to achieve improved query performance. Depending on the compressibility of the data, we have seen compression ratios up to 25x for high compression. Compression can also lead to improved performance since less data needs to be read from and written to disk.</p> <p>Fast Insertions and Deletions: TokuDB\u2019s Fractal Tree technology enables fast indexed insertions and deletions. Fractal Trees match B-trees in their indexing sweet spot (sequential data) and are up to two orders of magnitude faster for random data with high cardinality.</p> <p>Eliminates Replica Lag: TokuDB replication replicas can be configured to process the replication stream with virtually no read IO. Uniqueness checking is performed on the TokuDB source and can be skipped on all TokuDB replicas. Also, row based replication ensures that all before and after row images are captured in the binary logs, so the TokuDB replicas can harness the power of Fractal Tree indexes and bypass traditional read-modify-write behavior. This \u201cRead Free Replication\u201d ensures that replication replicas do not fall behind the source and can be used for read scaling, backups, and disaster recovery, without sharding, expensive hardware, or limits on what can be replicated.</p> <p>Hot Index Creation: TokuDB allows the addition of indexes to an existing table while inserts and queries are being performed on that table. This means that MySQL can be run continuously with no blocking of queries or insertions while indexes are added and eliminates the down-time that index changes would otherwise require.</p> <p>Hot Column Addition, Deletion, Expansion and Rename: TokuDB allows the addition of new columns to an existing table, the deletion of existing columns from an existing table, the expansion of <code>char</code>, <code>varchar</code>, <code>varbinary</code>, and <code>integer</code> type columns in an existing table, and the renaming of an existing column while inserts and queries are being performed on that table.</p> <p>Online (Hot) Backup: The TokuDB can create backups of online database servers without downtime.</p> <p>Fast Indexing: In practice, slow indexing often leads users to choose a smaller number of sub-optimal indexes in order to keep up with incoming data rates. These sub-optimal indexes result in disproportionately slower queries, since the difference in speed between a query with an index and the same query when no index is available can be many orders of magnitude. Thus, fast indexing means fast queries.</p> <p>Clustering Keys and Other Indexing Improvements: TokuDB tables are clustered on the primary key. TokuDB also supports clustering secondary keys, providing better performance on a broader range of queries. A clustering key includes (or clusters) all of the columns in a table along with the key. As a result, one can efficiently retrieve any column when doing a range query on a clustering key. Also, with TokuDB, an auto-increment column can be used in any index and in any position within an index. Lastly, TokuDB indexes can include up to 32 columns.</p> <p>Less Aging/Fragmentation: TokuDB can run much longer, likely indefinitely, without the need to perform the customary practice of dump/reload or <code>OPTIMIZE TABLE</code> to restore database performance. The key is the fundamental difference with which the Fractal Tree stores data on disk. Since, by default, the Fractal Tree will store data in 4MB chunks (pre-compression), as compared to InnoDB\u2019s 16KB, TokuDB has the ability to avoid \u201cdatabase disorder\u201d up to 250x better than InnoDB.</p> <p>Bulk Loader: TokuDB uses a parallel loader to create tables and offline indexes. This parallel loader will use multiple cores for fast offline table and index creation.</p> <p>Full-Featured Database: TokuDB supports fully ACID-compliant transactions, MVCC (Multi-Version Concurrency Control), serialized isolation levels, row-level locking, and XA. TokuDB scales with high number of client connections, even for large tables.</p> <p>Lock Diagnostics: TokuDB provides users with the tools to diagnose locking and deadlock issues. For more information, see Lock Visualization in TokuDB.</p> <p>Progress Tracking: Running <code>SHOW PROCESSLIST</code> when adding indexes provides status on how many rows have been processed. Running <code>SHOW PROCESSLIST</code> also shows progress on queries, as well as insertions, deletions and updates. This information is helpful for estimating how long operations will take to complete.</p> <p>Fast Recovery: TokuDB supports very fast recovery, typically less than a minute.</p>"},{"location":"tokudb/tokudb_performance_schema.html","title":"TokuDB Performance Schema Integration","text":"<p>In Percona Server for MySQL Percona Server 5.7.20-18 has implemented TokuDB integration with Performance Schema</p> <p>This integration can be used for profiling additional TokuDB operations.</p> <p>TokuDB instruments available in Performance Schema can be seen in PERFORMANCE_SCHEMA.SETUP_INSTRUMENTS table:</p> <pre><code>mysqlSELECT * FROM performance_schema.setup_instruments WHERE NAME LIKE \"%/fti/%\";\n</code></pre> <p>The output could be the following:</p> <pre><code>+------------------------------------------------------------+---------+-------+\n| NAME                                                       | ENABLED | TIMED |\n+------------------------------------------------------------+---------+-------+\n| wait/synch/mutex/fti/kibbutz_mutex                         | NO      | NO    |\n| wait/synch/mutex/fti/minicron_p_mutex                      | NO      | NO    |\n| wait/synch/mutex/fti/queue_result_mutex                    | NO      | NO    |\n| wait/synch/mutex/fti/tpool_lock_mutex                      | NO      | NO    |\n| wait/synch/mutex/fti/workset_lock_mutex                    | NO      | NO    |\n| wait/synch/mutex/fti/bjm_jobs_lock_mutex                   | NO      | NO    |\n| wait/synch/mutex/fti/log_internal_lock_mutex               | NO      | NO    |\n| wait/synch/mutex/fti/cachetable_ev_thread_lock_mutex       | NO      | NO    |\n| wait/synch/mutex/fti/cachetable_disk_nb_mutex              | NO      | NO    |\n| wait/synch/mutex/fti/safe_file_size_lock_mutex             | NO      | NO    |\n| wait/synch/mutex/fti/cachetable_m_mutex_key                | NO      | NO    |\n| wait/synch/mutex/fti/checkpoint_safe_mutex                 | NO      | NO    |\n| wait/synch/mutex/fti/ft_ref_lock_mutex                     | NO      | NO    |\n| wait/synch/mutex/fti/ft_open_close_lock_mutex              | NO      | NO    |\n| wait/synch/mutex/fti/loader_error_mutex                    | NO      | NO    |\n| wait/synch/mutex/fti/bfs_mutex                             | NO      | NO    |\n| wait/synch/mutex/fti/loader_bl_mutex                       | NO      | NO    |\n| wait/synch/mutex/fti/loader_fi_lock_mutex                  | NO      | NO    |\n| wait/synch/mutex/fti/loader_out_mutex                      | NO      | NO    |\n| wait/synch/mutex/fti/result_output_condition_lock_mutex    | NO      | NO    |\n| wait/synch/mutex/fti/block_table_mutex                     | NO      | NO    |\n| wait/synch/mutex/fti/rollback_log_node_cache_mutex         | NO      | NO    |\n| wait/synch/mutex/fti/txn_lock_mutex                        | NO      | NO    |\n| wait/synch/mutex/fti/txn_state_lock_mutex                  | NO      | NO    |\n| wait/synch/mutex/fti/txn_child_manager_mutex               | NO      | NO    |\n| wait/synch/mutex/fti/txn_manager_lock_mutex                | NO      | NO    |\n| wait/synch/mutex/fti/treenode_mutex                        | NO      | NO    |\n| wait/synch/mutex/fti/locktree_request_info_mutex           | NO      | NO    |\n| wait/synch/mutex/fti/locktree_request_info_retry_mutex_key | NO      | NO    |\n| wait/synch/mutex/fti/manager_mutex                         | NO      | NO    |\n| wait/synch/mutex/fti/manager_escalation_mutex              | NO      | NO    |\n| wait/synch/mutex/fti/db_txn_struct_i_txn_mutex             | NO      | NO    |\n| wait/synch/mutex/fti/manager_escalator_mutex               | NO      | NO    |\n| wait/synch/mutex/fti/indexer_i_indexer_lock_mutex          | NO      | NO    |\n| wait/synch/mutex/fti/indexer_i_indexer_estimate_lock_mutex | NO      | NO    |\n| wait/synch/mutex/fti/fti_probe_1                           | NO      | NO    |\n| wait/synch/rwlock/fti/multi_operation_lock                 | NO      | NO    |\n| wait/synch/rwlock/fti/low_priority_multi_operation_lock    | NO      | NO    |\n| wait/synch/rwlock/fti/cachetable_m_list_lock               | NO      | NO    |\n| wait/synch/rwlock/fti/cachetable_m_pending_lock_expensive  | NO      | NO    |\n| wait/synch/rwlock/fti/cachetable_m_pending_lock_cheap      | NO      | NO    |\n| wait/synch/rwlock/fti/cachetable_m_lock                    | NO      | NO    |\n| wait/synch/rwlock/fti/result_i_open_dbs_rwlock             | NO      | NO    |\n| wait/synch/rwlock/fti/checkpoint_safe_rwlock               | NO      | NO    |\n| wait/synch/rwlock/fti/cachetable_value                     | NO      | NO    |\n| wait/synch/rwlock/fti/safe_file_size_lock_rwlock           | NO      | NO    |\n| wait/synch/rwlock/fti/cachetable_disk_nb_rwlock            | NO      | NO    |\n| wait/synch/cond/fti/result_state_cond                      | NO      | NO    |\n| wait/synch/cond/fti/bjm_jobs_wait                          | NO      | NO    |\n| wait/synch/cond/fti/cachetable_p_refcount_wait             | NO      | NO    |\n| wait/synch/cond/fti/cachetable_m_flow_control_cond         | NO      | NO    |\n| wait/synch/cond/fti/cachetable_m_ev_thread_cond            | NO      | NO    |\n| wait/synch/cond/fti/bfs_cond                               | NO      | NO    |\n| wait/synch/cond/fti/result_output_condition                | NO      | NO    |\n| wait/synch/cond/fti/manager_m_escalator_done               | NO      | NO    |\n| wait/synch/cond/fti/lock_request_m_wait_cond               | NO      | NO    |\n| wait/synch/cond/fti/queue_result_cond                      | NO      | NO    |\n| wait/synch/cond/fti/ws_worker_wait                         | NO      | NO    |\n| wait/synch/cond/fti/rwlock_wait_read                       | NO      | NO    |\n| wait/synch/cond/fti/rwlock_wait_write                      | NO      | NO    |\n| wait/synch/cond/fti/rwlock_cond                            | NO      | NO    |\n| wait/synch/cond/fti/tp_thread_wait                         | NO      | NO    |\n| wait/synch/cond/fti/tp_pool_wait_free                      | NO      | NO    |\n| wait/synch/cond/fti/frwlock_m_wait_read                    | NO      | NO    |\n| wait/synch/cond/fti/kibbutz_k_cond                         | NO      | NO    |\n| wait/synch/cond/fti/minicron_p_condvar                     | NO      | NO    |\n| wait/synch/cond/fti/locktree_request_info_retry_cv_key     | NO      | NO    |\n| wait/io/file/fti/tokudb_data_file                          | YES     | YES   |\n| wait/io/file/fti/tokudb_load_file                          | YES     | YES   |\n| wait/io/file/fti/tokudb_tmp_file                           | YES     | YES   |\n| wait/io/file/fti/tokudb_log_file                           | YES     | YES   |\n+------------------------------------------------------------+---------+-------+\n</code></pre> <p>For TokuDB-related objects, following clauses can be used when querying Performance Schema tables:</p> <ul> <li> <p><code>WHERE EVENT_NAME LIKE '%fti%'</code> or</p> </li> <li> <p><code>WHERE NAME LIKE '%fti%'</code></p> </li> </ul> <p>For example, to get the information about TokuDB related events you can query PERFORMANCE_SCHEMA.events_waits_summary_global_by_event_name like:</p> <pre><code>mysqlSELECT * FROM performance_schema.events_waits_summary_global_by_event_name WHERE EVENT_NAME LIKE '%fti%';\n</code></pre> <p>The output could be the following:</p> <pre><code>+-----------------------------------------+------------+----------------+----------------+----------------+----------------+\n| EVENT_NAME                              | COUNT_STAR | SUM_TIMER_WAIT | MIN_TIMER_WAIT | AVG_TIMER_WAIT | MAX_TIMER_WAIT |\n+-----------------------------------------+------------+----------------+----------------+----------------+----------------+\n| wait/synch/mutex/fti/kibbutz_mutex      |          0 |              0 |              0 |              0 |              0 |\n| wait/synch/mutex/fti/minicron_p_mutex   |          0 |              0 |              0 |              0 |              0 |\n| wait/synch/mutex/fti/queue_result_mutex |          0 |              0 |              0 |              0 |              0 |\n| wait/synch/mutex/fti/tpool_lock_mutex   |          0 |              0 |              0 |              0 |              0 |\n| wait/synch/mutex/fti/workset_lock_mutex |          0 |              0 |              0 |              0 |              0 |\n...\n| wait/io/file/fti/tokudb_data_file       |         30 |      179862410 |              0 |        5995080 |       68488420 |\n| wait/io/file/fti/tokudb_load_file       |          0 |              0 |              0 |              0 |              0 |\n| wait/io/file/fti/tokudb_tmp_file        |          0 |              0 |              0 |              0 |              0 |\n| wait/io/file/fti/tokudb_log_file        |       1367 |  2925647870145 |              0 |     2140195785 |    12013357720 |\n+-----------------------------------------+------------+----------------+----------------+----------------+----------------+\n71 rows in set (0.02 sec)\n</code></pre>"},{"location":"tokudb/tokudb_quickstart.html","title":"Getting Started with TokuDB","text":""},{"location":"tokudb/tokudb_quickstart.html#system-and-hardware-requirements","title":"System and Hardware Requirements","text":"<p>Operating Systems: TokuDB is currently supported on 64-bit Linux only.</p> <p>Memory: TokuDB Requires at least 1GB of main memory but for best results, we recommend to run with at least 2GB of main memory.</p> <p>Disk space and configuration: Please make sure to allocate enough disk space for data, indexes and logs. In our users\u2019 experience, TokuDB achieves up to 25x space savings on data and indexes over InnoDB due to high compression.</p>"},{"location":"tokudb/tokudb_quickstart.html#creating-tables-and-loading-data","title":"Creating Tables and Loading Data","text":""},{"location":"tokudb/tokudb_quickstart.html#creating-tokudb-tables","title":"Creating TokuDB Tables","text":"<p>TokuDB tables are created the same way as other tables in MySQL by specifying <code>ENGINE=TokuDB</code> in the table definition. For example, the following command creates a table with a single column and uses the TokuDB storage engine to store its data:</p> <pre><code>CREATE TABLE table (\nid INT(11) NOT NULL) ENGINE=TokuDB;\n</code></pre>"},{"location":"tokudb/tokudb_quickstart.html#loading-data","title":"Loading Data","text":"<p>Once TokuDB tables have been created, data can be inserted or loaded using standard MySQL insert or bulk load operations. For example, the following command loads data from a file into the table:</p> <pre><code>LOAD DATA INFILE file\nINTO TABLE table;\n</code></pre> <p>NOTE: For more information about loading data, see the MySQL 5.6 reference manual.</p>"},{"location":"tokudb/tokudb_quickstart.html#migrating-data-from-an-existing-database","title":"Migrating Data from an Existing Database","text":"<p>Use the following command to convert an existing table for the TokuDB storage engine:</p> <pre><code>ALTER TABLE table\nENGINE=TokuDB;\n</code></pre>"},{"location":"tokudb/tokudb_quickstart.html#bulk-loading-data","title":"Bulk Loading Data","text":"<p>The TokuDB bulk loader imports data much faster than regular MySQL with InnoDB. To make use of the loader you need flat files in either comma separated or tab separated format. The MySQL <code>LOAD DATA INFILE ...</code> statement will invoke the bulk loader if the table is empty. Keep in mind that while this is the most convenient and, in most cases, the fastest way to initialize a TokuDB table, it may not be replication safe if applied to the source</p> <p>For more information, see the MySQL 5.6 Reference Manual: LOAD DATA INFILE.</p> <p>To obtain the logical backup and then bulk load into TokuDB, follow these steps:</p> <ol> <li>Create a logical backup of the original table. The easiest way to achieve this is using <code>SELECT ... INTO OUTFILE</code>. Keep in mind that the file will be created on the server.</li> </ol> <pre><code>SELECT * FROM table\nINTO OUTFILE 'file.csv';\n</code></pre> <ol> <li> <p>The output file should either be copied to the destination server or the client machine from which you plan to load it.</p> </li> <li> <p>To load the data into the server use <code>LOAD DATA INFILE</code>. If loading from a machine other than the server use the keyword <code>LOCAL</code> to point to the file on local machine. Keep in mind that you will need enough disk space on the temporary directory on the server since the local file will be copied onto the server by the MySQL client utility.</p> </li> </ol> <pre><code>LOAD DATA [LOCAL] INFILE 'file.csv';\n</code></pre> <p>It is possible to create the CSV file using either mysqldump or the MySQL client utility as well, in which case the resulting file will reside on a local directory. In these 2 cases you have to make sure to use the correct command line options to create a file compatible with <code>LOAD DATA INFILE</code>.</p> <p>The bulk loader will use more space than normal for logs and temporary files while running, make sure that your file system has enough disk space to process your load. As a rule of thumb, it should be approximately 1.5 times the size of the raw data.</p> <p>Note</p> <p>Please read the original MySQL documentation to understand the needed privileges and replication issues needed around <code>LOAD DATA INFILE</code>.</p>"},{"location":"tokudb/tokudb_quickstart.html#considerations-to-run-tokudb-in-production","title":"Considerations to Run TokuDB in Production","text":"<p>In most cases, the default options should be left in-place to run TokuDB, however it is a good idea to review some of the configuration parameters.</p>"},{"location":"tokudb/tokudb_quickstart.html#memory-allocation","title":"Memory allocation","text":"<p>TokuDB will allocate 50% of the installed RAM for its own cache (global variable tokudb_cache_size). While this is optimal in most situations, there are cases where it may lead to memory over allocation. If the system tries to allocate more memory than is available, the machine will begin swapping and run much slower than normal.</p> <p>It is necessary to set the tokudb_cache_size to a value other than the default in the following cases:</p> <ul> <li>Running other memory heavy processes on the same server as TokuDB: In many cases, the database process needs to share the system with other server processes like additional database instances, http server, application server, e-mail server, monitoring systems and others. In order to properly configure TokuDB\u2019s memory consumption, it\u2019s important to understand how much free memory will be left and assign a sensible value for TokuDB. There is no fixed rule, but a conservative choice would be 50% of available RAM while all the other processes are running. If the result is under 2 GB, you should consider moving some of the other processes to a different system or using a dedicated database server.</li> </ul> <p>tokudb_cache_size is a static variable, so it needs to be set before starting the server and cannot be changed while the server is running. For example, to set up TokuDB\u2019s cache to 4G, add the following line to your <code>my.cnf</code> file:</p> <pre><code>tokudb_cache_size = 4G\n</code></pre> <ul> <li>System using InnoDB and TokuDB: When using both the TokuDB and InnoDB storage engines, you need to manage the cache size for each. For example, on a server with 16 GB of RAM you could use the following values in your configuration file:</li> </ul> <pre><code>innodb_buffer_pool_size = 2G\ntokudb_cache_size = 8G\n</code></pre> <ul> <li>Using TokuDB with Federated or FederatedX tables: The Federated engine in MySQL and FederatedX in MariaDB allow you to connect to a table on a remote server and query it as if it were a local table (please see the MySQL documentation: 14.11. The FEDERATED Storage Engine for details). When accessing the remote table, these engines could import the complete table contents to the local server to execute a query. In this case, you will have to make sure that there is enough free memory on the server to handle these remote tables. For example, if your remote table is 8 GB in size, the server has to have more than 8 GB of free RAM to process queries against that table without going into swapping or causing a kernel panic and crash the MySQL process. There are no parameters to limit the amount of memory that the Federated or FederatedX engine will allocate while importing the remote dataset.</li> </ul>"},{"location":"tokudb/tokudb_quickstart.html#specifying-the-location-for-files","title":"Specifying the Location for Files","text":"<p>As with InnoDB, it is possible to specify different locations than the default for TokuDB\u2019s data, log and temporary files. This way you may distribute the load and control the disk space. The following variables control file location:</p> <ul> <li> <p>tokudb_data_dir: This variable defines the directory where the TokuDB tables are stored. The default location for TokuDB\u2019s data files is the MySQL data directory.</p> </li> <li> <p>tokudb_log_dir: This variable defines the directory where the TokuDB log files are stored. The default location for TokuDB\u2019s log files is the MySQL data directory. Configuring a separate log directory is somewhat involved and should be done only if absolutely necessary. We recommend to keep the data and log files under the same directory.</p> </li> <li> <p>tokudb_tmp_dir: This variable defines the directory where the TokuDB bulk loader stores temporary files. The bulk loader can create large temporary files while it is loading a table, so putting these temporary files on a disk separate from the data directory can be useful. For example, it can make sense to use a high-performance disk for the data directory and a very inexpensive disk for the temporary directory. The default location for TokuDB\u2019s temporary files is the MySQL data directory.</p> </li> </ul>"},{"location":"tokudb/tokudb_quickstart.html#table-maintenance","title":"Table Maintenance","text":"<p>Overview</p> <p>The fractal tree provides fast performance by inserting small messages in the buffers in the fractal trees instead of requiring a potential IO for an update on every row in the table as required by a B-tree. Additional background information on how fractal trees operate can be found here. For tables whose workload pattern is a high number of sequential deletes, it may be beneficial to flush these delete messages down to the basement nodes in order to allow for faster access. The way to perform this operation is via the <code>OPTIMIZE</code> command.</p> <p>The following extensions to the <code>OPTIMIZE</code> command have been added in TokuDB version 7.5.5:</p> <ul> <li>Hot Optimize Throttling</li> </ul> <p>By default, table optimization will run with all available resources. To limit the amount of resources, it is possible to limit the speed of table optimization. The tokudb_optimize_throttle session variable determines an upper bound on how many fractal tree leaf nodes per second are optimized. The default is 0 (no upper bound) with a valid range of [0,1000000]. For example, to limit the table optimization to 1 leaf node per second, use the following setting:</p> <pre><code>SET tokudb_optimize_throttle=1;\n</code></pre> <ul> <li>Optimize a Single Index of a Table</li> </ul> <p>To optimize a single index in a table, the tokudb_optimize_index_name session variable can be set to select the index by name. For example, to optimize the primary key of a table:</p> <p><pre><code>SET tokudb_optimize_index_name='primary';\nOPTIMIZE TABLE t;\n</code></pre> * Optimize a Subset of a Fractal Tree Index</p> <p>For patterns where the left side of the tree has many deletions (a common pattern with increasing id or date values), it may be useful to delete a percentage of the tree. In this case, it is possible to optimize a subset of a fractal tree starting at the left side. The tokudb_optimize_index_fraction session variable controls the size of the sub tree. Valid values are in the range [0.0,1.0] with default 1.0 (optimize the whole tree). For example, to optimize the leftmost 10% of the primary key:</p> <pre><code>SET tokudb_optimize_index_name='primary';\nSET tokudb_optimize_index_fraction=0.1;\nOPTIMIZE TABLE t;\n</code></pre>"},{"location":"tokudb/tokudb_status_variables.html","title":"TokuDB Status Variables","text":"<p>TokuDB status variables provide details about the inner workings of TokuDB storage engine and they can be useful in tuning the storage engine to a particular environment.</p> <p>You can view these variables and their values by running:</p> <pre><code>mysqlSHOW STATUS LIKE 'tokudb%';\n</code></pre>"},{"location":"tokudb/tokudb_status_variables.html#tokudb-status-variables-summary","title":"TokuDB Status Variables Summary","text":"<p>The following global status variables are available:</p> Name Var Type Tokudb_DB_OPENS integer Tokudb_DB_CLOSES integer Tokudb_DB_OPEN_CURRENT integer Tokudb_DB_OPEN_MAX integer Tokudb_LEAF_ENTRY_MAX_COMMITTED_XR integer Tokudb_LEAF_ENTRY_MAX_PROVISIONAL_XR integer Tokudb_LEAF_ENTRY_EXPANDED integer Tokudb_LEAF_ENTRY_MAX_MEMSIZE integer Tokudb_LEAF_ENTRY_APPLY_GC_BYTES_IN integer Tokudb_LEAF_ENTRY_APPLY_GC_BYTES_OUT integer Tokudb_LEAF_ENTRY_NORMAL_GC_BYTES_IN integer Tokudb_LEAF_ENTRY_NORMAL_GC_BYTES_OUT integer Tokudb_CHECKPOINT_PERIOD integer Tokudb_CHECKPOINT_FOOTPRINT integer Tokudb_CHECKPOINT_LAST_BEGAN datetime Tokudb_CHECKPOINT_LAST_COMPLETE_BEGAN datetime Tokudb_CHECKPOINT_LAST_COMPLETE_ENDED datetime [Tokudb_CHECKPOINT_DURATION](#tokudbcheckpointduration integer Tokudb_CHECKPOINT_DURATION_LAST integer Tokudb_CHECKPOINT_LAST_LSN integer Tokudb_CHECKPOINT_TAKEN integer Tokudb_CHECKPOINT_FAILED integer Tokudb_CHECKPOINT_WAITERS_NOW integer Tokudb_CHECKPOINT_WAITERS_MAX integer Tokudb_CHECKPOINT_CLIENT_WAIT_ON_MO integer Tokudb_CHECKPOINT_CLIENT_WAIT_ON_CS integer Tokudb_CHECKPOINT_BEGIN_TIME integer Tokudb_CHECKPOINT_LONG_BEGIN_TIME integer Tokudb_CHECKPOINT_LONG_BEGIN_COUNT integer Tokudb_CHECKPOINT_END_TIME integer Tokudb_CHECKPOINT_LONG_END_TIME integer Tokudb_CHECKPOINT_LONG_END_COUNT integer Tokudb_CACHETABLE_MISS integer Tokudb_CACHETABLE_MISS_TIME integer Tokudb_CACHETABLE_PREFETCHES integer Tokudb_CACHETABLE_SIZE_CURRENT integer Tokudb_CACHETABLE_SIZE_LIMIT integer Tokudb_CACHETABLE_SIZE_WRITING integer Tokudb_CACHETABLE_SIZE_NONLEAF integer Tokudb_CACHETABLE_SIZE_LEAF integer Tokudb_CACHETABLE_SIZE_ROLLBACK integer Tokudb_CACHETABLE_SIZE_CACHEPRESSURE integer Tokudb_CACHETABLE_SIZE_CLONED integer Tokudb_CACHETABLE_EVICTIONS integer Tokudb_CACHETABLE_CLEANER_EXECUTIONS integer Tokudb_CACHETABLE_CLEANER_PERIOD integer Tokudb_CACHETABLE_CLEANER_ITERATIONS integer Tokudb_CACHETABLE_WAIT_PRESSURE_COUNT integer Tokudb_CACHETABLE_WAIT_PRESSURE_TIME integer Tokudb_CACHETABLE_LONG_WAIT_PRESSURE_COUNT integer Tokudb_CACHETABLE_LONG_WAIT_PRESSURE_TIME integer Tokudb_CACHETABLE_POOL_CLIENT_NUM_THREADS integer Tokudb_CACHETABLE_POOL_CLIENT_NUM_THREADS_ACTIVE integer Tokudb_CACHETABLE_POOL_CLIENT_QUEUE_SIZE integer Tokudb_CACHETABLE_POOL_CLIENT_MAX_QUEUE_SIZE integer Tokudb_CACHETABLE_POOL_CLIENT_TOTAL_ITEMS_PROCESSED integer Tokudb_CACHETABLE_POOL_CLIENT_TOTAL_EXECUTION_TIME integer Tokudb_CACHETABLE_POOL_CACHETABLE_NUM_THREADS integer Tokudb_CACHETABLE_POOL_CACHETABLE_NUM_THREADS_ACTIVE integer Tokudb_CACHETABLE_POOL_CACHETABLE_QUEUE_SIZE integer Tokudb_CACHETABLE_POOL_CACHETABLE_MAX_QUEUE_SIZE integer Tokudb_CACHETABLE_POOL_CACHETABLE_TOTAL_ITEMS_PROCESSED integer Tokudb_CACHETABLE_POOL_CACHETABLE_TOTAL_EXECUTION_TIME integer Tokudb_CACHETABLE_POOL_CHECKPOINT_NUM_THREADS integer Tokudb_CACHETABLE_POOL_CHECKPOINT_NUM_THREADS_ACTIVE integer Tokudb_CACHETABLE_POOL_CHECKPOINT_QUEUE_SIZE integer Tokudb_CACHETABLE_POOL_CHECKPOINT_MAX_QUEUE_SIZE integer Tokudb_CACHETABLE_POOL_CHECKPOINT_TOTAL_ITEMS_PROCESSED integer Tokudb_CACHETABLE_POOL_CHECKPOINT_TOTAL_EXECUTION_TIME integer Tokudb_LOCKTREE_MEMORY_SIZE integer Tokudb_LOCKTREE_MEMORY_SIZE_LIMIT integer Tokudb_LOCKTREE_ESCALATION_NUM integer Tokudb_LOCKTREE_ESCALATION_SECONDS numeric Tokudb_LOCKTREE_LATEST_POST_ESCALATION_MEMORY_SIZE integer Tokudb_LOCKTREE_OPEN_CURRENT integer Tokudb_LOCKTREE_PENDING_LOCK_REQUESTS integer Tokudb_LOCKTREE_STO_ELIGIBLE_NUM integer Tokudb_LOCKTREE_STO_ENDED_NUM integer Tokudb_LOCKTREE_STO_ENDED_SECONDS numeric Tokudb_LOCKTREE_WAIT_COUNT integer Tokudb_LOCKTREE_WAIT_TIME integer Tokudb_LOCKTREE_LONG_WAIT_COUNT integer Tokudb_LOCKTREE_LONG_WAIT_TIME integer Tokudb_LOCKTREE_TIMEOUT_COUNT integer Tokudb_LOCKTREE_WAIT_ESCALATION_COUNT integer Tokudb_LOCKTREE_WAIT_ESCALATION_TIME integer Tokudb_LOCKTREE_LONG_WAIT_ESCALATION_COUNT integer Tokudb_LOCKTREE_LONG_WAIT_ESCALATION_TIME integer Tokudb_DICTIONARY_UPDATES integer Tokudb_DICTIONARY_BROADCAST_UPDATES integer Tokudb_DESCRIPTOR_SET integer Tokudb_MESSAGES_IGNORED_BY_LEAF_DUE_TO_MSN integer Tokudb_TOTAL_SEARCH_RETRIES integer Tokudb_SEARCH_TRIES_GT_HEIGHT integer Tokudb_SEARCH_TRIES_GT_HEIGHTPLUS3 integer Tokudb_LEAF_NODES_FLUSHED_NOT_CHECKPOINT integer Tokudb_LEAF_NODES_FLUSHED_NOT_CHECKPOINT_BYTES integer Tokudb_LEAF_NODES_FLUSHED_NOT_CHECKPOINT_UNCOMPRESSED_BYTES integer Tokudb_LEAF_NODES_FLUSHED_NOT_CHECKPOINT_SECONDS numeric Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_NOT_CHECKPOINT integer Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_NOT_CHECKPOINT_BYTES integer Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_NOT_CHECKPOINT_UNCOMPRESSE integer Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_NOT_CHECKPOINT_SECONDS numeric Tokudb_LEAF_NODES_FLUSHED_CHECKPOINT integer Tokudb_LEAF_NODES_FLUSHED_CHECKPOINT_BYTES integer Tokudb_LEAF_NODES_FLUSHED_CHECKPOINT_UNCOMPRESSED_BYTES integer Tokudb_LEAF_NODES_FLUSHED_CHECKPOINT_SECONDS numeric Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_CHECKPOINT integer Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_CHECKPOINT_BYTES integer Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_CHECKPOINT_UNCOMPRESSED_BY integer Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_CHECKPOINT_SECONDS numeric Tokudb_LEAF_NODE_COMPRESSION_RATIO numeric Tokudb_NONLEAF_NODE_COMPRESSION_RATIO numeric Tokudb_OVERALL_NODE_COMPRESSION_RATIO numeric Tokudb_NONLEAF_NODE_PARTIAL_EVICTIONS numeric Tokudb_NONLEAF_NODE_PARTIAL_EVICTIONS_BYTES integer Tokudb_LEAF_NODE_PARTIAL_EVICTIONS integer Tokudb_LEAF_NODE_PARTIAL_EVICTIONS_BYTES integer Tokudb_LEAF_NODE_FULL_EVICTIONS integer Tokudb_LEAF_NODE_FULL_EVICTIONS_BYTES integer Tokudb_NONLEAF_NODE_FULL_EVICTIONS integer Tokudb_NONLEAF_NODE_FULL_EVICTIONS_BYTES integer Tokudb_LEAF_NODES_CREATED integer Tokudb_NONLEAF_NODES_CREATED integer Tokudb_LEAF_NODES_DESTROYED integer Tokudb_NONLEAF_NODES_DESTROYED integer Tokudb_MESSAGES_INJECTED_AT_ROOT_BYTES integer Tokudb_MESSAGES_FLUSHED_FROM_H1_TO_LEAVES_BYTES integer Tokudb_MESSAGES_IN_TREES_ESTIMATE_BYTES integer Tokudb_MESSAGES_INJECTED_AT_ROOT integer Tokudb_BROADCASE_MESSAGES_INJECTED_AT_ROOT integer Tokudb_BASEMENTS_DECOMPRESSED_TARGET_QUERY integer Tokudb_BASEMENTS_DECOMPRESSED_PRELOCKED_RANGE integer Tokudb_BASEMENTS_DECOMPRESSED_PREFETCH integer Tokudb_BASEMENTS_DECOMPRESSED_FOR_WRITE integer Tokudb_BUFFERS_DECOMPRESSED_TARGET_QUERY integer Tokudb_BUFFERS_DECOMPRESSED_PRELOCKED_RANGE integer Tokudb_BUFFERS_DECOMPRESSED_PREFETCH integer Tokudb_BUFFERS_DECOMPRESSED_FOR_WRITE integer Tokudb_PIVOTS_FETCHED_FOR_QUERY integer Tokudb_PIVOTS_FETCHED_FOR_QUERY_BYTES integer Tokudb_PIVOTS_FETCHED_FOR_QUERY_SECONDS numeric Tokudb_PIVOTS_FETCHED_FOR_PREFETCH integer Tokudb_PIVOTS_FETCHED_FOR_PREFETCH_BYTES integer Tokudb_PIVOTS_FETCHED_FOR_PREFETCH_SECONDS numeric Tokudb_PIVOTS_FETCHED_FOR_WRITE integer Tokudb_PIVOTS_FETCHED_FOR_WRITE_BYTES integer Tokudb_PIVOTS_FETCHED_FOR_WRITE_SECONDS numeric Tokudb_BASEMENTS_FETCHED_TARGET_QUERY integer Tokudb_BASEMENTS_FETCHED_TARGET_QUERY_BYTES integer Tokudb_BASEMENTS_FETCHED_TARGET_QUERY_SECONDS numeric Tokudb_BASEMENTS_FETCHED_PRELOCKED_RANGE integer Tokudb_BASEMENTS_FETCHED_PRELOCKED_RANGE_BYTES integer Tokudb_BASEMENTS_FETCHED_PRELOCKED_RANGE_SECONDS numeric Tokudb_BASEMENTS_FETCHED_PREFETCH integer Tokudb_BASEMENTS_FETCHED_PREFETCH_BYTES integer Tokudb_BASEMENTS_FETCHED_PREFETCH_SECONDS numeric Tokudb_BASEMENTS_FETCHED_FOR_WRITE integer Tokudb_BASEMENTS_FETCHED_FOR_WRITE_BYTES integer Tokudb_BASEMENTS_FETCHED_FOR_WRITE_SECONDS numeric Tokudb_BUFFERS_FETCHED_TARGET_QUERY integer Tokudb_BUFFERS_FETCHED_TARGET_QUERY_BYTES integer Tokudb_BUFFERS_FETCHED_TARGET_QUERY_SECONDS numeric Tokudb_BUFFERS_FETCHED_PRELOCKED_RANGE integer Tokudb_BUFFERS_FETCHED_PRELOCKED_RANGE_BYTES integer Tokudb_BUFFERS_FETCHED_PRELOCKED_RANGE_SECONDS numeric Tokudb_BUFFERS_FETCHED_PREFETCH integer Tokudb_BUFFERS_FETCHED_PREFETCH_BYTES integer Tokudb_BUFFERS_FETCHED_PREFETCH_SECONDS numeric Tokudb_BUFFERS_FETCHED_FOR_WRITE integer Tokudb_BUFFERS_FETCHED_FOR_WRITE_BYTES integer Tokudb_BUFFERS_FETCHED_FOR_WRITE_SECONDS integer Tokudb_LEAF_COMPRESSION_TO_MEMORY_SECONDS numeric Tokudb_LEAF_SERIALIZATION_TO_MEMORY_SECONDS numeric Tokudb_LEAF_DECOMPRESSION_TO_MEMORY_SECONDS numeric Tokudb_LEAF_DESERIALIZATION_TO_MEMORY_SECONDS numeric Tokudb_NONLEAF_COMPRESSION_TO_MEMORY_SECONDS numeric Tokudb_NONLEAF_SERIALIZATION_TO_MEMORY_SECONDS numeric Tokudb_NONLEAF_DECOMPRESSION_TO_MEMORY_SECONDS numeric Tokudb_NONLEAF_DESERIALIZATION_TO_MEMORY_SECONDS numeric Tokudb_PROMOTION_ROOTS_SPLIT integer Tokudb_PROMOTION_LEAF_ROOTS_INJECTED_INTO integer Tokudb_PROMOTION_H1_ROOTS_INJECTED_INTO integer Tokudb_PROMOTION_INJECTIONS_AT_DEPTH_0 integer Tokudb_PROMOTION_INJECTIONS_AT_DEPTH_1 integer Tokudb_PROMOTION_INJECTIONS_AT_DEPTH_2 integer Tokudb_PROMOTION_INJECTIONS_AT_DEPTH_3 integer Tokudb_PROMOTION_INJECTIONS_LOWER_THAN_DEPTH_3 integer Tokudb_PROMOTION_STOPPED_NONEMPTY_BUFFER integer Tokudb_PROMOTION_STOPPED_AT_HEIGHT_1 integer Tokudb_PROMOTION_STOPPED_CHILD_LOCKED_OR_NOT_IN_MEMORY integer Tokudb_PROMOTION_STOPPED_CHILD_NOT_FULLY_IN_MEMORY integer Tokudb_PROMOTION_STOPPED_AFTER_LOCKING_CHILD integer Tokudb_BASEMENT_DESERIALIZATION_FIXED_KEY integer Tokudb_BASEMENT_DESERIALIZATION_VARIABLE_KEY integer Tokudb_PRO_RIGHTMOST_LEAF_SHORTCUT_SUCCESS integer Tokudb_PRO_RIGHTMOST_LEAF_SHORTCUT_FAIL_POS integer Tokudb_RIGHTMOST_LEAF_SHORTCUT_FAIL_REACTIVE integer Tokudb_CURSOR_SKIP_DELETED_LEAF_ENTRY integer Tokudb_FLUSHER_CLEANER_TOTAL_NODES integer Tokudb_FLUSHER_CLEANER_H1_NODES integer Tokudb_FLUSHER_CLEANER_HGT1_NODES integer Tokudb_FLUSHER_CLEANER_EMPTY_NODES integer Tokudb_FLUSHER_CLEANER_NODES_DIRTIED integer Tokudb_FLUSHER_CLEANER_MAX_BUFFER_SIZE integer Tokudb_FLUSHER_CLEANER_MIN_BUFFER_SIZE integer Tokudb_FLUSHER_CLEANER_TOTAL_BUFFER_SIZE integer Tokudb_FLUSHER_CLEANER_MAX_BUFFER_WORKDONE integer Tokudb_FLUSHER_CLEANER_MIN_BUFFER_WORKDONE integer Tokudb_FLUSHER_CLEANER_TOTAL_BUFFER_WORKDONE integer Tokudb_FLUSHER_CLEANER_NUM_LEAF_MERGES_STARTED integer Tokudb_FLUSHER_CLEANER_NUM_LEAF_MERGES_RUNNING integer Tokudb_FLUSHER_CLEANER_NUM_LEAF_MERGES_COMPLETED integer Tokudb_FLUSHER_CLEANER_NUM_DIRTIED_FOR_LEAF_MERGE integer Tokudb_FLUSHER_FLUSH_TOTAL integer Tokudb_FLUSHER_FLUSH_IN_MEMORY integer Tokudb_FLUSHER_FLUSH_NEEDED_IO integer Tokudb_FLUSHER_FLUSH_CASCADES integer Tokudb_FLUSHER_FLUSH_CASCADES_1 integer Tokudb_FLUSHER_FLUSH_CASCADES_2 integer Tokudb_FLUSHER_FLUSH_CASCADES_3 integer Tokudb_FLUSHER_FLUSH_CASCADES_4 integer Tokudb_FLUSHER_FLUSH_CASCADES_5 integer Tokudb_FLUSHER_FLUSH_CASCADES_GT_5 integer Tokudb_FLUSHER_SPLIT_LEAF integer Tokudb_FLUSHER_SPLIT_NONLEAF integer Tokudb_FLUSHER_MERGE_LEAF integer Tokudb_FLUSHER_MERGE_NONLEAF integer Tokudb_FLUSHER_BALANCE_LEAF integer Tokudb_HOT_NUM_STARTED integer Tokudb_HOT_NUM_COMPLETED integer Tokudb_HOT_NUM_ABORTED integer Tokudb_HOT_MAX_ROOT_FLUSH_COUNT integer Tokudb_TXN_BEGIN integer Tokudb_TXN_BEGIN_READ_ONLY integer Tokudb_TXN_COMMITS integer Tokudb_TXN_ABORTS integer Tokudb_LOGGER_NEXT_LSN integer Tokudb_LOGGER_WRITES integer Tokudb_LOGGER_WRITES_BYTES integer Tokudb_LOGGER_WRITES_UNCOMPRESSED_BYTES integer Tokudb_LOGGER_WRITES_SECONDS numeric Tokudb_LOGGER_WAIT_LONG integer Tokudb_LOADER_NUM_CREATED integer Tokudb_LOADER_NUM_CURRENT integer Tokudb_LOADER_NUM_MAX integer Tokudb_MEMORY_MALLOC_COUNT integer Tokudb_MEMORY_FREE_COUNT integer Tokudb_MEMORY_REALLOC_COUNT integer Tokudb_MEMORY_MALLOC_FAIL integer Tokudb_MEMORY_REALLOC_FAIL integer Tokudb_MEMORY_REQUESTED integer Tokudb_MEMORY_USED integer Tokudb_MEMORY_FREED integer Tokudb_MEMORY_MAX_REQUESTED_SIZE integer Tokudb_MEMORY_LAST_FAILED_SIZE integer Tokudb_MEM_ESTIMATED_MAXIMUM_MEMORY_FOOTPRINT integer Tokudb_MEMORY_MALLOCATOR_VERSION string Tokudb_MEMORY_MMAP_THRESHOLD integer Tokudb_FILESYSTEM_THREADS_BLOCKED_BY_FULL_DISK integer Tokudb_FILESYSTEM_FSYNC_TIME integer Tokudb_FILESYSTEM_FSYNC_NUM integer Tokudb_FILESYSTEM_LONG_FSYNC_TIME integer Tokudb_FILESYSTEM_LONG_FSYNC_NUM integer"},{"location":"tokudb/tokudb_status_variables.html#tokudb_db_opens","title":"<code>Tokudb_DB_OPENS</code>","text":"<p>This variable shows the number of times an individual PerconaFT dictionary file was opened. This is a not a useful value for a regular user to use for any purpose due to layers of open/close caching on top.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_db_closes","title":"<code>Tokudb_DB_CLOSES</code>","text":"<p>This variable shows the number of times an individual PerconaFT dictionary file was closed. This is a not a useful value for a regular user to use for any purpose due to layers of open/close caching on top.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_db_open_current","title":"<code>Tokudb_DB_OPEN_CURRENT</code>","text":"<p>This variable shows the number of currently opened databases.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_db_open_max","title":"<code>Tokudb_DB_OPEN_MAX</code>","text":"<p>This variable shows the maximum number of concurrently opened databases.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_entry_max_committed_xr","title":"<code>Tokudb_LEAF_ENTRY_MAX_COMMITTED_XR</code>","text":"<p>This variable shows the maximum number of committed transaction records that were stored on disk in a new or modified row.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_entry_max_provisional_xr","title":"<code>Tokudb_LEAF_ENTRY_MAX_PROVISIONAL_XR</code>","text":"<p>This variable shows the maximum number of provisional transaction records that were stored on disk in a new or modified row.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_entry_expanded","title":"<code>Tokudb_LEAF_ENTRY_EXPANDED</code>","text":"<p>This variable shows the number of times that an expanded memory mechanism was used to store a new or modified row on disk.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_entry_max_memsize","title":"<code>Tokudb_LEAF_ENTRY_MAX_MEMSIZE</code>","text":"<p>This variable shows the maximum number of bytes that were stored on disk as a new or modified row. This is the maximum uncompressed size of any row stored in TokuDB that was created or modified since the server started.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_entry_apply_gc_bytes_in","title":"<code>Tokudb_LEAF_ENTRY_APPLY_GC_BYTES_IN</code>","text":"<p>This variable shows the total number of bytes of leaf nodes data before performing garbage collection for non-flush events.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_entry_apply_gc_bytes_out","title":"<code>Tokudb_LEAF_ENTRY_APPLY_GC_BYTES_OUT</code>","text":"<p>This variable shows the total number of bytes of leaf nodes data after performing garbage collection for non-flush events.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_entry_normal_gc_bytes_in","title":"<code>Tokudb_LEAF_ENTRY_NORMAL_GC_BYTES_IN</code>","text":"<p>This variable shows the total number of bytes of leaf nodes data before performing garbage collection for flush events.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_entry_normal_gc_bytes_out","title":"<code>Tokudb_LEAF_ENTRY_NORMAL_GC_BYTES_OUT</code>","text":"<p>This variable shows the total number of bytes of leaf nodes data after performing garbage collection for flush events.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_period","title":"<code>Tokudb_CHECKPOINT_PERIOD</code>","text":"<p>This variable shows the interval in seconds between the end of an automatic checkpoint and the beginning of the next automatic checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_footprint","title":"<code>Tokudb_CHECKPOINT_FOOTPRINT</code>","text":"<p>This variable shows at what stage the checkpointer is at. It\u2019s used for debugging purposes only and not a useful value for a normal user.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_last_began","title":"<code>Tokudb_CHECKPOINT_LAST_BEGAN</code>","text":"<p>This variable shows the time the last checkpoint began. If a checkpoint is currently in progress, then this time may be later than the time the last checkpoint completed. If no checkpoint has ever taken place, then this value will be <code>Dec 31, 1969</code> on Linux hosts.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_last_complete_began","title":"<code>Tokudb_CHECKPOINT_LAST_COMPLETE_BEGAN</code>","text":"<p>This variable shows the time the last complete checkpoint started. Any data that changed after this time will not be captured in the checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_last_complete_ended","title":"<code>Tokudb_CHECKPOINT_LAST_COMPLETE_ENDED</code>","text":"<p>This variable shows the time the last complete checkpoint ended.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_duration","title":"<code>Tokudb_CHECKPOINT_DURATION</code>","text":"<p>This variable shows time (in seconds) required to complete all checkpoints.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_duration_last","title":"<code>Tokudb_CHECKPOINT_DURATION_LAST</code>","text":"<p>This variable shows time (in seconds) required to complete the last checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_last_lsn","title":"<code>Tokudb_CHECKPOINT_LAST_LSN</code>","text":"<p>This variable shows the last successful checkpoint LSN. Each checkpoint from the time the PerconaFT environment is created has a monotonically incrementing LSN. This is not a useful value for a normal user to use for any purpose other than having some idea of how many checkpoints have occurred since the system was first created.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_taken","title":"<code>Tokudb_CHECKPOINT_TAKEN</code>","text":"<p>This variable shows the number of complete checkpoints that have been taken.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_failed","title":"<code>Tokudb_CHECKPOINT_FAILED</code>","text":"<p>This variable shows the number of checkpoints that have failed for any reason.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_waiters_now","title":"<code>Tokudb_CHECKPOINT_WAITERS_NOW</code>","text":"<p>This variable shows the current number of threads waiting for the <code>checkpoint safe</code> lock. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_waiters_max","title":"<code>Tokudb_CHECKPOINT_WAITERS_MAX</code>","text":"<p>This variable shows the maximum number of threads that concurrently waited for the <code>checkpoint safe</code> lock. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_client_wait_on_mo","title":"<code>Tokudb_CHECKPOINT_CLIENT_WAIT_ON_MO</code>","text":"<p>This variable shows the number of times a non-checkpoint client thread waited for the multi-operation lock. It is an internal <code>rwlock</code> that is similar in nature to the InnoDB kernel mutex, it effectively halts all access to the PerconaFT API when write locked. The <code>begin</code> phase of the checkpoint takes this lock for a brief period.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_client_wait_on_cs","title":"<code>Tokudb_CHECKPOINT_CLIENT_WAIT_ON_CS</code>","text":"<p>This variable shows the number of times a non-checkpoint client thread waited for the checkpoint-safe lock. This is the lock taken when you <code>SET tokudb_checkpoint_lock=1</code>. If a client trying to lock/postpone the checkpointer has to wait for the currently running checkpoint to complete, that wait time will be reflected here and summed. This is not a useful metric as regular users should never be manipulating the checkpoint lock.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_begin_time","title":"<code>Tokudb_CHECKPOINT_BEGIN_TIME</code>","text":"<p>This variable shows the cumulative time (in microseconds) required to mark all dirty nodes as pending a checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_long_begin_time","title":"<code>Tokudb_CHECKPOINT_LONG_BEGIN_TIME</code>","text":"<p>This variable shows the cumulative actual time (in microseconds) of checkpoint <code>begin</code> stages that took longer than 1 second.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_long_begin_count","title":"<code>Tokudb_CHECKPOINT_LONG_BEGIN_COUNT</code>","text":"<p>This variable shows the number of checkpoints whose <code>begin</code> stage took longer than 1 second.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_end_time","title":"<code>Tokudb_CHECKPOINT_END_TIME</code>","text":"<p>This variable shows the time spent in checkpoint end operation in seconds.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_long_end_time","title":"<code>Tokudb_CHECKPOINT_LONG_END_TIME</code>","text":"<p>This variable shows the total time of long checkpoints in seconds.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_checkpoint_long_end_count","title":"<code>Tokudb_CHECKPOINT_LONG_END_COUNT</code>","text":"<p>This variable shows the number of checkpoints whose <code>end_checkpoint</code> operations exceeded 1 minute.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_miss","title":"<code>Tokudb_CACHETABLE_MISS</code>","text":"<p>This variable shows the number of times the application was unable to access the data in the internal cache. A cache miss means that date will need to be read from disk.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_miss_time","title":"<code>Tokudb_CACHETABLE_MISS_TIME</code>","text":"<p>This variable shows the total time, in microseconds, of how long the database has had to wait for a disk read to complete.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_prefetches","title":"<code>Tokudb_CACHETABLE_PREFETCHES</code>","text":"<p>This variable shows the total number of times that a block of memory has been prefetched into the database\u2019s cache. Data is prefetched when the database\u2019s algorithms determine that a block of memory is likely to be accessed by the application.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_size_current","title":"<code>Tokudb_CACHETABLE_SIZE_CURRENT</code>","text":"<p>This variable shows how much of the uncompressed data, in bytes, is currently in the database\u2019s internal cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_size_limit","title":"<code>Tokudb_CACHETABLE_SIZE_LIMIT</code>","text":"<p>This variable shows how much of the uncompressed data, in bytes, will fit in the database\u2019s internal cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_size_writing","title":"<code>Tokudb_CACHETABLE_SIZE_WRITING</code>","text":"<p>This variable shows the number of bytes that are currently queued up to be written to disk.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_size_nonleaf","title":"<code>Tokudb_CACHETABLE_SIZE_NONLEAF</code>","text":"<p>This variable shows the amount of memory, in bytes, the current set of non-leaf nodes occupy in the cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_size_leaf","title":"<code>Tokudb_CACHETABLE_SIZE_LEAF</code>","text":"<p>This variable shows the amount of memory, in bytes, the current set of (decompressed) leaf nodes occupy in the cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_size_rollback","title":"<code>Tokudb_CACHETABLE_SIZE_ROLLBACK</code>","text":"<p>This variable shows the rollback nodes size, in bytes, in the cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_size_cachepressure","title":"<code>Tokudb_CACHETABLE_SIZE_CACHEPRESSURE</code>","text":"<p>This variable shows the number of bytes causing cache pressure (the sum of buffers and work done counters), helps to understand if cleaner threads are keeping up with workload. It should really be looked at as more of a value to use in a ratio of cache pressure / cache table size. The closer that ratio evaluates to 1, the higher the cache pressure.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_size_cloned","title":"<code>Tokudb_CACHETABLE_SIZE_CLONED</code>","text":"<p>This variable shows the amount of memory, in bytes, currently used for cloned nodes. During the checkpoint operation, dirty nodes are cloned prior to serialization/compression, then written to disk. After which, the memory for the cloned block is returned for re-use.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_evictions","title":"<code>Tokudb_CACHETABLE_EVICTIONS</code>","text":"<p>This variable shows the number of blocks evicted from cache. On its own this is not a useful number as its impact on performance depends entirely on the hardware and workload in use. For example, two workloads, one random, one linear for the same starting data set will have two wildly different eviction patterns.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_cleaner_executions","title":"<code>Tokudb_CACHETABLE_CLEANER_EXECUTIONS</code>","text":"<p>This variable shows the total number of times the cleaner thread loop has executed.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_cleaner_period","title":"<code>Tokudb_CACHETABLE_CLEANER_PERIOD</code>","text":"<p>TokuDB includes a cleaner thread that optimizes indexes in the background. This variable is the time, in seconds, between the completion of a group of cleaner operations and the beginning of the next group of cleaner operations. The cleaner operations run on a background thread performing work that does not need to be done on the client thread.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_cleaner_iterations","title":"<code>Tokudb_CACHETABLE_CLEANER_ITERATIONS</code>","text":"<p>This variable shows the number of cleaner operations that are performed every cleaner period.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_wait_pressure_count","title":"<code>Tokudb_CACHETABLE_WAIT_PRESSURE_COUNT</code>","text":"<p>This variable shows the number of times a thread was stalled due to cache pressure.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_wait_pressure_time","title":"<code>Tokudb_CACHETABLE_WAIT_PRESSURE_TIME</code>","text":"<p>This variable shows the total time, in microseconds, waiting on cache pressure to subside.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_long_wait_pressure_count","title":"<code>Tokudb_CACHETABLE_LONG_WAIT_PRESSURE_COUNT</code>","text":"<p>This variable shows the number of times a thread was stalled for more than one second due to cache pressure.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_long_wait_pressure_time","title":"<code>Tokudb_CACHETABLE_LONG_WAIT_PRESSURE_TIME</code>","text":"<p>This variable shows the total time, in microseconds, waiting on cache pressure to subside for more than one second.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_client_num_threads","title":"<code>Tokudb_CACHETABLE_POOL_CLIENT_NUM_THREADS</code>","text":"<p>This variable shows the number of threads in the client thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_client_num_threads_active","title":"<code>Tokudb_CACHETABLE_POOL_CLIENT_NUM_THREADS_ACTIVE</code>","text":"<p>This variable shows the number of currently active threads in the client thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_client_queue_size","title":"<code>Tokudb_CACHETABLE_POOL_CLIENT_QUEUE_SIZE</code>","text":"<p>This variable shows the number of currently queued work items in the client thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_client_max_queue_size","title":"<code>Tokudb_CACHETABLE_POOL_CLIENT_MAX_QUEUE_SIZE</code>","text":"<p>This variable shows the largest number of queued work items in the client thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_client_total_items_processed","title":"<code>Tokudb_CACHETABLE_POOL_CLIENT_TOTAL_ITEMS_PROCESSED</code>","text":"<p>This variable shows the total number of work items processed in the client thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_client_total_execution_time","title":"<code>Tokudb_CACHETABLE_POOL_CLIENT_TOTAL_EXECUTION_TIME</code>","text":"<p>This variable shows the total execution time of processing work items in the client thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_cachetable_num_threads","title":"<code>Tokudb_CACHETABLE_POOL_CACHETABLE_NUM_THREADS</code>","text":"<p>This variable shows the number of threads in the cachetable threadpool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_cachetable_num_threads_active","title":"<code>Tokudb_CACHETABLE_POOL_CACHETABLE_NUM_THREADS_ACTIVE</code>","text":"<p>This variable shows the number of currently active threads in the cachetable thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_cachetable_queue_size","title":"<code>Tokudb_CACHETABLE_POOL_CACHETABLE_QUEUE_SIZE</code>","text":"<p>This variable shows the number of currently queued work items in the cachetable thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_cachetable_max_queue_size","title":"<code>Tokudb_CACHETABLE_POOL_CACHETABLE_MAX_QUEUE_SIZE</code>","text":"<p>This variable shows the largest number of queued work items in the cachetable thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_cachetable_total_items_processed","title":"<code>Tokudb_CACHETABLE_POOL_CACHETABLE_TOTAL_ITEMS_PROCESSED</code>","text":"<p>This variable shows the total number of work items processed in the cachetable thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_cachetable_total_execution_time","title":"<code>Tokudb_CACHETABLE_POOL_CACHETABLE_TOTAL_EXECUTION_TIME</code>","text":"<p>This variable shows the total execution time of processing work items in the cachetable thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_checkpoint_num_threads","title":"<code>Tokudb_CACHETABLE_POOL_CHECKPOINT_NUM_THREADS</code>","text":"<p>This variable shows the number of threads in the checkpoint threadpool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_checkpoint_num_threads_active","title":"<code>Tokudb_CACHETABLE_POOL_CHECKPOINT_NUM_THREADS_ACTIVE</code>","text":"<p>This variable shows the number of currently active threads in the checkpoint thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_checkpoint_queue_size","title":"<code>Tokudb_CACHETABLE_POOL_CHECKPOINT_QUEUE_SIZE</code>","text":"<p>This variable shows the number of currently queued work items in the checkpoint thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_checkpoint_max_queue_size","title":"<code>Tokudb_CACHETABLE_POOL_CHECKPOINT_MAX_QUEUE_SIZE</code>","text":"<p>This variable shows the largest number of queued work items in the checkpoint thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_checkpoint_total_items_processed","title":"<code>Tokudb_CACHETABLE_POOL_CHECKPOINT_TOTAL_ITEMS_PROCESSED</code>","text":"<p>This variable shows the total number of work items processed in the checkpoint thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cachetable_pool_checkpoint_total_execution_time","title":"<code>Tokudb_CACHETABLE_POOL_CHECKPOINT_TOTAL_EXECUTION_TIME</code>","text":"<p>This variable shows the total execution time of processing work items in the checkpoint thread pool.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_memory_size","title":"<code>Tokudb_LOCKTREE_MEMORY_SIZE</code>","text":"<p>This variable shows the amount of memory, in bytes, that the locktree is currently using.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_memory_size_limit","title":"<code>Tokudb_LOCKTREE_MEMORY_SIZE_LIMIT</code>","text":"<p>This variable shows the maximum amount of memory, in bytes, that the locktree is allowed to use.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_escalation_num","title":"<code>Tokudb_LOCKTREE_ESCALATION_NUM</code>","text":"<p>This variable shows the number of times the locktree needed to run lock escalation to reduce its memory footprint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_escalation_seconds","title":"<code>Tokudb_LOCKTREE_ESCALATION_SECONDS</code>","text":"<p>This variable shows the total number of seconds spent performing locktree escalation.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_latest_post_escalation_memory_size","title":"<code>Tokudb_LOCKTREE_LATEST_POST_ESCALATION_MEMORY_SIZE</code>","text":"<p>This variable shows the locktree size, in bytes, after most current locktree escalation.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_open_current","title":"<code>Tokudb_LOCKTREE_OPEN_CURRENT</code>","text":"<p>This variable shows the number of locktrees that are currently opened.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_pending_lock_requests","title":"<code>Tokudb_LOCKTREE_PENDING_LOCK_REQUESTS</code>","text":"<p>This variable shows the number of requests waiting for a lock grant.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_sto_eligible_num","title":"<code>Tokudb_LOCKTREE_STO_ELIGIBLE_NUM</code>","text":"<p>This variable shows the number of locktrees eligible for <code>Single Transaction optimizations</code>. STO optimization are behaviors that can happen within the locktree when there is exactly one transaction active within the locktree. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_sto_ended_num","title":"<code>Tokudb_LOCKTREE_STO_ENDED_NUM</code>","text":"<p>This variable shows the total number of times a <code>Single Transaction Optimization</code> was ended early due to another transaction starting. STO optimization are behaviors that can happen within the locktree when there is exactly one transaction active within the locktree. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_sto_ended_seconds","title":"<code>Tokudb_LOCKTREE_STO_ENDED_SECONDS</code>","text":"<p>This variable shows the total number of seconds ending the <code>Single Transaction Optimizations</code>. STO optimization are behaviors that can happen within the locktree when there is exactly one transaction active within the locktree. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_wait_count","title":"<code>Tokudb_LOCKTREE_WAIT_COUNT</code>","text":"<p>This variable shows the number of times that a lock request could not be acquired because of a conflict with some other transaction. PerconaFT lock request  cycles to try to obtain a lock, if it can not get a lock, it sleeps/waits and times out, checks to get the lock again, repeat. This value indicates the number of cycles it needed to execute before it obtained the lock.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_wait_time","title":"<code>Tokudb_LOCKTREE_WAIT_TIME</code>","text":"<p>This variable shows the total time, in microseconds, spent by client waiting for a lock conflict to be resolved.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_long_wait_count","title":"<code>Tokudb_LOCKTREE_LONG_WAIT_COUNT</code>","text":"<p>This variable shows number of lock waits greater than one second in duration.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_long_wait_time","title":"<code>Tokudb_LOCKTREE_LONG_WAIT_TIME</code>","text":"<p>This variable shows the total time, in microseconds, of the long waits.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_timeout_count","title":"<code>Tokudb_LOCKTREE_TIMEOUT_COUNT</code>","text":"<p>This variable shows the number of times that a lock request timed out.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_wait_escalation_count","title":"<code>Tokudb_LOCKTREE_WAIT_ESCALATION_COUNT</code>","text":"<p>When the sum of the sizes of locks taken reaches the lock tree limit, we run lock escalation on a background thread. The clients threads need to wait for escalation to consolidate locks and free up memory. This variables shows the number of times a client thread had to wait on lock escalation.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_wait_escalation_time","title":"<code>Tokudb_LOCKTREE_WAIT_ESCALATION_TIME</code>","text":"<p>This variable shows the total time, in microseconds, that a client thread spent waiting for lock escalation to free up memory.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_long_wait_escalation_count","title":"<code>Tokudb_LOCKTREE_LONG_WAIT_ESCALATION_COUNT</code>","text":"<p>This variable shows number of times that a client thread had to wait on lock escalation and the wait time was greater than one second.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_locktree_long_wait_escalation_time","title":"<code>Tokudb_LOCKTREE_LONG_WAIT_ESCALATION_TIME</code>","text":"<p>This variable shows the total time, in microseconds, of the long waits for lock escalation to free up memory.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_dictionary_updates","title":"<code>Tokudb_DICTIONARY_UPDATES</code>","text":"<p>This variable shows the total number of rows that have been updated in all primary and secondary indexes combined, if those updates have been done with a separate recovery log entry per index.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_dictionary_broadcast_updates","title":"<code>Tokudb_DICTIONARY_BROADCAST_UPDATES</code>","text":"<p>This variable shows the number of broadcast updates that have been successfully performed. A broadcast update is an update that affects all rows in a dictionary.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_descriptor_set","title":"<code>Tokudb_DESCRIPTOR_SET</code>","text":"<p>This variable shows the number of time a descriptor was updated when the entire dictionary was updated (for example, when the schema has been changed).</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_messages_ignored_by_leaf_due_to_msn","title":"<code>Tokudb_MESSAGES_IGNORED_BY_LEAF_DUE_TO_MSN</code>","text":"<p>This variable shows the number of messages that were ignored by a leaf because it had already been applied.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_total_search_retries","title":"<code>Tokudb_TOTAL_SEARCH_RETRIES</code>","text":"<p>Internal value that is no use to anyone other than a developer debugging a specific query/search issue.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_search_tries_gt_height","title":"<code>Tokudb_SEARCH_TRIES_GT_HEIGHT</code>","text":"<p>Internal value that is no use to anyone other than a developer debugging a specific query/search issue.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_search_tries_gt_heightplus3","title":"<code>Tokudb_SEARCH_TRIES_GT_HEIGHTPLUS3</code>","text":"<p>Internal value that is no use to anyone other than a developer debugging a specific query/search issue.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_nodes_flushed_not_checkpoint","title":"<code>Tokudb_LEAF_NODES_FLUSHED_NOT_CHECKPOINT</code>","text":"<p>This variable shows the number of leaf nodes flushed to disk, not for checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_nodes_flushed_not_checkpoint_bytes","title":"<code>Tokudb_LEAF_NODES_FLUSHED_NOT_CHECKPOINT_BYTES</code>","text":"<p>This variable shows the size, in bytes, of leaf nodes flushed to disk, not for checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_nodes_flushed_not_checkpoint_uncompressed_bytes","title":"<code>Tokudb_LEAF_NODES_FLUSHED_NOT_CHECKPOINT_UNCOMPRESSED_BYTES</code>","text":"<p>This variable shows the size, in bytes, of uncompressed leaf nodes flushed to disk not for checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_nodes_flushed_not_checkpoint_seconds","title":"<code>Tokudb_LEAF_NODES_FLUSHED_NOT_CHECKPOINT_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when writing leaf nodes flushed to disk, not for checkpoint</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_nodes_flushed_to_disk_not_checkpoint","title":"<code>Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_NOT_CHECKPOINT</code>","text":"<p>This variable shows the number of non-leaf nodes flushed to disk, not for checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_nodes_flushed_to_disk_not_checkpoint_bytes","title":"<code>Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_NOT_CHECKPOINT_BYTES</code>","text":"<p>This variable shows the size, in bytes, of non-leaf nodes flushed to disk, not for checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_nodes_flushed_to_disk_not_checkpoint_uncompresse","title":"<code>Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_NOT_CHECKPOINT_UNCOMPRESSE</code>","text":"<p>This variable shows the size, in bytes, of uncompressed non-leaf nodes flushed to disk not for checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_nodes_flushed_to_disk_not_checkpoint_seconds","title":"<code>Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_NOT_CHECKPOINT_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when writing non-leaf nodes flushed to disk, not for checkpoint</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_nodes_flushed_checkpoint","title":"<code>Tokudb_LEAF_NODES_FLUSHED_CHECKPOINT</code>","text":"<p>This variable shows the number of leaf nodes flushed to disk, for checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_nodes_flushed_checkpoint_bytes","title":"<code>Tokudb_LEAF_NODES_FLUSHED_CHECKPOINT_BYTES</code>","text":"<p>This variable shows the size, in bytes, of leaf nodes flushed to disk, for checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_nodes_flushed_checkpoint_uncompressed_bytes","title":"<code>Tokudb_LEAF_NODES_FLUSHED_CHECKPOINT_UNCOMPRESSED_BYTES</code>","text":"<p>This variable shows the size, in bytes, of uncompressed leaf nodes flushed to disk for checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_nodes_flushed_checkpoint_seconds","title":"<code>Tokudb_LEAF_NODES_FLUSHED_CHECKPOINT_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when writing leaf nodes flushed to disk for checkpoint</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_nodes_flushed_to_disk_checkpoint","title":"<code>Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_CHECKPOINT</code>","text":"<p>This variable shows the number of non-leaf nodes flushed to disk, for checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_nodes_flushed_to_disk_checkpoint_bytes","title":"<code>Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_CHECKPOINT_BYTES</code>","text":"<p>This variable shows the size, in bytes, of non-leaf nodes flushed to disk, for checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_nodes_flushed_to_disk_checkpoint_uncompressed_by","title":"<code>Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_CHECKPOINT_UNCOMPRESSED_BY</code>","text":"<p>This variable shows the size, in bytes, of uncompressed non-leaf nodes flushed to disk for checkpoint.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_nodes_flushed_to_disk_checkpoint_seconds","title":"<code>Tokudb_NONLEAF_NODES_FLUSHED_TO_DISK_CHECKPOINT_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when writing non-leaf nodes flushed to disk for checkpoint</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_node_compression_ratio","title":"<code>Tokudb_LEAF_NODE_COMPRESSION_RATIO</code>","text":"<p>This variable shows the ratio of uncompressed bytes (in-memory) to compressed bytes (on-disk) for leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_node_compression_ratio","title":"<code>Tokudb_NONLEAF_NODE_COMPRESSION_RATIO</code>","text":"<p>This variable shows the ratio of uncompressed bytes (in-memory) to compressed bytes (on-disk) for non-leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_overall_node_compression_ratio","title":"<code>Tokudb_OVERALL_NODE_COMPRESSION_RATIO</code>","text":"<p>This variable shows the ratio of uncompressed bytes (in-memory) to compressed bytes (on-disk) for all nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_node_partial_evictions","title":"<code>Tokudb_NONLEAF_NODE_PARTIAL_EVICTIONS</code>","text":"<p>This variable shows the number of times a partition of a non-leaf node was evicted from the cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_node_partial_evictions_bytes","title":"<code>Tokudb_NONLEAF_NODE_PARTIAL_EVICTIONS_BYTES</code>","text":"<p>This variable shows the amount, in bytes, of memory freed by evicting partitions of non-leaf nodes from the cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_node_partial_evictions","title":"<code>Tokudb_LEAF_NODE_PARTIAL_EVICTIONS</code>","text":"<p>This variable shows the number of times a partition of a leaf node was evicted from the cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_node_partial_evictions_bytes","title":"<code>Tokudb_LEAF_NODE_PARTIAL_EVICTIONS_BYTES</code>","text":"<p>This variable shows the amount, in bytes, of memory freed by evicting partitions of leaf nodes from the cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_node_full_evictions","title":"<code>Tokudb_LEAF_NODE_FULL_EVICTIONS</code>","text":"<p>This variable shows the number of times a full leaf node was evicted from the cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_node_full_evictions_bytes","title":"<code>Tokudb_LEAF_NODE_FULL_EVICTIONS_BYTES</code>","text":"<p>This variable shows the amount, in bytes, of memory freed by evicting full leaf nodes from the cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_node_full_evictions","title":"<code>Tokudb_NONLEAF_NODE_FULL_EVICTIONS</code>","text":"<p>This variable shows the number of times a full non-leaf node was evicted from the cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_node_full_evictions_bytes","title":"<code>Tokudb_NONLEAF_NODE_FULL_EVICTIONS_BYTES</code>","text":"<p>This variable shows the amount, in bytes, of memory freed by evicting full non-leaf nodes from the cache.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_nodes_created","title":"<code>Tokudb_LEAF_NODES_CREATED</code>","text":"<p>This variable shows the number of created leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_nodes_created","title":"<code>Tokudb_NONLEAF_NODES_CREATED</code>","text":"<p>This variable shows the number of created non-leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_nodes_destroyed","title":"<code>Tokudb_LEAF_NODES_DESTROYED</code>","text":"<p>This variable shows the number of destroyed leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_nodes_destroyed","title":"<code>Tokudb_NONLEAF_NODES_DESTROYED</code>","text":"<p>This variable shows the number of destroyed non-leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_messages_injected_at_root_bytes","title":"<code>Tokudb_MESSAGES_INJECTED_AT_ROOT_BYTES</code>","text":"<p>This variable shows the size, in bytes, of messages injected at root (for all trees).</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_messages_flushed_from_h1_to_leaves_bytes","title":"<code>Tokudb_MESSAGES_FLUSHED_FROM_H1_TO_LEAVES_BYTES</code>","text":"<p>This variable shows the size, in bytes, of messages flushed from <code>h1</code> nodes to leaves.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_messages_in_trees_estimate_bytes","title":"<code>Tokudb_MESSAGES_IN_TREES_ESTIMATE_BYTES</code>","text":"<p>This variable shows the estimated size, in bytes, of messages currently in trees.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_messages_injected_at_root","title":"<code>Tokudb_MESSAGES_INJECTED_AT_ROOT</code>","text":"<p>This variables shows the number of messages that were injected at root node of a tree.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_broadcase_messages_injected_at_root","title":"<code>Tokudb_BROADCASE_MESSAGES_INJECTED_AT_ROOT</code>","text":"<p>This variable shows the number of broadcast messages dropped into the root node of a tree. These are things such as the result of <code>OPTIMIZE TABLE</code> and a few other operations. This is not a useful metric for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_decompressed_target_query","title":"<code>Tokudb_BASEMENTS_DECOMPRESSED_TARGET_QUERY</code>","text":"<p>This variable shows the number of basement nodes decompressed for queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_decompressed_prelocked_range","title":"<code>Tokudb_BASEMENTS_DECOMPRESSED_PRELOCKED_RANGE</code>","text":"<p>This variable shows the number of basement nodes aggressively decompressed by queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_decompressed_prefetch","title":"<code>Tokudb_BASEMENTS_DECOMPRESSED_PREFETCH</code>","text":"<p>This variable shows the number of basement nodes decompressed by a prefetch thread.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_decompressed_for_write","title":"<code>Tokudb_BASEMENTS_DECOMPRESSED_FOR_WRITE</code>","text":"<p>This variable shows the number of basement nodes decompressed for writes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_decompressed_target_query","title":"<code>Tokudb_BUFFERS_DECOMPRESSED_TARGET_QUERY</code>","text":"<p>This variable shows the number of buffers decompressed for queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_decompressed_prelocked_range","title":"<code>Tokudb_BUFFERS_DECOMPRESSED_PRELOCKED_RANGE</code>","text":"<p>This variable shows the number of buffers decompressed by queries aggressively.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_decompressed_prefetch","title":"<code>Tokudb_BUFFERS_DECOMPRESSED_PREFETCH</code>","text":"<p>This variable shows the number of buffers decompressed by a prefetch thread.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_decompressed_for_write","title":"<code>Tokudb_BUFFERS_DECOMPRESSED_FOR_WRITE</code>","text":"<p>This variable shows the number of buffers decompressed for writes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_pivots_fetched_for_query","title":"<code>Tokudb_PIVOTS_FETCHED_FOR_QUERY</code>","text":"<p>This variable shows the number of pivot nodes fetched for queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_pivots_fetched_for_query_bytes","title":"<code>Tokudb_PIVOTS_FETCHED_FOR_QUERY_BYTES</code>","text":"<p>This variable shows the number of bytes of pivot nodes fetched for queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_pivots_fetched_for_query_seconds","title":"<code>Tokudb_PIVOTS_FETCHED_FOR_QUERY_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when fetching pivot nodes for queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_pivots_fetched_for_prefetch","title":"<code>Tokudb_PIVOTS_FETCHED_FOR_PREFETCH</code>","text":"<p>This variable shows the number of pivot nodes fetched by a prefetch thread.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_pivots_fetched_for_prefetch_bytes","title":"<code>Tokudb_PIVOTS_FETCHED_FOR_PREFETCH_BYTES</code>","text":"<p>This variable shows the number of bytes of pivot nodes fetched for queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_pivots_fetched_for_prefetch_seconds","title":"<code>Tokudb_PIVOTS_FETCHED_FOR_PREFETCH_SECONDS</code>","text":"<p>This variable shows the number seconds waiting for I/O when fetching pivot nodes by a prefetch thread.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_pivots_fetched_for_write","title":"<code>Tokudb_PIVOTS_FETCHED_FOR_WRITE</code>","text":"<p>This variable shows the number of pivot nodes fetched for writes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_pivots_fetched_for_write_bytes","title":"<code>Tokudb_PIVOTS_FETCHED_FOR_WRITE_BYTES</code>","text":"<p>This variable shows the number of bytes of pivot nodes fetched for writes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_pivots_fetched_for_write_seconds","title":"<code>Tokudb_PIVOTS_FETCHED_FOR_WRITE_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when fetching pivot nodes for writes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_fetched_target_query","title":"<code>Tokudb_BASEMENTS_FETCHED_TARGET_QUERY</code>","text":"<p>This variable shows the number of basement nodes fetched from disk for queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_fetched_target_query_bytes","title":"<code>Tokudb_BASEMENTS_FETCHED_TARGET_QUERY_BYTES</code>","text":"<p>This variable shows the number of basement node bytes fetched from disk for queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_fetched_target_query_seconds","title":"<code>Tokudb_BASEMENTS_FETCHED_TARGET_QUERY_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when fetching basement nodes from disk for queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_fetched_prelocked_range","title":"<code>Tokudb_BASEMENTS_FETCHED_PRELOCKED_RANGE</code>","text":"<p>This variable shows the number of basement nodes fetched from disk aggressively.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_fetched_prelocked_range_bytes","title":"<code>Tokudb_BASEMENTS_FETCHED_PRELOCKED_RANGE_BYTES</code>","text":"<p>This variable shows the number of basement node bytes fetched from disk aggressively.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_fetched_prelocked_range_seconds","title":"<code>Tokudb_BASEMENTS_FETCHED_PRELOCKED_RANGE_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when fetching basement nodes from disk aggressively.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_fetched_prefetch","title":"<code>Tokudb_BASEMENTS_FETCHED_PREFETCH</code>","text":"<p>This variable shows the number of basement nodes fetched from disk by a prefetch thread.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_fetched_prefetch_bytes","title":"<code>Tokudb_BASEMENTS_FETCHED_PREFETCH_BYTES</code>","text":"<p>This variable shows the number of basement node bytes fetched from disk by a prefetch thread.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_fetched_prefetch_seconds","title":"<code>Tokudb_BASEMENTS_FETCHED_PREFETCH_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when fetching basement nodes from disk by a prefetch thread.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_fetched_for_write","title":"<code>Tokudb_BASEMENTS_FETCHED_FOR_WRITE</code>","text":"<p>This variable shows the number of buffers fetched from disk for writes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_fetched_for_write_bytes","title":"<code>Tokudb_BASEMENTS_FETCHED_FOR_WRITE_BYTES</code>","text":"<p>This variable shows the number of buffer bytes fetched from disk for writes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basements_fetched_for_write_seconds","title":"<code>Tokudb_BASEMENTS_FETCHED_FOR_WRITE_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when fetching buffers from disk for writes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_fetched_target_query","title":"<code>Tokudb_BUFFERS_FETCHED_TARGET_QUERY</code>","text":"<p>This variable shows the number of buffers fetched from disk for queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_fetched_target_query_bytes","title":"<code>Tokudb_BUFFERS_FETCHED_TARGET_QUERY_BYTES</code>","text":"<p>This variable shows the number of buffer bytes fetched from disk for queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_fetched_target_query_seconds","title":"<code>Tokudb_BUFFERS_FETCHED_TARGET_QUERY_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when fetching buffers from disk for queries.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_fetched_prelocked_range","title":"<code>Tokudb_BUFFERS_FETCHED_PRELOCKED_RANGE</code>","text":"<p>This variable shows the number of buffers fetched from disk aggressively.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_fetched_prelocked_range_bytes","title":"<code>Tokudb_BUFFERS_FETCHED_PRELOCKED_RANGE_BYTES</code>","text":"<p>This variable shows the number of buffer bytes fetched from disk aggressively.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_fetched_prelocked_range_seconds","title":"<code>Tokudb_BUFFERS_FETCHED_PRELOCKED_RANGE_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when fetching buffers from disk aggressively.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_fetched_prefetch","title":"<code>Tokudb_BUFFERS_FETCHED_PREFETCH</code>","text":"<p>This variable shows the number of buffers fetched from disk aggressively.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_fetched_prefetch_bytes","title":"<code>Tokudb_BUFFERS_FETCHED_PREFETCH_BYTES</code>","text":"<p>This variable shows the number of buffer bytes fetched from disk by a prefetch thread.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_fetched_prefetch_seconds","title":"<code>Tokudb_BUFFERS_FETCHED_PREFETCH_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when fetching buffers from disk by a prefetch thread.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_fetched_for_write","title":"<code>Tokudb_BUFFERS_FETCHED_FOR_WRITE</code>","text":"<p>This variable shows the number of buffers fetched from disk for writes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_fetched_for_write_bytes","title":"<code>Tokudb_BUFFERS_FETCHED_FOR_WRITE_BYTES</code>","text":"<p>This variable shows the number of buffer bytes fetched from disk for writes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_buffers_fetched_for_write_seconds","title":"<code>Tokudb_BUFFERS_FETCHED_FOR_WRITE_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for I/O when fetching buffers from disk for writes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_compression_to_memory_seconds","title":"<code>Tokudb_LEAF_COMPRESSION_TO_MEMORY_SECONDS</code>","text":"<p>This variable shows the total time, in seconds, spent compressing leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_serialization_to_memory_seconds","title":"<code>Tokudb_LEAF_SERIALIZATION_TO_MEMORY_SECONDS</code>","text":"<p>This variable shows the total time, in seconds, spent serializing leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_decompression_to_memory_seconds","title":"<code>Tokudb_LEAF_DECOMPRESSION_TO_MEMORY_SECONDS</code>","text":"<p>This variable shows the total time, in seconds, spent decompressing leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_leaf_deserialization_to_memory_seconds","title":"<code>Tokudb_LEAF_DESERIALIZATION_TO_MEMORY_SECONDS</code>","text":"<p>This variable shows the total time, in seconds, spent deserializing leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_compression_to_memory_seconds","title":"<code>Tokudb_NONLEAF_COMPRESSION_TO_MEMORY_SECONDS</code>","text":"<p>This variable shows the total time, in seconds, spent compressing non leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_serialization_to_memory_seconds","title":"<code>Tokudb_NONLEAF_SERIALIZATION_TO_MEMORY_SECONDS</code>","text":"<p>This variable shows the total time, in seconds, spent serializing non leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_decompression_to_memory_seconds","title":"<code>Tokudb_NONLEAF_DECOMPRESSION_TO_MEMORY_SECONDS</code>","text":"<p>This variable shows the total time, in seconds, spent decompressing non leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_nonleaf_deserialization_to_memory_seconds","title":"<code>Tokudb_NONLEAF_DESERIALIZATION_TO_MEMORY_SECONDS</code>","text":"<p>This variable shows the total time, in seconds, spent deserializing non leaf nodes.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_roots_split","title":"<code>Tokudb_PROMOTION_ROOTS_SPLIT</code>","text":"<p>This variable shows the number of times the root split during promotion.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_leaf_roots_injected_into","title":"<code>Tokudb_PROMOTION_LEAF_ROOTS_INJECTED_INTO</code>","text":"<p>This variable shows the number of times a message stopped at a root with height <code>0</code>.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_h1_roots_injected_into","title":"<code>Tokudb_PROMOTION_H1_ROOTS_INJECTED_INTO</code>","text":"<p>This variable shows the number of times a message stopped at a root with height <code>1</code>.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_injections_at_depth_0","title":"<code>Tokudb_PROMOTION_INJECTIONS_AT_DEPTH_0</code>","text":"<p>This variable shows the number of times a message stopped at depth <code>0</code>.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_injections_at_depth_1","title":"<code>Tokudb_PROMOTION_INJECTIONS_AT_DEPTH_1</code>","text":"<p>This variable shows the number of times a message stopped at depth <code>1</code>.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_injections_at_depth_2","title":"<code>Tokudb_PROMOTION_INJECTIONS_AT_DEPTH_2</code>","text":"<p>This variable shows the number of times a message stopped at depth <code>2</code>.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_injections_at_depth_3","title":"<code>Tokudb_PROMOTION_INJECTIONS_AT_DEPTH_3</code>","text":"<p>This variable shows the number of times a message stopped at depth <code>3</code>.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_injections_lower_than_depth_3","title":"<code>Tokudb_PROMOTION_INJECTIONS_LOWER_THAN_DEPTH_3</code>","text":"<p>This variable shows the number of times a message was promoted past depth <code>3</code>.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_stopped_nonempty_buffer","title":"<code>Tokudb_PROMOTION_STOPPED_NONEMPTY_BUFFER</code>","text":"<p>This variable shows the number of times a message stopped because it reached a nonempty buffer.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_stopped_at_height_1","title":"<code>Tokudb_PROMOTION_STOPPED_AT_HEIGHT_1</code>","text":"<p>This variable shows the number of times a message stopped because it had reached height <code>1</code>.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_stopped_child_locked_or_not_in_memory","title":"<code>Tokudb_PROMOTION_STOPPED_CHILD_LOCKED_OR_NOT_IN_MEMORY</code>","text":"<p>This variable shows the number of times a message stopped because it could not cheaply get access to a child.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_stopped_child_not_fully_in_memory","title":"<code>Tokudb_PROMOTION_STOPPED_CHILD_NOT_FULLY_IN_MEMORY</code>","text":"<p>This variable shows the number of times a message stopped because it could not cheaply get access to a child.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_promotion_stopped_after_locking_child","title":"<code>Tokudb_PROMOTION_STOPPED_AFTER_LOCKING_CHILD</code>","text":"<p>This variable shows the number of times a message stopped before a child which had been locked.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basement_deserialization_fixed_key","title":"<code>Tokudb_BASEMENT_DESERIALIZATION_FIXED_KEY</code>","text":"<p>This variable shows the number of basement nodes deserialized where all keys had the same size, leaving the basement in a format that is optimal for in-memory workloads.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_basement_deserialization_variable_key","title":"<code>Tokudb_BASEMENT_DESERIALIZATION_VARIABLE_KEY</code>","text":"<p>This variable shows the number of basement nodes deserialized where all keys did not have the same size, and thus ineligible for an in-memory optimization.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_pro_rightmost_leaf_shortcut_success","title":"<code>Tokudb_PRO_RIGHTMOST_LEAF_SHORTCUT_SUCCESS</code>","text":"<p>This variable shows the number of times a message injection detected a series of sequential inserts to the rightmost side of the tree and successfully applied an insert message directly to the rightmost leaf node. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_pro_rightmost_leaf_shortcut_fail_pos","title":"<code>Tokudb_PRO_RIGHTMOST_LEAF_SHORTCUT_FAIL_POS</code>","text":"<p>This variable shows the number of times a message injection detected a series of sequential inserts to the rightmost side of the tree and was unable to follow the pattern of directly applying an insert message directly to the rightmost leaf node because the key does not continue the sequence. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_rightmost_leaf_shortcut_fail_reactive","title":"<code>Tokudb_RIGHTMOST_LEAF_SHORTCUT_FAIL_REACTIVE</code>","text":"<p>This variable shows the number of times a message injection detected a series of sequential inserts to the rightmost side of the tree and was unable to follow the pattern of directly applying an insert message directly to the rightmost leaf node because the leaf is full. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_cursor_skip_deleted_leaf_entry","title":"<code>Tokudb_CURSOR_SKIP_DELETED_LEAF_ENTRY</code>","text":"<p>This variable shows the number of leaf entries skipped during search/scan because the result of message application and reconciliation of the leaf entry MVCC stack reveals that the leaf entry is <code>deleted</code> in the current transactions view. It is a good indicator that there might be excessive garbage in a tree if a range scan seems to take too long.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_total_nodes","title":"<code>Tokudb_FLUSHER_CLEANER_TOTAL_NODES</code>","text":"<p>This variable shows the total number of nodes potentially flushed by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_h1_nodes","title":"<code>Tokudb_FLUSHER_CLEANER_H1_NODES</code>","text":"<p>This variable shows the number of height <code>1</code> nodes that had messages flushed by flusher or cleaner threads, i.e., internal nodes immediately above leaf nodes. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_hgt1_nodes","title":"<code>Tokudb_FLUSHER_CLEANER_HGT1_NODES</code>","text":"<p>This variable shows the number of nodes with height greater than <code>1</code> that had messages flushed by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_empty_nodes","title":"<code>Tokudb_FLUSHER_CLEANER_EMPTY_NODES</code>","text":"<p>This variable shows the number of nodes cleaned by flusher or cleaner threads which had empty message buffers. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_nodes_dirtied","title":"<code>Tokudb_FLUSHER_CLEANER_NODES_DIRTIED</code>","text":"<p>This variable shows the number of nodes dirtied by flusher or cleaner threads as a result of flushing messages downward. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_max_buffer_size","title":"<code>Tokudb_FLUSHER_CLEANER_MAX_BUFFER_SIZE</code>","text":"<p>This variable shows the maximum bytes in a message buffer flushed by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_min_buffer_size","title":"<code>Tokudb_FLUSHER_CLEANER_MIN_BUFFER_SIZE</code>","text":"<p>This variable shows the minimum bytes in a message buffer flushed by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_total_buffer_size","title":"<code>Tokudb_FLUSHER_CLEANER_TOTAL_BUFFER_SIZE</code>","text":"<p>This variable shows the total bytes in buffers flushed by flusher and cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_max_buffer_workdone","title":"<code>Tokudb_FLUSHER_CLEANER_MAX_BUFFER_WORKDONE</code>","text":"<p>This variable shows the maximum bytes worth of work done in a message buffer flushed by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_min_buffer_workdone","title":"<code>Tokudb_FLUSHER_CLEANER_MIN_BUFFER_WORKDONE</code>","text":"<p>This variable shows the minimum bytes worth of work done in a message buffer flushed by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_total_buffer_workdone","title":"<code>Tokudb_FLUSHER_CLEANER_TOTAL_BUFFER_WORKDONE</code>","text":"<p>This variable shows the total bytes worth of work done in buffers flushed by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_num_leaf_merges_started","title":"<code>Tokudb_FLUSHER_CLEANER_NUM_LEAF_MERGES_STARTED</code>","text":"<p>This variable shows the number of times flusher and cleaner threads tried to merge two leafs. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_num_leaf_merges_running","title":"<code>Tokudb_FLUSHER_CLEANER_NUM_LEAF_MERGES_RUNNING</code>","text":"<p>This variable shows the number of flusher and cleaner threads leaf merges in progress. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_num_leaf_merges_completed","title":"<code>Tokudb_FLUSHER_CLEANER_NUM_LEAF_MERGES_COMPLETED</code>","text":"<p>This variable shows the number of successful flusher and cleaner threads leaf merges. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_cleaner_num_dirtied_for_leaf_merge","title":"<code>Tokudb_FLUSHER_CLEANER_NUM_DIRTIED_FOR_LEAF_MERGE</code>","text":"<p>This variable shows the number of nodes dirtied by flusher or cleaner threads performing leaf node merges. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_flush_total","title":"<code>Tokudb_FLUSHER_FLUSH_TOTAL</code>","text":"<p>This variable shows the total number of flushes done by flusher threads or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_flush_in_memory","title":"<code>Tokudb_FLUSHER_FLUSH_IN_MEMORY</code>","text":"<p>This variable shows the number of in memory flushes (required no disk reads) by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_flush_needed_io","title":"<code>Tokudb_FLUSHER_FLUSH_NEEDED_IO</code>","text":"<p>This variable shows the number of flushes that read something off disk by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_flush_cascades","title":"<code>Tokudb_FLUSHER_FLUSH_CASCADES</code>","text":"<p>This variable shows the number of flushes that triggered a flush in child node by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_flush_cascades_1","title":"<code>Tokudb_FLUSHER_FLUSH_CASCADES_1</code>","text":"<p>This variable shows the number of flushes that triggered one cascading flush by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_flush_cascades_2","title":"<code>Tokudb_FLUSHER_FLUSH_CASCADES_2</code>","text":"<p>This variable shows the number of flushes that triggered two cascading flushes by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_flush_cascades_3","title":"<code>Tokudb_FLUSHER_FLUSH_CASCADES_3</code>","text":"<p>This variable shows the number of flushes that triggered three cascading flushes by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_flush_cascades_4","title":"<code>Tokudb_FLUSHER_FLUSH_CASCADES_4</code>","text":"<p>This variable shows the number of flushes that triggered four cascading flushes by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_flush_cascades_5","title":"<code>Tokudb_FLUSHER_FLUSH_CASCADES_5</code>","text":"<p>This variable shows the number of flushes that triggered five cascading flushes by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_flush_cascades_gt_5","title":"<code>Tokudb_FLUSHER_FLUSH_CASCADES_GT_5</code>","text":"<p>This variable shows the number of flushes that triggered more than five cascading flushes by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_split_leaf","title":"<code>Tokudb_FLUSHER_SPLIT_LEAF</code>","text":"<p>This variable shows the total number of leaf node splits done by flusher threads or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_split_nonleaf","title":"<code>Tokudb_FLUSHER_SPLIT_NONLEAF</code>","text":"<p>This variable shows the total number of non-leaf node splits done by flusher threads or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_merge_leaf","title":"<code>Tokudb_FLUSHER_MERGE_LEAF</code>","text":"<p>This variable shows the total number of leaf node merges done by flusher threads or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_merge_nonleaf","title":"<code>Tokudb_FLUSHER_MERGE_NONLEAF</code>","text":"<p>This variable shows the total number of non-leaf node merges done by flusher threads or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_flusher_balance_leaf","title":"<code>Tokudb_FLUSHER_BALANCE_LEAF</code>","text":"<p>This variable shows the number of times two adjacent leaf nodes were rebalanced or had their content redistributed evenly by flusher or cleaner threads. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_hot_num_started","title":"<code>Tokudb_HOT_NUM_STARTED</code>","text":"<p>This variable shows the number of hot operations started (<code>OPTIMIZE TABLE</code>). This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_hot_num_completed","title":"<code>Tokudb_HOT_NUM_COMPLETED</code>","text":"<p>This variable shows the number of hot operations completed (<code>OPTIMIZE TABLE</code>). This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_hot_num_aborted","title":"<code>Tokudb_HOT_NUM_ABORTED</code>","text":"<p>This variable shows the number of hot operations aborted (<code>OPTIMIZE TABLE</code>). This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_hot_max_root_flush_count","title":"<code>Tokudb_HOT_MAX_ROOT_FLUSH_COUNT</code>","text":"<p>This variable shows the maximum number of flushes from root ever required to optimize trees. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_txn_begin","title":"<code>Tokudb_TXN_BEGIN</code>","text":"<p>This variable shows the number of transactions that have been started.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_txn_begin_read_only","title":"<code>Tokudb_TXN_BEGIN_READ_ONLY</code>","text":"<p>This variable shows the number of read-only transactions started.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_txn_commits","title":"<code>Tokudb_TXN_COMMITS</code>","text":"<p>This variable shows the total number of transactions that have been committed.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_txn_aborts","title":"<code>Tokudb_TXN_ABORTS</code>","text":"<p>This variable shows the total number of transactions that have been aborted.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_logger_next_lsn","title":"<code>Tokudb_LOGGER_NEXT_LSN</code>","text":"<p>This variable shows the recovery logger next LSN. This is a not a useful value for a regular user to use for any purpose.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_logger_writes","title":"<code>Tokudb_LOGGER_WRITES</code>","text":"<p>This variable shows the number of times the logger has written to disk.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_logger_writes_bytes","title":"<code>Tokudb_LOGGER_WRITES_BYTES</code>","text":"<p>This variable shows the number of bytes the logger has written to disk.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_logger_writes_uncompressed_bytes","title":"<code>Tokudb_LOGGER_WRITES_UNCOMPRESSED_BYTES</code>","text":"<p>This variable shows the number of uncompressed bytes the logger has written to disk.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_logger_writes_seconds","title":"<code>Tokudb_LOGGER_WRITES_SECONDS</code>","text":"<p>This variable shows the number of seconds waiting for IO when writing logs to disk.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_logger_wait_long","title":"<code>Tokudb_LOGGER_WAIT_LONG</code>","text":"<p>This variable shows the number of times a logger write operation required 100ms or more.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_loader_num_created","title":"<code>Tokudb_LOADER_NUM_CREATED</code>","text":"<p>This variable shows the number of times one of our internal objects, a loader, has been created.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_loader_num_current","title":"<code>Tokudb_LOADER_NUM_CURRENT</code>","text":"<p>This variable shows the number of loaders that currently exist.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_loader_num_max","title":"<code>Tokudb_LOADER_NUM_MAX</code>","text":"<p>This variable shows the maximum number of loaders that ever existed simultaneously.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_memory_malloc_count","title":"<code>Tokudb_MEMORY_MALLOC_COUNT</code>","text":"<p>This variable shows the number of <code>malloc</code> operations by PerconaFT.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_memory_free_count","title":"<code>Tokudb_MEMORY_FREE_COUNT</code>","text":"<p>This variable shows the number of <code>free</code> operations by PerconaFT.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_memory_realloc_count","title":"<code>Tokudb_MEMORY_REALLOC_COUNT</code>","text":"<p>This variable shows the number of <code>realloc</code> operations by PerconaFT.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_memory_malloc_fail","title":"<code>Tokudb_MEMORY_MALLOC_FAIL</code>","text":"<p>This variable shows the number of <code>malloc</code> operations that failed by PerconaFT.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_memory_realloc_fail","title":"<code>Tokudb_MEMORY_REALLOC_FAIL</code>","text":"<p>This variable shows the number of <code>realloc</code> operations that failed by PerconaFT.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_memory_requested","title":"<code>Tokudb_MEMORY_REQUESTED</code>","text":"<p>This variable shows the number of bytes requested by PerconaFT.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_memory_used","title":"<code>Tokudb_MEMORY_USED</code>","text":"<p>This variable shows the number of bytes used (requested + overhead) by PerconaFT.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_memory_freed","title":"<code>Tokudb_MEMORY_FREED</code>","text":"<p>This variable shows the number of bytes freed by PerconaFT.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_memory_max_requested_size","title":"<code>Tokudb_MEMORY_MAX_REQUESTED_SIZE</code>","text":"<p>This variable shows the largest attempted allocation size by PerconaFT.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_memory_last_failed_size","title":"<code>Tokudb_MEMORY_LAST_FAILED_SIZE</code>","text":"<p>This variable shows the size of the last failed allocation attempt by PerconaFT.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_mem_estimated_maximum_memory_footprint","title":"<code>Tokudb_MEM_ESTIMATED_MAXIMUM_MEMORY_FOOTPRINT</code>","text":"<p>This variable shows the maximum memory footprint of the storage engine, the max value of (used - freed).</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_memory_mallocator_version","title":"<code>Tokudb_MEMORY_MALLOCATOR_VERSION</code>","text":"<p>This variable shows the version of the memory allocator library detected by PerconaFT.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_memory_mmap_threshold","title":"<code>Tokudb_MEMORY_MMAP_THRESHOLD</code>","text":"<p>This variable shows the <code>mmap</code> threshold in PerconaFT, anything larger than this gets <code>mmap'ed</code>.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_filesystem_threads_blocked_by_full_disk","title":"<code>Tokudb_FILESYSTEM_THREADS_BLOCKED_BY_FULL_DISK</code>","text":"<p>This variable shows the number of threads that are currently blocked because they are attempting to write to a full disk. This is normally zero. If this value is non-zero, then a warning will appear in the <code>disk free space</code> field.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_filesystem_fsync_time","title":"<code>Tokudb_FILESYSTEM_FSYNC_TIME</code>","text":"<p>This variable shows the total time, in microseconds, used to <code>fsync</code> to disk.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_filesystem_fsync_num","title":"<code>Tokudb_FILESYSTEM_FSYNC_NUM</code>","text":"<p>This variable shows the total number of times the database has flushed the operating system\u2019s file buffers to disk.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_filesystem_long_fsync_time","title":"<code>Tokudb_FILESYSTEM_LONG_FSYNC_TIME</code>","text":"<p>This variable shows the total time, in microseconds, used to <code>fsync</code> to dis k when the operation required more than one second.</p>"},{"location":"tokudb/tokudb_status_variables.html#tokudb_filesystem_long_fsync_num","title":"<code>Tokudb_FILESYSTEM_LONG_FSYNC_NUM</code>","text":"<p>This variable shows the total number of times the database has flushed the operating system\u2019s file buffers to disk and this operation required more than one second.</p>"},{"location":"tokudb/tokudb_troubleshooting.html","title":"TokuDB Troubleshooting","text":""},{"location":"tokudb/tokudb_troubleshooting.html#known-issues","title":"Known Issues","text":"<p>Replication and binary logging: TokuDB supports binary logging and replication, with one restriction. TokuDB does not implement a lock on the auto-increment function, so concurrent insert statements with one or more of the statements inserting multiple rows may result in a non-deterministic interleaving of the auto-increment values. When running replication with these concurrent inserts, the auto-increment values on the replica table may not match the auto-increment values on the source table. Note that this is only an issue with Statement Based Replication (SBR), and not Row Based Replication (RBR).</p> <p>For more information about auto-increment and replication, see the MySQL Reference Manual: AUTO_INCREMENT handling in InnoDB.</p> <p>In addition, when using the <code>REPLACE INTO</code> or <code>INSERT IGNORE</code> on tables with no secondary indexes or tables where secondary indexes are subsets of the primary, the session variable tokudb_pk_insert_mode controls whether row based replication will work.</p> <p>Uninformative error message: The <code>LOAD DATA INFILE</code> command can sometimes produce <code>ERROR 1030 (HY000): Got error 1 from storage engine</code>. The message should say that the error is caused by insufficient disk space for the temporary files created by the loader.</p> <p>Transparent Huge Pages: TokuDB will refuse to start if transparent huge pages are enabled. Transparent huge page support can be disabled by issuing the following as root:</p> <pre><code># echo never &gt; /sys/kernel/mm/redhat_transparent_hugepage/enabled\n</code></pre> <p>Note</p> <p>Execute this command after every reboot because the default is <code>always</code>.</p> <p>XA behavior vs. InnoDB: InnoDB forces a deadlocked XA transaction to abort, TokuDB does not.</p> <p>Disabling the unique checks: For tables with unique keys, every insertion into the table causes a lookup by key followed by an insertion, if the key is not in the table. This greatly limits insertion performance. If one knows by design that the rows being inserted into the table have unique keys, then one can disable the key lookup prior to insertion.</p> <p>If your primary key is an auto-increment key, and none of your secondary keys are declared to be unique, then setting <code>unique_checks=OFF</code> will provide limited performance gains. On the other hand, if your primary key has a lot of entropy (it looks random), or your secondary keys are declared unique and have a lot of entropy, then disabling unique checks can provide a significant performance boost.</p> <p>If unique_checks is disabled when the primary key is not unique, secondary indexes may become corrupted. In this case, the indexes should be dropped and rebuilt. This behavior differs from that of InnoDB, in which uniqueness is always checked on the primary key, and setting unique_checks to off turns off uniqueness checking on secondary indexes only. Turning off uniqueness checking on the primary key can provide large performance boosts, but it should only be done when the primary key is known to be unique.</p> <p>Group Replication: TokuDB storage engine doesn\u2019t support Group Replication.</p>"},{"location":"tokudb/tokudb_troubleshooting.html#lock-visualization-in-tokudb","title":"Lock Visualization in TokuDB","text":"<p>TokuDB uses key range locks to implement serializable transactions, which are acquired as the transaction progresses. The locks are released when the transaction commits or aborts (this implements two phase locking).</p> <p>TokuDB stores these locks in a data structure called the lock tree. The lock tree stores the set of range locks granted to each transaction. In addition, the lock tree stores the set of locks that are not granted due to a conflict with locks granted to some other transaction. When these other transactions are retired, these pending lock requests are retried. If a pending lock request is not granted before the lock timer expires, then the lock request is aborted.</p> <p>Lock visualization in TokuDB exposes the state of the lock tree with tables in the information schema. We also provide a mechanism that may be used by a database client to retrieve details about lock conflicts that it encountered while executing a transaction.</p>"},{"location":"tokudb/tokudb_troubleshooting.html#the-tokudb_trx-table","title":"The <code>TOKUDB_TRX</code> table","text":"<p>The TOKUDB_TRX table in the <code>INFORMATION_SCHEMA</code> maps TokuDB transaction identifiers to MySQL client identifiers. This mapping allows one to associate a TokuDB transaction with a MySQL client operation.</p> <p>The following query returns the MySQL clients that have a live TokuDB transaction:</p> <pre><code>SELECT * FROM INFORMATION_SCHEMA.TOKUDB_TRX,\nINFORMATION_SCHEMA.PROCESSLIST\nWHERE trx_mysql_thread_id = id;\n</code></pre>"},{"location":"tokudb/tokudb_troubleshooting.html#the-tokudb_locks-table","title":"The <code>TOKUDB_LOCKS</code> table","text":"<p>The tokudb_locks table in the information schema contains the set of locks granted to TokuDB transactions.</p> <p>The following query returns all of the locks granted to some TokuDB transaction:</p> <pre><code>SELECT * FROM INFORMATION_SCHEMA.TOKUDB_LOCKS;\n</code></pre> <p>The following query returns the locks granted to some MySQL client:</p> <pre><code>SELECT id FROM INFORMATION_SCHEMA.TOKUDB_LOCKS,\nINFORMATION_SCHEMA.PROCESSLIST\nWHERE locks_mysql_thread_id = id;\n</code></pre>"},{"location":"tokudb/tokudb_troubleshooting.html#the-tokudb_lock_waits-table","title":"The <code>TOKUDB_LOCK_WAITS</code> table","text":"<p>The tokudb_lock_waits table in the information schema contains the set of lock requests that are not granted due to a lock conflict with some other transaction.</p> <p>The following query returns the locks that are waiting to be granted due to a lock conflict with some other transaction:</p> <pre><code>SELECT * FROM INFORMATION_SCHEMA.TOKUDB_LOCK_WAITS;\n</code></pre>"},{"location":"tokudb/tokudb_troubleshooting.html#the-tokudb_lock_timeout_debug-session-variable","title":"The tokudb_lock_timeout_debug session variable","text":"<p>The tokudb_lock_timeout_debug session variable controls how lock timeouts and lock deadlocks seen by the database client are reported.</p> <p>The following values are available:</p> <ul> <li> <p>0</p> <p>No lock timeouts or lock deadlocks are reported.</p> </li> <li> <p>1</p> <p>A JSON document that describes the lock conflict is stored in the tokudb_last_lock_timeout session variable</p> </li> <li> <p>2</p> <p>A JSON document that describes the lock conflict is printed to the MySQL error log.</p> <p>Supported since 7.5.5: In addition to the JSON document describing the lock conflict, the following lines are printed to the MySQL error log:</p> <ul> <li> <p>A line containing the blocked thread id and blocked SQL</p> </li> <li> <p>A line containing the blocking thread id and the blocking SQL.</p> </li> </ul> </li> <li> <p>3</p> <p>A JSON document that describes the lock conflict is stored in the tokudb_last_lock_timeout session variable and is printed to the MySQL error log.</p> <p>Supported since 7.5.5: In addition to the JSON document describing the lock conflict, the following lines are printed to the MySQL error log:</p> <ul> <li> <p>A line containing the blocked thread id and blocked SQL</p> </li> <li> <p>A line containing the blocking thread id and the blocking SQL.</p> </li> </ul> </li> </ul>"},{"location":"tokudb/tokudb_troubleshooting.html#the-tokudb_last_lock_timeout-session-variable","title":"The tokudb_last_lock_timeout session variable","text":"<p>The tokudb_last_lock_timeout session variable contains a JSON document that describes the last lock conflict seen by the current MySQL client. It gets set when a blocked lock request times out or a lock deadlock is detected. The tokudb_lock_timeout_debug session variable should have bit <code>0</code> set (decimal <code>1</code>).</p>"},{"location":"tokudb/tokudb_troubleshooting.html#example","title":"Example","text":"<p>Suppose that we create a table with a single column that is the primary key.</p> <pre><code>mysql&gt; SHOW CREATE TABLE table;\n\nCreate Table: CREATE TABLE \u2018table\u2018 (\n\u2018id\u2018 int(11) NOT NULL,\nPRIMARY KEY (\u2018id\u2018)) ENGINE=TokuDB DEFAULT CHARSET=latin1\n</code></pre> <p>Suppose that we have 2 MySQL clients with ID\u2019s 1 and 2 respectively. Suppose that MySQL client 1 inserts some values into <code>table</code>. TokuDB transaction 51 is created for the insert statement. Since autocommit is disabled, transaction 51 is still live after the insert statement completes, and we can query the tokudb_locks table in information schema to see the locks that are held by the transaction.</p> <pre><code>mysql&gt; SET AUTOCOMMIT=OFF;\nmysql&gt; INSERT INTO table VALUES (1),(10),(100);\n</code></pre> <p>The output could be:</p> <pre><code>Query OK, 3 rows affected (0.00 sec)\nRecords: 3  Duplicates: 0  Warnings: 0\n</code></pre> <pre><code>mysql&gt; SELECT * FROM INFORMATION_SCHEMA.TOKUDB_LOCKS;\n</code></pre> <p>The output could be:</p> <p><pre><code>+--------------+-----------------------+---------------+----------------+-----------------+--------------------+------------------+-----------------------------+\n| locks_trx_id | locks_mysql_thread_id | locks_dname   | locks_key_left | locks_key_right | locks_table_schema | locks_table_name | locks_table_dictionary_name |\n+--------------+-----------------------+---------------+----------------+-----------------+--------------------+------------------+-----------------------------+\n|           51 |                     1 | ./test/t-main | 0001000000     | 0001000000      | test               | t                | main                        |\n|           51 |                     1 | ./test/t-main | 000a000000     | 000a000000      | test               | t                | main                        |\n|           51 |                     1 | ./test/t-main | 0064000000     | 0064000000      | test               | t                | main                        |\n+--------------+-----------------------+---------------+----------------+-----------------+--------------------+------------------+-----------------------------+\n</code></pre> <pre><code>mysql&gt; SELECT * FROM INFORMATION_SCHEMA.TOKUDB_LOCK_WAITS;\n</code></pre></p> <p>The output could be:</p> <pre><code>Empty set (0.00 sec)\n</code></pre> <p>The keys are currently hex dumped.</p> <p>Now we switch to the other MySQL client with ID 2.</p> <pre><code>mysql&gt; INSERT INTO table VALUES (2),(20),(100);\n</code></pre> <p>The insert gets blocked since there is a conflict on the primary key with value 100.</p> <p>The granted TokuDB locks are:</p> <pre><code>mysql&gt; SELECT * FROM INFORMATION_SCHEMA.TOKUDB_LOCKS;\n</code></pre> <p>The output could be:</p> <pre><code>+--------------+-----------------------+---------------+----------------+-----------------+--------------------+------------------+-----------------------------+\n| locks_trx_id | locks_mysql_thread_id | locks_dname   | locks_key_left | locks_key_right | locks_table_schema | locks_table_name | locks_table_dictionary_name |\n+--------------+-----------------------+---------------+----------------+-----------------+--------------------+------------------+-----------------------------+\n|           51 |                     1 | ./test/t-main | 0001000000     | 0001000000      | test               | t                | main                        |\n|           51 |                     1 | ./test/t-main | 000a000000     | 000a000000      | test               | t                | main                        |\n|           51 |                     1 | ./test/t-main | 0064000000     | 0064000000      | test               | t                | main                        |\n|           51 |                     1 | ./test/t-main | 0002000000     | 0002000000      | test               | t                | main                        |\n|           51 |                     1 | ./test/t-main | 0014000000     | 0014000000      | test               | t                | main                        |\n+--------------+-----------------------+---------------+----------------+-----------------+--------------------+------------------+-----------------------------+\n</code></pre> <p>The locks that are pending due to a conflict are:</p> <pre><code>SELECT * FROM INFORMATION_SCHEMA.TOKUDB_LOCK_WAITS;\n</code></pre> <p>The output could be:</p> <pre><code>+-------------------+-----------------+------------------+---------------------+----------------------+-----------------------+--------------------+------------------+-----------------------------+\n| requesting_trx_id | blocking_trx_id | lock_waits_dname | lock_waits_key_left | lock_waits_key_right | lock_waits_start_time | locks_table_schema | locks_table_name | locks_table_dictionary_name |\n+-------------------+-----------------+------------------+---------------------+----------------------+-----------------------+--------------------+------------------+-----------------------------+\n|                62 |              51 | ./test/t-main    | 0064000000          | 0064000000           |         1380656990910 | test               | t                | main                        |\n+-------------------+-----------------+------------------+---------------------+----------------------+-----------------------+--------------------+------------------+-----------------------------+\n</code></pre> <p>Eventually, the lock for client 2 times out, and we can retrieve a JSON document that describes the conflict.</p> <pre><code>ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction\n</code></pre> <pre><code>mysql&gt; SELECT @@TOKUDB_LAST_LOCK_TIMEOUT;\n</code></pre> <p>The output could be:</p> <pre><code>+---------------------------------------------------------------------------------------------------------------+\n| @@tokudb_last_lock_timeout                                                                                    |\n+---------------------------------------------------------------------------------------------------------------+\n| \"mysql_thread_id\":2, \"dbname\":\"./test/t-main\", \"requesting_txnid\":62, \"blocking_txnid\":51, \"key\":\"0064000000\" |\n+---------------------------------------------------------------------------------------------------------------+\n\nROLLBACK;\n</code></pre> <p>Since transaction 62 was rolled back, all of the locks taken by it are released.</p> <pre><code>mysql&gt; SELECT * FROM INFORMATION_SCHEMA.TOKUDB_LOCKS;\n</code></pre> <p>The output could be:</p> <pre><code>+--------------+-----------------------+---------------+----------------+-----------------+--------------------+------------------+-----------------------------+\n| locks_trx_id | locks_mysql_thread_id | locks_dname   | locks_key_left | locks_key_right | locks_table_schema | locks_table_name | locks_table_dictionary_name |\n+--------------+-----------------------+---------------+----------------+-----------------+--------------------+------------------+-----------------------------+\n|           51 |                     1 | ./test/t-main | 0001000000     | 0001000000      | test               | t                | main                        |\n|           51 |                     1 | ./test/t-main | 000a000000     | 000a000000      | test               | t                | main                        |\n|           51 |                     1 | ./test/t-main | 0064000000     | 0064000000      | test               | t                | main                        |\n|           51 |                     2 | ./test/t-main | 0002000000     | 0002000000      | test               | t                | main                        |\n|           51 |                     2 | ./test/t-main | 0014000000     | 0014000000      | test               | t                | main                        |\n+--------------+-----------------------+---------------+----------------+-----------------+--------------------+------------------+-----------------------------+\n</code></pre>"},{"location":"tokudb/tokudb_troubleshooting.html#engine-status","title":"Engine Status","text":"<p>Engine status provides details about the inner workings of TokuDB and can be useful in tuning your particular environment. The engine status can be determined by running the following command:</p> <pre><code>SHOW ENGINE tokudb STATUS;\n</code></pre> <p>The following is a reference of table status statements:</p> <p><code>disk free space</code>:</p> <pre><code>This is a gross estimate of how much of your file system is available.\nPossible displays in this field are:\n\n\n* More than twice the reserve (\u201cmore than 10 percent of total file system\nspace\u201d)\n\n\n* Less than twice the reserve\n\n\n* Less than the reserve\n\n\n* File system is completely full\n</code></pre> <p><code>time of environment creation</code>:</p> <pre><code>This is the time when the TokuDB storage engine was first started up.\nNormally, this is when `mysqld` was initially installed with TokuDB. If\nthe environment was upgraded from TokuDB 4.x (4.2.0 or later), then this\nwill be displayed as \u201cDec 31, 1969\u201d on Linux hosts.\n</code></pre> <p><code>time of engine startup</code>:</p> <pre><code>This is the time when the TokuDB storage engine started up. Normally, this\nis when `mysqld` started.\n</code></pre> <p><code>time now</code>:</p> <pre><code>Current date/time on server.\n</code></pre> <p><code>db opens</code>:</p> <pre><code>This is the number of times an individual PerconaFT dictionary file was\nopened. This is a not a useful value for a regular user to use for any purpose\ndue to layers of open/close caching on top.\n</code></pre> <p><code>db closes</code>:</p> <pre><code>This is the number of times an individual PerconaFT dictionary file was\nclosed. This is a not a useful value for a regular user to use for any purpose\ndue to layers of open/close caching on top.\n</code></pre> <p><code>num open dbs now</code>:</p> <pre><code>This is the number of currently open databases.\n</code></pre> <p><code>max open dbs</code>:</p> <pre><code>This is the maximum number of concurrently opened databases.\n</code></pre> <p><code>period, in ms, that recovery log is automatically fsynced</code>:</p> <pre><code>`fsync()` frequency in milliseconds.\n</code></pre> <p><code>dictionary inserts</code>:</p> <pre><code>This is the total number of rows that have been inserted into all primary and\nsecondary indexes combined, when those inserts have been done with a separate\nrecovery log entry per index. For example, inserting a row into a table with\none primary and two secondary indexes will increase this count by three, if\nthe inserts were done with separate recovery log entries.\n</code></pre> <p><code>dictionary inserts fail</code>:</p> <pre><code>This is the number of single-index insert operations that failed.\n</code></pre> <p><code>dictionary deletes</code>:</p> <pre><code>This is the total number of rows that have been deleted from all primary and\nsecondary indexes combined, if those deletes have been done with a separate\nrecovery log entry per index.\n</code></pre> <p><code>dictionary deletes fail</code>:</p> <pre><code>This is the number of single-index delete operations that failed.\n</code></pre> <p><code>dictionary updates</code>:</p> <pre><code>This is the total number of rows that have been updated in all primary and\nsecondary indexes combined, if those updates have been done with a separate\nrecovery log entry per index.\n</code></pre> <p><code>dictionary updates fail</code>:</p> <pre><code>This is the number of single-index update operations that failed.\n</code></pre> <p><code>dictionary broadcast updates</code>:</p> <pre><code>This is the number of broadcast updates that have been successfully performed.\nA broadcast update is an update that affects all rows in a dictionary.\n</code></pre> <p><code>dictionary broadcast updates fail</code>:</p> <pre><code>This is the number of broadcast updates that have failed.\n</code></pre> <p><code>dictionary multi inserts</code>:</p> <pre><code>This is the total number of rows that have been inserted into all primary and\nsecondary indexes combined, when those inserts have been done with a single\nrecovery log entry for the entire row. (For example, inserting a row into a\ntable with one primary and two secondary indexes will normally increase this\ncount by three).\n</code></pre> <p><code>dictionary multi inserts fail</code>:</p> <pre><code>This is the number of multi-index insert operations that failed.\n</code></pre> <p><code>dictionary multi deletes</code>:</p> <pre><code>This is the total number of rows that have been deleted from all primary and\nsecondary indexes combined, when those deletes have been done with a single\nrecovery log entry for the entire row.\n</code></pre> <p><code>dictionary multi deletes fail</code>:</p> <pre><code>This is the number of multi-index delete operations that failed.\n</code></pre> <p><code>dictionary updates multi</code>:</p> <pre><code>This is the total number of rows that have been updated in all primary and\nsecondary indexes combined, if those updates have been done with a single\nrecovery log entry for the entire row.\n</code></pre> <p><code>dictionary updates fail multi</code>:</p> <pre><code>This is the number of multi-index update operations that failed.\n</code></pre> <p><code>le: max committed xr</code>:</p> <pre><code>This is the maximum number of committed transaction records that were stored\non disk in a new or modified row.\n</code></pre> <p><code>le: max provisional xr</code>:</p> <pre><code>This is the maximum number of provisional transaction records that were stored\non disk in a new or modified row.\n</code></pre> <p><code>le: expanded</code>:</p> <pre><code>This is the number of times that an expanded memory mechanism was used to\nstore a new or modified row on disk.\n</code></pre> <p><code>le: max memsize</code>:</p> <pre><code>This is the maximum number of bytes that were stored on disk as a new or\nmodified row. This is the maximum uncompressed size of any row stored in\nTokuDB that was created or modified since the server started.\n</code></pre> <p><code>le: size of leafentries before garbage collection (during message application)</code>:</p> <pre><code>Total number of bytes of leaf nodes data before performing garbage collection\nfor non-flush events.\n</code></pre> <p><code>le: size of leafentries after garbage collection (during message application)</code>:</p> <pre><code>Total number of bytes of leaf nodes data after performing garbage collection\nfor non-flush events.\n</code></pre> <p><code>le: size of leafentries before garbage collection (outside message application)</code>:</p> <pre><code>Total number of bytes of leaf nodes data before performing garbage collection\nfor flush events.\n</code></pre> <p><code>le: size of leafentries after garbage collection (outside message application)</code>:</p> <pre><code>Total number of bytes of leaf nodes data after performing garbage collection\nfor flush events.\n</code></pre> <p><code>checkpoint: period</code>:</p> <pre><code>This is the interval in seconds between the end of an automatic checkpoint and\nthe beginning of the next automatic checkpoint.\n</code></pre> <p><code>checkpoint: footprint</code>:</p> <pre><code>Where the database is in the checkpoint process.\n</code></pre> <p><code>checkpoint: last checkpoint began</code>:</p> <pre><code>This is the time the last checkpoint began. If a checkpoint is currently in\nprogress, then this time may be later than the time the last checkpoint\ncompleted.\n\n**NOTE**: If no checkpoint has ever taken place, then this value will be `Dec 31,\n1969` on Linux hosts.\n</code></pre> <p><code>checkpoint: last complete checkpoint began</code>:</p> <pre><code>This is the time the last complete checkpoint started. Any data that changed\nafter this time will not be captured in the checkpoint.\n</code></pre> <p><code>checkpoint: last complete checkpoint ended</code>:</p> <pre><code>This is the time the last complete checkpoint ended.\n</code></pre> <p><code>checkpoint: time spent during checkpoint (begin and end phases)</code>:</p> <pre><code>Time (in seconds) required to complete all checkpoints.\n</code></pre> <p><code>checkpoint: time spent during last checkpoint (begin and end phases)</code>:</p> <pre><code>Time (in seconds) required to complete the last checkpoint.\n</code></pre> <p><code>checkpoint: last complete checkpoint LSN</code>:</p> <pre><code>This is the Log Sequence Number of the last complete checkpoint.\n</code></pre> <p><code>checkpoint: checkpoints taken</code>:</p> <pre><code>This is the number of complete checkpoints that have been taken.\n</code></pre> <p><code>checkpoint: checkpoints failed</code>:</p> <pre><code>This is the number of checkpoints that have failed for any reason.\n</code></pre> <p><code>checkpoint: waiters now</code>:</p> <pre><code>This is the current number of threads simultaneously waiting for the\ncheckpoint-safe lock to perform a checkpoint.\n</code></pre> <p><code>checkpoint: waiters max</code>:</p> <pre><code>This is the maximum number of threads ever simultaneously waiting for the\ncheckpoint-safe lock to perform a checkpoint.\n</code></pre> <p><code>checkpoint: non-checkpoint client wait on mo lock</code>:</p> <pre><code>The number of times a non-checkpoint client thread waited for the\nmulti-operation lock.\n</code></pre> <p><code>checkpoint: non-checkpoint client wait on cs lock</code>:</p> <pre><code>The number of times a non-checkpoint client thread waited for the\ncheckpoint-safe lock.\n</code></pre> <p><code>checkpoint: checkpoint begin time</code>:</p> <pre><code>Cumulative time (in microseconds) required to mark all dirty nodes as\npending a checkpoint.\n</code></pre> <p><code>checkpoint: long checkpoint begin time</code>:</p> <pre><code>The total time, in microseconds, of long checkpoint begins. A long checkpoint\nbegin is one taking more than 1 second.\n</code></pre> <p><code>checkpoint: long checkpoint begin count</code>:</p> <pre><code>The total number of times a checkpoint begin took more than 1 second.\n</code></pre> <p><code>checkpoint: checkpoint end time</code>:</p> <pre><code>The time spent in checkpoint end operation in seconds.\n</code></pre> <p><code>checkpoint: long checkpoint end time</code>:</p> <pre><code>The time spent in checkpoint end operation in seconds.\n</code></pre> <p><code>checkpoint: long checkpoint end count</code>:</p> <pre><code>This is the count of end_checkpoint operations that exceeded 1 minute.\n</code></pre> <p><code>cachetable: miss</code>:</p> <pre><code>This is a count of how many times the application was unable to access your\ndata in the internal cache.\n</code></pre> <p><code>cachetable: miss time</code>:</p> <pre><code>This is the total time, in microseconds, of how long the database has had to\nwait for a disk read to complete.\n</code></pre> <p><code>cachetable: prefetches</code>:</p> <pre><code>This is the total number of times that a block of memory has been prefetched\ninto the database\u2019s cache. Data is prefetched when the database\u2019s algorithms\ndetermine that a block of memory is likely to be accessed by the application.\n</code></pre> <p><code>cachetable: size current</code>:</p> <pre><code>This shows how much of the uncompressed data, in bytes, is currently in the\ndatabase\u2019s internal cache.\n</code></pre> <p><code>cachetable: size limit</code>:</p> <pre><code>This shows how much of the uncompressed data, in bytes, will fit in the\ndatabase\u2019s internal cache.\n</code></pre> <p><code>cachetable: size writing</code></p> <pre><code>This is the number of bytes that are currently queued up to be written to\ndisk.\n</code></pre> <p><code>cachetable: size nonleaf</code>:</p> <pre><code>This shows the amount of memory, in bytes, the current set of non-leaf nodes\noccupy in the cache.\n</code></pre> <p><code>cachetable: size leaf</code>:</p> <pre><code>This shows the amount of memory, in bytes, the current set of (decompressed)\nleaf nodes occupy in the cache.\n</code></pre> <p><code>cachetable: size rollback</code>:</p> <pre><code>This shows the rollback nodes size, in bytes, in the cache.\n</code></pre> <p><code>cachetable: size cachepressure</code>:</p> <pre><code>This shows the number of bytes causing cache pressure (the sum of buffers and\nwork done counters), helps to understand if cleaner threads are keeping up\nwith workload. It should really be looked at as more of a value to use in a\nratio of cache pressure / cache table size. The closer that ratio evaluates to\n1, the higher the cache pressure.\n</code></pre> <p><code>cachetable: size currently cloned data for checkpoint</code>:</p> <pre><code>Amount of memory, in bytes, currently used for cloned nodes. During the\ncheckpoint operation, dirty nodes are cloned prior to\nserialization/compression, then written to disk. After which, the memory for\nthe cloned block is returned for re-use.\n</code></pre> <p><code>cachetable: evictions</code>:</p> <pre><code>Number of blocks evicted from cache.\n</code></pre> <p><code>cachetable: cleaner executions</code>:</p> <pre><code>Total number of times the cleaner thread loop has executed.\n</code></pre> <p><code>cachetable: cleaner period</code>:</p> <pre><code>TokuDB includes a cleaner thread that optimizes indexes in the background.\nThis variable is the time, in seconds, between the completion of a group of\ncleaner operations and the beginning of the next group of cleaner operations.\nThe cleaner operations run on a background thread performing work that does\nnot need to be done on the client thread.\n</code></pre> <p><code>cachetable: cleaner iterations:</code></p> <pre><code>This is the number of cleaner operations that are performed every cleaner\nperiod.\n</code></pre> <p><code>cachetable: number of waits on cache pressure</code>:</p> <pre><code>The number of times a thread was stalled due to cache pressure.\n</code></pre> <p><code>cachetable: time waiting on cache pressure</code>:</p> <pre><code>Total time, in microseconds, waiting on cache pressure to subside.\n</code></pre> <p><code>cachetable: number of long waits on cache pressure</code>:</p> <pre><code>The number of times a thread was stalled for more than 1 second due to cache\npressure.\n</code></pre> <p><code>cachetable: long time waiting on cache pressure</code>:</p> <pre><code>Total time, in microseconds, waiting on cache pressure to subside for more\nthan 1 second.\n</code></pre> <p><code>cachetable: client pool: number of threads in pool</code>:</p> <pre><code>The number of threads in the client thread pool.\n</code></pre> <p><code>cachetable: client pool: number of currently active threads in pool</code>:</p> <pre><code>The number of currently active threads in the client thread pool.\n</code></pre> <p><code>cachetable: client pool: number of currently queued work items</code>:</p> <pre><code>The number of currently queued work items in the client thread pool.\n</code></pre> <p><code>cachetable: client pool: largest number of queued work items</code>:</p> <pre><code>The largest number of queued work items in the client thread pool.\n</code></pre> <p><code>cachetable: client pool: total number of work items processed</code>:</p> <pre><code>The total number of work items processed in the client thread pool.\n</code></pre> <p><code>cachetable: client pool: total execution time of processing work items</code>:</p> <pre><code>The total execution time of processing work items in the client thread pool.\n</code></pre> <p><code>cachetable: cachetable pool: number of threads in pool</code>:</p> <pre><code>The number of threads in the cachetable thread pool.\n</code></pre> <p><code>cachetable: cachetable pool: number of currently active threads in pool</code>:</p> <pre><code>The number of currently active threads in the cachetable thread pool.\n</code></pre> <p><code>cachetable: cachetable pool: number of currently queued work items</code>:</p> <pre><code>The number of currently queued work items in the cachetable thread pool.\n</code></pre> <p><code>cachetable: cachetable pool: largest number of queued work items</code>:</p> <pre><code>The largest number of queued work items in the cachetable thread pool.\n</code></pre> <p><code>cachetable: cachetable pool: total number of work items processed</code>:</p> <pre><code>The total number of work items processed in the cachetable thread pool.\n</code></pre> <p><code>cachetable: cachetable pool: total execution time of processing work items</code>:</p> <pre><code>The total execution time of processing work items in the cachetable thread\npool.\n</code></pre> <p><code>cachetable: checkpoint pool: number of threads in pool</code>:</p> <pre><code>The number of threads in the checkpoint thread pool.\n</code></pre> <p><code>cachetable: checkpoint pool: number of currently active threads in pool</code>:</p> <pre><code>The number of currently active threads in the checkpoint thread pool.\n</code></pre> <p><code>cachetable: checkpoint pool: number of currently queued work items</code>:</p> <pre><code>The number of currently queued work items in the checkpoint thread pool.\n</code></pre> <p><code>cachetable: checkpoint pool: largest number of queued work items</code>:</p> <pre><code>The largest number of queued work items in the checkpoint thread pool.\n</code></pre> <p><code>cachetable: checkpoint pool: total number of work items processed</code>:</p> <pre><code>The total number of work items processed in the checkpoint thread pool.\n</code></pre> <p><code>cachetable: checkpoint pool: total execution time of processing work items</code>:</p> <pre><code>The total execution time of processing work items in the checkpoint thread\npool.\n</code></pre> <p><code>locktree: memory size</code>:</p> <pre><code>The amount of memory, in bytes, that the locktree is currently using.\n</code></pre> <p><code>locktree: memory size limit</code>:</p> <pre><code>The maximum amount of memory, in bytes, that the locktree is allowed to use.\n</code></pre> <p><code>locktree: number of times lock escalation ran</code>:</p> <pre><code>Number of times the locktree needed to run lock escalation to reduce its\nmemory footprint.\n</code></pre> <p><code>locktree: time spent running escalation (seconds)</code>:</p> <pre><code>Total number of seconds spent performing locktree escalation.\n</code></pre> <p><code>locktree: latest post-escalation memory size</code>:</p> <pre><code>Size of the locktree, in bytes, after most current locktree escalation.\n</code></pre> <p><code>locktree: number of locktrees open now</code>:</p> <pre><code>Number of locktrees currently open.\n</code></pre> <p><code>locktree: number of pending lock requests</code>:</p> <pre><code>Number of requests waiting for a lock grant.\n</code></pre> <p><code>locktree: number of locktrees eligible for the STO</code>:</p> <pre><code>Number of locktrees eligible for \u201cSingle Transaction Optimizations\u201d. `STO`\noptimization are behaviors that can happen within the locktree when there is\nexactly one transaction active within the locktree. This is a not a useful\nvalue for a regular user to use for any purpose.\n</code></pre> <p><code>locktree: number of times a locktree ended the STO early</code>:</p> <pre><code>Total number of times a \u201csingle transaction optimization\u201d was ended early due\nto another trans- action starting.\n</code></pre> <p><code>locktree: time spent ending the STO early (seconds)</code>:</p> <pre><code>Total number of seconds ending \u201cSingle Transaction Optimizations\u201d. `STO`\noptimization are behaviors that can happen within the locktree when there is\nexactly one transaction active within the locktree. This is a not a useful\nvalue for a regular user to use for any purpose.\n</code></pre> <p><code>locktree: number of wait locks</code>:</p> <pre><code>Number of times that a lock request could not be acquired because of a\nconflict with some other transaction.\n</code></pre> <p><code>locktree: time waiting for locks</code>:</p> <pre><code>Total time, in microseconds, spend by some client waiting for a lock conflict\nto be resolved.\n</code></pre> <p><code>locktree: number of long wait locks</code>:</p> <pre><code>Number of lock waits greater than 1 second in duration.\n</code></pre> <p><code>locktree: long time waiting for locks</code>:</p> <pre><code>Total time, in microseconds, of the long waits.\n</code></pre> <p><code>locktree: number of lock timeouts</code>:</p> <pre><code>Count of the number of times that a lock request timed out.\n</code></pre> <p><code>locktree: number of waits on lock escalation</code>:</p> <pre><code>When the sum of the sizes of locks taken reaches the lock tree limit, we run\nlock escalation on a background thread. The clients threads need to wait for\nescalation to consolidate locks and free up memory. This counter counts the\nnumber of times a client thread has to wait on lock escalation.\n</code></pre> <p><code>locktree: time waiting on lock escalation</code>:</p> <pre><code>Total time, in microseconds, that a client thread spent waiting for lock\nescalation to free up memory.\n</code></pre> <p><code>locktree: number of long waits on lock escalation</code>:</p> <pre><code>Number of times that a client thread had to wait on lock escalation and the\nwait time was greater than 1 second.\n</code></pre> <p><code>locktree: long time waiting on lock escalation</code>:</p> <pre><code>Total time, in microseconds, of the long waits for lock escalation to free up\nmemory.\n</code></pre> <p><code>ft: dictionary updates</code>:</p> <pre><code>This is the total number of rows that have been updated in all primary and\nsecondary indexes combined, if those updates have been done with a separate\nrecovery log entry per index.\n</code></pre> <p><code>ft: dictionary broadcast updates</code>:</p> <pre><code>This is the number of broadcast updates that have been successfully performed.\nA broadcast update is an update that affects all rows in a dictionary.\n</code></pre> <p><code>ft: descriptor set</code>:</p> <pre><code>This is the number of time a descriptor was updated when the entire dictionary\nwas updated (for example, when the schema has been changed).\n</code></pre> <p><code>ft: messages ignored by leaf due to msn</code>:</p> <pre><code>The number of messages that were ignored by a leaf because it had already been\napplied.\n</code></pre> <p><code>ft: total search retries due to TRY AGAIN</code></p> <pre><code>Total number of search retries due to TRY AGAIN. Internal value that is no use\nto anyone other than a developer debugging a specific query/search issue.\n</code></pre> <p><code>ft: searches requiring more tries than the height of the tree</code>:</p> <pre><code>Number of searches that required more tries than the height of the tree.\n</code></pre> <p><code>ft: searches requiring more tries than the height of the tree plus three</code></p> <pre><code>Number of searches that required more tries than the height of the tree plus\nthree.\n</code></pre> <p><code>ft: leaf nodes flushed to disk (not for checkpoint)</code>:</p> <pre><code>Number of leaf nodes flushed to disk, not for checkpoint.\n</code></pre> <p><code>ft: leaf nodes flushed to disk (not for checkpoint) (bytes)</code>:</p> <pre><code>Number of bytes of leaf nodes flushed to disk, not for checkpoint.\n</code></pre> <p><code>ft: leaf nodes flushed to disk (not for checkpoint) (uncompressed bytes)</code>:</p> <pre><code>Number of bytes of leaf nodes flushed to disk, not for checkpoint.\n</code></pre> <p><code>ft: leaf nodes flushed to disk (not for checkpoint) (seconds)</code>:</p> <pre><code>Number of seconds waiting for IO when writing leaf nodes flushed to disk, not\nfor checkpoint.\n</code></pre> <p><code>ft: nonleaf nodes flushed to disk (not for checkpoint)</code>:</p> <pre><code>Number of non-leaf nodes flushed to disk, not for checkpoint.\n</code></pre> <p><code>ft: nonleaf nodes flushed to disk (not for checkpoint) (bytes)</code>:</p> <pre><code>Number of bytes of non-leaf nodes flushed to disk, not for checkpoint.\n</code></pre> <p><code>ft: nonleaf nodes flushed to disk (not for checkpoint) (uncompressed bytes)</code>:</p> <pre><code>Number of uncompressed bytes of non-leaf nodes flushed to disk, not for\ncheckpoint.\n</code></pre> <p><code>ft: nonleaf nodes flushed to disk (not for checkpoint) (seconds)</code>:</p> <pre><code>Number of seconds waiting for I/O when writing non-leaf nodes flushed to disk,\nnot for checkpoint.\n</code></pre> <p><code>ft: leaf nodes flushed to disk (for checkpoint)</code>:</p> <pre><code>Number of leaf nodes flushed to disk for checkpoint.\n</code></pre> <p><code>ft: leaf nodes flushed to disk (for checkpoint) (bytes)</code>:</p> <pre><code>Number of bytes of leaf nodes flushed to disk for checkpoint.\n</code></pre> <p><code>ft: leaf nodes flushed to disk (for checkpoint) (uncompressed bytes)</code>:</p> <pre><code>Number of uncompressed bytes of leaf nodes flushed to disk for checkpoint.\n</code></pre> <p><code>ft: leaf nodes flushed to disk (for checkpoint) (seconds)</code></p> <pre><code>Number of seconds waiting for IO when writing leaf nodes flushed to disk for\ncheckpoint.\n</code></pre> <p><code>ft: nonleaf nodes flushed to disk (for checkpoint)</code>:</p> <pre><code>Number of non-leaf nodes flushed to disk for checkpoint.\n</code></pre> <p><code>ft: nonleaf nodes flushed to disk (for checkpoint) (bytes)</code>:</p> <pre><code>Number of bytes of non-leaf nodes flushed to disk for checkpoint.\n</code></pre> <p><code>ft: nonleaf nodes flushed to disk (for checkpoint) (uncompressed bytes)</code>:</p> <pre><code>Number of uncompressed bytes of non-leaf nodes flushed to disk for checkpoint.\n</code></pre> <p><code>ft: nonleaf nodes flushed to disk (for checkpoint) (seconds)</code>:</p> <pre><code>Number of seconds waiting for IO when writing non-leaf nodes flushed to disk\nfor checkpoint.\n</code></pre> <p><code>ft: uncompressed / compressed bytes written (leaf)</code>:</p> <pre><code>Ratio of uncompressed bytes (in-memory) to compressed bytes (on-disk) for leaf\nnodes.\n</code></pre> <p><code>ft: uncompressed / compressed bytes written (nonleaf)</code>:</p> <pre><code>Ratio of uncompressed bytes (in-memory) to compressed bytes (on-disk) for\nnon-leaf nodes.\n</code></pre> <p><code>ft: uncompressed / compressed bytes written (overall)</code>:</p> <pre><code>Ratio of uncompressed bytes (in-memory) to compressed bytes (on-disk) for all\nnodes.\n</code></pre> <p><code>ft: nonleaf node partial evictions</code>:</p> <pre><code>The number of times a partition of a non-leaf node was evicted from the cache.\n</code></pre> <p><code>ft: nonleaf node partial evictions (bytes)</code>:</p> <pre><code>The number of bytes freed by evicting partitions of non-leaf nodes from the\ncache.\n</code></pre> <p><code>ft: leaf node partial evictions</code>:</p> <pre><code>The number of times a partition of a leaf node was evicted from the cache.\n</code></pre> <p><code>ft: leaf node partial evictions (bytes)</code>:</p> <pre><code>The number of bytes freed by evicting partitions of leaf nodes from the cache.\n</code></pre> <p><code>ft: leaf node full evictions</code></p> <pre><code>The number of times a full leaf node was evicted from the cache.\n</code></pre> <p><code>ft: leaf node full evictions (bytes)</code>:</p> <pre><code>The number of bytes freed by evicting full leaf nodes from the cache.\n</code></pre> <p><code>ft: nonleaf node full evictions (bytes)</code>:</p> <pre><code>The number of bytes freed by evicting full non-leaf nodes from the cache.\n</code></pre> <p><code>ft: nonleaf node full evictions</code>:</p> <pre><code>The number of times a full non-leaf node was evicted from the cache.\n</code></pre> <p><code>ft: leaf nodes created</code>:</p> <pre><code>Number of created leaf nodes .\n</code></pre> <p><code>ft: nonleaf nodes created</code>:</p> <pre><code>Number of created non-leaf nodes.\n</code></pre> <p><code>ft: leaf nodes destroyed</code>:</p> <pre><code>Number of destroyed leaf nodes.\n</code></pre> <p><code>ft: nonleaf nodes destroyed</code>:</p> <pre><code>Number of destroyed non-leaf nodes.\n</code></pre> <p><code>ft: bytes of messages injected at root (all trees)</code>:</p> <pre><code>Amount of messages, in bytes, injected at root (for all trees).\n</code></pre> <p><code>ft: bytes of messages flushed from h1 nodes to leaves</code></p> <pre><code>Amount of messages, in bytes, flushed from `h1` nodes to leaves.\n</code></pre> <p><code>ft: bytes of messages currently in trees (estimate)</code>:</p> <pre><code>Amount of messages, in bytes, currently in trees (estimate).\n</code></pre> <p><code>ft: messages injected at root</code>:</p> <pre><code>Number of messages injected at root node of a tree.\n</code></pre> <p><code>ft: broadcast messages injected at root</code>:</p> <pre><code>Number of broadcast messages injected at root node of a tree.\n</code></pre> <p><code>ft: basements decompressed as a target of a query</code>:</p> <pre><code>Number of basement nodes decompressed for queries.\n</code></pre> <p><code>ft: basements decompressed for prelocked range</code>:</p> <pre><code>Number of basement nodes decompressed by queries aggressively.\n</code></pre> <p><code>ft: basements decompressed for prefetch</code>:</p> <pre><code>Number of basement nodes decompressed by a prefetch thread.\n</code></pre> <p><code>ft: basements decompressed for write</code>:</p> <pre><code>Number of basement nodes decompressed for writes.\n</code></pre> <p><code>ft: buffers decompressed as a target of a query</code>:</p> <pre><code>Number of buffers decompressed for queries.\n</code></pre> <p><code>ft: buffers decompressed for prelocked range</code>:</p> <pre><code>Number of buffers decompressed by queries aggressively.\n</code></pre> <p><code>ft: buffers decompressed for prefetch</code>:</p> <pre><code>Number of buffers decompressed by a prefetch thread.\n</code></pre> <p><code>ft: buffers decompressed for write</code>:</p> <pre><code>Number of buffers decompressed for writes.\n</code></pre> <p><code>ft: pivots fetched for query</code>:</p> <pre><code>Number of pivot nodes fetched for queries.\n</code></pre> <p><code>ft: pivots fetched for query (bytes)</code>:</p> <pre><code>Number of bytes of pivot nodes fetched for queries.\n</code></pre> <p><code>ft: pivots fetched for query (seconds)</code>:</p> <pre><code>Number of seconds waiting for I/O when fetching pivot nodes for queries.\n</code></pre> <p><code>ft: pivots fetched for prefetch</code>:</p> <pre><code>Number of pivot nodes fetched by a prefetch thread.\n</code></pre> <p><code>ft: pivots fetched for prefetch (bytes)</code>:</p> <pre><code>Number of bytes of pivot nodes fetched by a prefetch thread.\n</code></pre> <p><code>ft: pivots fetched for prefetch (seconds)</code>:</p> <pre><code>Number seconds waiting for I/O when fetching pivot nodes by a prefetch thread.\n</code></pre> <p><code>ft: pivots fetched for write</code>:</p> <pre><code>Number of pivot nodes fetched for writes.\n</code></pre> <p><code>ft: pivots fetched for write (bytes)</code>:</p> <pre><code>Number of bytes of pivot nodes fetched for writes.\n</code></pre> <p><code>ft: pivots fetched for write (seconds)</code>:</p> <pre><code>Number of seconds waiting for I/O when fetching pivot nodes for writes.\n</code></pre> <p><code>ft: basements fetched as a target of a query</code>:</p> <pre><code>Number of basement nodes fetched from disk for queries.\n</code></pre> <p><code>ft: basements fetched as a target of a query (bytes)</code>:</p> <pre><code>Number of basement node bytes fetched from disk for queries.\n</code></pre> <p><code>ft: basements fetched as a target of a query (seconds)</code>:</p> <pre><code>Number of seconds waiting for IO when fetching basement nodes from disk for\nqueries.\n</code></pre> <p><code>ft: basements fetched for prelocked range</code>:</p> <pre><code>Number of basement nodes fetched from disk aggressively.\n</code></pre> <p><code>ft: basements fetched for prelocked range (bytes)</code>:</p> <pre><code>Number of basement node bytes fetched from disk aggressively.\n</code></pre> <p><code>ft: basements fetched for prelocked range (seconds)</code>:</p> <pre><code>Number of seconds waiting for I/O when fetching basement nodes from disk\naggressively.\n</code></pre> <p><code>ft: basements fetched for prefetch</code>:</p> <pre><code>Number of basement nodes fetched from disk by a prefetch thread.\n</code></pre> <p><code>ft: basements fetched for prefetch (bytes)</code>:</p> <pre><code>Number of basement node bytes fetched from disk by a prefetch thread.\n</code></pre> <p><code>ft: basements fetched for prefetch (seconds)</code>:</p> <pre><code>Number of seconds waiting for I/O when fetching basement nodes from disk by a\nprefetch thread.\n</code></pre> <p><code>ft: basements fetched for write</code>:</p> <pre><code>Number of basement nodes fetched from disk for writes.\n</code></pre> <p><code>ft: basements fetched for write (bytes)</code>:</p> <pre><code>Number of basement node bytes fetched from disk for writes.\n</code></pre> <p><code>ft: basements fetched for write (seconds)</code>:</p> <pre><code>Number of seconds waiting for I/O when fetching basement nodes from disk for\nwrites.\n</code></pre> <p><code>ft: buffers fetched as a target of a query</code>:</p> <pre><code>Number of buffers fetched from disk for queries.\n</code></pre> <p><code>ft: buffers fetched as a target of a query (bytes)</code>:</p> <pre><code>Number of buffer bytes fetched from disk for queries.\n</code></pre> <p><code>ft: buffers fetched as a target of a query (seconds)</code>:</p> <pre><code>Number of seconds waiting for I/O when fetching buffers from disk for queries.\n</code></pre> <p><code>ft: buffers fetched for prelocked range</code>:</p> <pre><code>Number of buffers fetched from disk aggressively.\n</code></pre> <p><code>ft: buffers fetched for prelocked range (bytes)</code>:</p> <pre><code>Number of buffer bytes fetched from disk aggressively.\n</code></pre> <p><code>ft: buffers fetched for prelocked range (seconds)</code>:</p> <pre><code>Number of seconds waiting for I/O when fetching buffers from disk\naggressively.\n</code></pre> <p><code>ft: buffers fetched for prefetch</code>:</p> <pre><code>Number of buffers fetched from disk by a prefetch thread.\n</code></pre> <p><code>ft: buffers fetched for prefetch (bytes)</code>:</p> <pre><code>Number of buffer bytes fetched from disk by a prefetch thread.\n</code></pre> <p><code>ft: buffers fetched for prefetch (seconds)</code>:</p> <pre><code>Number of seconds waiting for I/O when fetching buffers from disk by a\nprefetch thread.\n</code></pre> <p><code>ft: buffers fetched for write</code>:</p> <pre><code>Number of buffers fetched from disk for writes.\n</code></pre> <p><code>ft: buffers fetched for write (bytes)</code>:</p> <pre><code>Number of buffer bytes fetched from disk for writes.\n</code></pre> <p><code>ft: buffers fetched for write (seconds)</code>:</p> <pre><code>Number of seconds waiting for I/O when fetching buffers from disk for writes.\n</code></pre> <p><code>ft: leaf compression to memory (seconds)</code>:</p> <pre><code>Total time, in seconds, spent compressing leaf nodes.\n</code></pre> <p><code>ft: leaf serialization to memory (seconds)</code>:</p> <pre><code>Total time, in seconds, spent serializing leaf nodes.\n</code></pre> <p><code>ft: leaf decompression to memory (seconds)</code>:</p> <pre><code>Total time, in seconds, spent decompressing leaf nodes.\n</code></pre> <p><code>ft: leaf deserialization to memory (seconds)</code>:</p> <pre><code>Total time, in seconds, spent deserializing leaf nodes.\n</code></pre> <p><code>ft: nonleaf compression to memory (seconds)</code>:</p> <pre><code>Total time, in seconds, spent compressing non leaf nodes.\n</code></pre> <p><code>ft: nonleaf serialization to memory (seconds)</code>:</p> <pre><code>Total time, in seconds, spent serializing non leaf nodes.\n</code></pre> <p><code>ft: nonleaf decompression to memory (seconds)</code>:</p> <pre><code>Total time, in seconds, spent decompressing non leaf nodes.\n</code></pre> <p><code>ft: nonleaf deserialization to memory (seconds)</code>:</p> <pre><code>Total time, in seconds, spent deserializing non leaf nodes.\n</code></pre> <p><code>ft: promotion: roots split</code>:</p> <pre><code>Number of times the root split during promotion.\n</code></pre> <p><code>ft: promotion: leaf roots injected into</code>:</p> <pre><code>Number of times a message stopped at a root with height `0`.\n</code></pre> <p><code>ft: promotion: h1 roots injected into</code>:</p> <pre><code>Number of times a message stopped at a root with height `1`.\n</code></pre> <p><code>ft: promotion: injections at depth 0</code>:</p> <pre><code>Number of times a message stopped at depth `0`.\n</code></pre> <p><code>ft: promotion: injections at depth 1</code>:</p> <pre><code>Number of times a message stopped at depth `1`.\n</code></pre> <p><code>ft: promotion: injections at depth 2</code>:</p> <pre><code>Number of times a message stopped at depth `2`.\n</code></pre> <p><code>ft: promotion: injections at depth 3</code>:</p> <pre><code>Number of times a message stopped at depth `3`.\n</code></pre> <p><code>ft: promotion: injections lower than depth 3</code>:</p> <pre><code>Number of times a message was promoted past depth `3`.\n</code></pre> <p><code>ft: promotion: stopped because of a nonempty buffer</code>:</p> <pre><code>Number of times a message stopped because it reached a nonempty buffer.\n</code></pre> <p><code>ft: promotion: stopped at height 1</code></p> <pre><code>Number of times a message stopped because it had reached height `1`.\n</code></pre> <p><code>ft: promotion: stopped because the child was locked or not at all in memory</code>:</p> <pre><code>Number of times promotion was stopped because the child node was locked or not\nat all in memory. This is a not a useful value for a regular user to use for\nany purpose.\n</code></pre> <p><code>ft: promotion: stopped because the child was not fully in memory</code>:</p> <pre><code>Number of times promotion was stopped because the child node was not at all in\nmemory. This is a not a useful value for a normal user to use for any purpose.\n</code></pre> <p><code>ft: promotion: stopped anyway, after locking the child</code>:</p> <pre><code>Number of times a message stopped before a child which had been locked.\n</code></pre> <p><code>ft: basement nodes deserialized with fixed-keysize</code>:</p> <pre><code>The number of basement nodes deserialized where all keys had the same size,\nleaving the basement in a format that is optimal for in-memory workloads.\n</code></pre> <p><code>ft: basement nodes deserialized with variable-keysize</code>:</p> <pre><code>The number of basement nodes deserialized where all keys did not have the same\nsize, and thus ineligible for an in-memory optimization.\n</code></pre> <p><code>ft: promotion: succeeded in using the rightmost leaf shortcut</code>:</p> <pre><code>Rightmost insertions used the rightmost-leaf pin path, meaning that the\nFractal Tree index detected and properly optimized rightmost inserts.\n</code></pre> <p><code>ft: promotion: tried the rightmost leaf shortcut but failed (out-of-bounds)</code>:</p> <pre><code>Rightmost insertions did not use the rightmost-leaf pin path, due to the\ninsert not actually being into the rightmost leaf node.\n</code></pre> <p><code>ft: promotion: tried the rightmost leaf shortcut but failed (child reactive)</code>:</p> <pre><code>Rightmost insertions did not use the rightmost-leaf pin path, due to the\nleaf being too large (needed to split).\n</code></pre> <p><code>ft: cursor skipped deleted leaf entries</code>:</p> <pre><code>Number of leaf entries skipped during search/scan because the result of\nmessage application and reconciliation of the leaf entry MVCC stack reveals\nthat the leaf entry is deleted in the current transactions view. It is a good\nindicator that there might be excessive garbage in a tree if a range scan\nseems to take too long.\n</code></pre> <p><code>ft flusher: total nodes potentially flushed by cleaner thread</code>:</p> <pre><code>Total number of nodes whose buffers are potentially flushed by cleaner thread.\n</code></pre> <p><code>ft flusher: height-one nodes flushed by cleaner thread</code>:</p> <pre><code>Number of nodes of height one whose message buffers are flushed by cleaner\nthread.\n</code></pre> <p><code>ft flusher: height-greater-than-one nodes flushed by cleaner thread</code>:</p> <pre><code>Number of nodes of height &gt; 1 whose message buffers are flushed by cleaner\nthread.\n</code></pre> <p><code>ft flusher: nodes cleaned which had empty buffers</code>:</p> <pre><code>Number of nodes that are selected by cleaner, but whose buffers are empty.\n</code></pre> <p><code>ft flusher: nodes dirtied by cleaner thread</code>:</p> <pre><code>Number of nodes that are made dirty by the cleaner thread.\n</code></pre> <p><code>ft flusher: max bytes in a buffer flushed by cleaner thread</code>:</p> <pre><code>Max number of bytes in message buffer flushed by cleaner thread.\n</code></pre> <p><code>ft flusher: min bytes in a buffer flushed by cleaner thread</code>:</p> <pre><code>Min number of bytes in message buffer flushed by cleaner thread.\n</code></pre> <p><code>ft flusher: total bytes in buffers flushed by cleaner thread</code>:</p> <pre><code>Total number of bytes in message buffers flushed by cleaner thread.\n</code></pre> <p><code>ft flusher: max workdone in a buffer flushed by cleaner thread</code>:</p> <pre><code>Max workdone value of any message buffer flushed by cleaner thread.\n</code></pre> <p><code>ft flusher: min workdone in a buffer flushed by cleaner thread</code>:</p> <pre><code>Min workdone value of any message buffer flushed by cleaner thread.\n</code></pre> <p><code>ft flusher: total workdone in buffers flushed by cleaner thread</code>:</p> <pre><code>Total workdone value of message buffers flushed by cleaner thread.\n</code></pre> <p><code>ft flusher: times cleaner thread tries to merge a leaf</code>:</p> <pre><code>The number of times the cleaner thread tries to merge a leaf.\n</code></pre> <p><code>ft flusher: cleaner thread leaf merges in progress</code>:</p> <pre><code>The number of cleaner thread leaf merges in progress.\n</code></pre> <p><code>ft flusher: cleaner thread leaf merges successful</code>:</p> <pre><code>The number of times the cleaner thread successfully merges a leaf.\n</code></pre> <p><code>ft flusher: nodes dirtied by cleaner thread leaf merges</code>:</p> <pre><code>The number of nodes dirtied by the \u201cflush from root\u201d process to merge a leaf node.\n</code></pre> <p><code>ft flusher: total number of flushes done by flusher threads or cleaner threads</code>:</p> <pre><code>Total number of flushes done by flusher threads or cleaner threads.\n</code></pre> <p><code>ft flusher: number of in memory flushes</code>:</p> <pre><code>Number of in-memory flushes.\n</code></pre> <p><code>ft flusher: number of flushes that read something off disk</code>:</p> <pre><code>Number of flushes that had to read a child (or part) off disk.\n</code></pre> <p><code>ft flusher: number of flushes that triggered another flush in child</code>:</p> <pre><code>Number of flushes that triggered another flush in the child.\n</code></pre> <p><code>ft flusher: number of flushes that triggered 1 cascading flush</code>:</p> <pre><code>Number of flushes that triggered 1 cascading flush.\n</code></pre> <p><code>ft flusher: number of flushes that triggered 2 cascading flushes</code>:</p> <pre><code>Number of flushes that triggered 2 cascading flushes.\n</code></pre> <p><code>ft flusher: number of flushes that triggered 3 cascading flushes:</code></p> <pre><code>Number of flushes that triggered 3 cascading flushes.\n</code></pre> <p><code>ft flusher: number of flushes that triggered 4 cascading flushes</code>:</p> <pre><code>Number of flushes that triggered 4 cascading flushes.\n</code></pre> <p><code>ft flusher: number of flushes that triggered 5 cascading flushes</code>:</p> <pre><code>Number of flushes that triggered 5 cascading flushes.\n</code></pre> <p><code>ft flusher: number of flushes that triggered over 5 cascading flushes</code>:</p> <pre><code>Number of flushes that triggered more than 5 cascading flushes.\n</code></pre> <p><code>ft flusher: leaf node splits</code>:</p> <pre><code>Number of leaf nodes split.\n</code></pre> <p><code>ft flusher: nonleaf node splits</code>:</p> <pre><code>Number of non-leaf nodes split.\n</code></pre> <p><code>ft flusher: leaf node merges</code>:</p> <pre><code>Number of times leaf nodes are merged.\n</code></pre> <p><code>ft flusher: nonleaf node merges</code>:</p> <pre><code>Number of times non-leaf nodes are merged.\n</code></pre> <p><code>ft flusher: leaf node balances</code>:</p> <pre><code>Number of times a leaf node is balanced.\n</code></pre> <p><code>hot: operations ever started</code>:</p> <pre><code>This variable shows the number of hot operations started (`OPTIMIZE TABLE`).\nThis is a not a useful value for a regular user to use for any purpose.\n</code></pre> <p><code>hot: operations successfully completed</code>:</p> <pre><code>The number of hot operations that have successfully completed (`OPTIMIZE\nTABLE`). This is a not a useful value for a regular user to use for any\npurpose.\n</code></pre> <p><code>hot: operations aborted</code>:</p> <pre><code>The number of hot operations that have been aborted (`OPTIMIZE TABLE`).\nThis is a not a useful value for a regular user to use for any purpose.\n</code></pre> <p><code>hot: max number of flushes from root ever required to optimize a tree</code>:</p> <pre><code>The maximum number of flushes from the root ever required to optimize a tree.\n</code></pre> <p><code>txn: begin</code>:</p> <pre><code>This is the number of transactions that have been started.\n</code></pre> <p><code>txn: begin read only</code>:</p> <pre><code>Number of read only transactions started.\n</code></pre> <p><code>txn: successful commits</code>:</p> <pre><code>This is the total number of transactions that have been committed.\n</code></pre> <p><code>txn: aborts</code>:</p> <pre><code>This is the total number of transactions that have been aborted.\n</code></pre> <p><code>logger: next LSN</code>:</p> <pre><code>This is the next unassigned Log Sequence Number. It will be assigned to the\nnext entry in the recovery log.\n</code></pre> <p><code>logger: writes</code>:</p> <pre><code>Number of times the logger has written to disk.\n</code></pre> <p><code>logger: writes (bytes)</code>:</p> <pre><code>Number of bytes the logger has written to disk.\n</code></pre> <p><code>logger: writes (uncompressed bytes)</code>:</p> <pre><code>Number of uncompressed the logger has written to disk.\n</code></pre> <p><code>logger: writes (seconds)</code>:</p> <pre><code>Number of seconds waiting for I/O when writing logs to disk.\n</code></pre> <p><code>logger: number of long logger write operations</code>:</p> <pre><code>Number of times a logger write operation required 100ms or more.\n</code></pre> <p><code>indexer: number of indexers successfully created</code>:</p> <pre><code>This is the number of times one of our internal objects, a indexer, has been\ncreated.\n</code></pre> <p><code>indexer: number of calls to toku_indexer_create_indexer() that failed</code>:</p> <pre><code>This is the number of times a indexer was requested but could not be created.\n</code></pre> <p><code>indexer: number of calls to indexer-&gt;build() succeeded</code>:</p> <pre><code>This is the total number of times that indexes were created using a indexer.\n</code></pre> <p><code>indexer: number of calls to indexer-&gt;build() failed</code>:</p> <pre><code>This is the total number of times that indexes were unable to be created using a indexer\n</code></pre> <p><code>indexer: number of calls to indexer-&gt;close() that succeeded</code>:</p> <pre><code>This is the number of indexers that successfully created the requested index(es).\n</code></pre> <p><code>indexer: number of calls to indexer-&gt;close() that failed</code>:</p> <pre><code>This is the number of indexers that were unable to create the requested index(es).\n</code></pre> <p><code>indexer: number of calls to indexer-&gt;abort()</code>:</p> <pre><code>This is the number of indexers that were aborted.\n</code></pre> <p><code>indexer: number of indexers currently in existence</code>:</p> <pre><code>This is the number of indexers that currently exist.\n</code></pre> <p><code>indexer: max number of indexers that ever existed simultaneously</code>:</p> <pre><code>This is the maximum number of indexers that ever existed simultaneously.\n</code></pre> <p><code>loader: number of loaders successfully created</code>:</p> <pre><code>This is the number of times one of our internal objects, a loader, has been\ncreated.\n</code></pre> <p><code>loader: number of calls to toku_loader_create_loader() that failed</code>:</p> <pre><code>This is the number of times a loader was requested but could not be created.\n</code></pre> <p><code>loader: number of calls to loader-&gt;put() succeeded</code>:</p> <pre><code>This is the total number of rows that were inserted using a loader.\n</code></pre> <p><code>loader: number of calls to loader-&gt;put() failed</code>:</p> <pre><code>This is the total number of rows that were unable to be inserted using a\nloader.\n</code></pre> <p><code>loader: number of calls to loader-&gt;close() that succeeded</code>:</p> <pre><code>This is the number of loaders that successfully created the requested table.\n</code></pre> <p><code>loader: number of calls to loader-&gt;close() that failed</code>:</p> <pre><code>This is the number of loaders that were unable to create the requested table.\n</code></pre> <p><code>loader: number of calls to loader-&gt;abort()</code>:</p> <pre><code>This is the number of loaders that were aborted.\n</code></pre> <p><code>loader: number of loaders currently in existence</code>:</p> <pre><code>This is the number of loaders that currently exist.\n</code></pre> <p><code>loader: max number of loaders that ever existed simultaneously</code>:</p> <pre><code>This is the maximum number of loaders that ever existed simultaneously.\n</code></pre> <p><code>memory: number of malloc operations</code>:</p> <pre><code>Number of calls to `malloc()`.\n</code></pre> <p><code>memory: number of free operations</code>:</p> <pre><code>Number of calls to `free()`.\n</code></pre> <p><code>memory: number of realloc operations</code>:</p> <pre><code>Number of calls to `realloc()`.\n</code></pre> <p><code>memory: number of malloc operations that failed</code>:</p> <pre><code>Number of failed calls to `malloc()`.\n</code></pre> <p><code>memory: number of realloc operations that failed</code>:</p> <pre><code>Number of failed calls to `realloc()`.\n</code></pre> <p><code>memory: number of bytes requested</code>:</p> <pre><code>Total number of bytes requested from memory allocator library.\n</code></pre> <p><code>memory: number of bytes freed</code>:</p> <pre><code>Total number of bytes allocated from memory allocation library that have been\nfreed (used - freed = bytes in use).\n</code></pre> <p><code>memory: largest attempted allocation size</code>:</p> <pre><code>Largest number of bytes in a single successful `malloc()` operation.\n</code></pre> <p><code>memory: size of the last failed allocation attempt</code>:</p> <pre><code>Largest number of bytes in a single failed `malloc()` operation.\n</code></pre> <p><code>memory: number of bytes used (requested + overhead)</code>:</p> <pre><code>Total number of bytes allocated by memory allocator library.\n</code></pre> <p><code>memory: estimated maximum memory footprint</code>:</p> <pre><code>Maximum memory footprint of the storage engine,\nthe max value of (used - freed).\n</code></pre> <p><code>memory: mallocator version</code>:</p> <pre><code>Version string from in-use memory allocator.\n</code></pre> <p><code>memory: mmap threshold</code>:</p> <pre><code>The threshold for malloc to use mmap.\n</code></pre> <p><code>filesystem: ENOSPC redzone state</code>:</p> <pre><code>The state of how much disk space exists with respect to the red zone value.\nRedzone is space greater than tokudb_fs_reserve_percent and less\nthan full disk.\n\nValid values are:\n\n\n* **0**\n\n    Space is available\n\n\n\n* **1**\n\n    Warning, with 2x of redzone value. Operations are allowed, but engine\n    status prints a warning.\n\n\n\n* **2**\n\n    In red zone, insert operations are blocked\n\n\n\n* **3**\n\n    All operations are blocked\n</code></pre> <p><code>filesystem: threads currently blocked by full disk</code>:</p> <pre><code>This is the number of threads that are currently blocked because they are\nattempting to write to a full disk. This is normally zero. If this value is\nnon-zero, then a warning will appear in the \u201cdisk free space\u201d field.\n</code></pre> <p><code>filesystem: number of operations rejected by enospc prevention (red zone)</code>:</p> <pre><code>This is the number of database inserts that have been rejected because the\namount of disk free space was less than the reserve.\n</code></pre> <p><code>filesystem: most recent disk full</code>:</p> <pre><code>This is the most recent time when the disk file system was entirely full. If\nthe disk has never been full, then this value will be `Dec 31, 1969` on\nLinux hosts.\n</code></pre> <p><code>filesystem: number of write operations that returned ENOSPC</code>:</p> <pre><code>This is the number of times that an attempt to write to disk failed because\nthe disk was full. If the disk is full, this number will continue increasing\nuntil space is available.\n</code></pre> <p><code>filesystem: fsync time</code>:</p> <pre><code>This the total time, in microseconds, used to fsync to disk.\n</code></pre> <p><code>filesystem: fsync count</code>:</p> <pre><code>This is the total number of times the database has flushed the operating\nsystem\u2019s file buffers to disk.\n</code></pre> <p><code>filesystem: long fsync time</code>:</p> <pre><code>This the total time, in microseconds, used to fsync to disk when the operation\nrequired more than 1 second.\n</code></pre> <p><code>filesystem: long fsync count</code>:</p> <pre><code>This is the total number of times the database has flushed the operating\nsystem\u2019s file buffers to disk and this operation required more than 1 second.\n</code></pre> <p><code>context: tree traversals blocked by a full fetch</code>:</p> <pre><code>Number of times node `rwlock` contention was observed while pinning nodes\nfrom root to leaf because of a full fetch.\n</code></pre> <p><code>context: tree traversals blocked by a partial fetch</code>:</p> <pre><code>Number of times node `rwlock` contention was observed while pinning nodes\nfrom root to leaf because of a partial fetch.\n</code></pre> <p><code>context: tree traversals blocked by a full eviction</code></p> <pre><code>Number of times node `rwlock` contention was observed while pinning nodes\nfrom root to leaf because of a full eviction.\n</code></pre> <p><code>context: tree traversals blocked by a partial eviction</code></p> <pre><code>Number of times node `rwlock` contention was observed while pinning nodes\nfrom root to leaf because of a partial eviction.\n</code></pre> <p><code>context: tree traversals blocked by a message injection</code>:</p> <pre><code>Number of times node `rwlock` contention was observed while pinning nodes\nfrom root to leaf because of message injection.\n</code></pre> <p><code>context: tree traversals blocked by a message application</code></p> <pre><code>Number of times node `rwlock` contention was observed while pinning nodes\nfrom root to leaf because of message application (applying fresh ancestors\nmessages to a basement node).\n</code></pre> <p><code>context: tree traversals blocked by a flush</code>:</p> <pre><code>Number of times node `rwlock` contention was observed while pinning nodes\nfrom root to leaf because of a buffer flush from parent to child.\n</code></pre> <p><code>context: tree traversals blocked by a the cleaner thread</code>:</p> <pre><code>Number of times node `rwlock` contention was observed while pinning nodes\nfrom root to leaf because of a cleaner thread.\n</code></pre> <p><code>context: tree traversals blocked by something uninstrumented</code>:</p> <pre><code>Number of times node `rwlock` contention was observed while pinning nodes\nfrom root to leaf because of something uninstrumented.\n</code></pre> <p><code>context: promotion blocked by a full fetch (should never happen)</code>:</p> <pre><code>Number of times node `rwlock` contention was observed within promotion\n(pinning nodes from root to the buffer to receive the message) because of a\nfull fetch.\n</code></pre> <p><code>context: promotion blocked by a partial fetch (should never happen)</code>:</p> <pre><code>Number of times node `rwlock` contention was observed within promotion\n(pinning nodes from root to the buffer to receive the message) because of a\npartial fetch.\n</code></pre> <p><code>context: promotion blocked by a full eviction (should never happen)</code>:</p> <pre><code>Number of times node `rwlock` contention was observed within promotion\n(pinning nodes from root to the buffer to receive the message) because of a\nfull eviction.\n</code></pre> <p><code>context: promotion blocked by a partial eviction (should never happen)</code>:</p> <pre><code>Number of times node `rwlock` contention was observed within promotion\n(pinning nodes from root to the buffer to receive the message) because of a\npartial eviction.\n</code></pre> <p><code>context: promotion blocked by a message injection</code>:</p> <pre><code>Number of times node `rwlock` contention was observed within promotion\n(pinning nodes from root to the buffer to receive the message) because of\nmessage injection.\n</code></pre> <p><code>context: promotion blocked by a message application</code>:</p> <pre><code>Number of times node `rwlock` contention was observed within promotion\n(pinning nodes from root to the buffer to receive the message) because of\nmessage application (applying fresh ancestors messages to a basement node).\n</code></pre> <p><code>context: promotion blocked by a flush</code>:</p> <pre><code>Number of times node `rwlock` contention was observed within promotion\n(pinning nodes from root to the buffer to receive the message) because of a\nbuffer flush from parent to child.\n</code></pre> <p><code>context: promotion blocked by the cleaner thread</code>:</p> <pre><code>Number of times node `rwlock` contention was observed within promotion\n(pinning nodes from root to the buffer to receive the message) because of a\ncleaner thread.\n</code></pre> <p><code>context: promotion blocked by something uninstrumented</code>:</p> <pre><code>Number of times node `rwlock` contention was observed within promotion\n(pinning nodes from root to the buffer to receive the message) because of\nsomething uninstrumented.\n</code></pre> <p><code>context: something uninstrumented blocked by something uninstrumented</code>:</p> <pre><code>Number of times node `rwlock` contention was observed for an uninstrumented\nprocess because of something uninstrumented.\n</code></pre> <p><code>handlerton: primary key bytes inserted</code>:</p> <pre><code>Total number of bytes inserted into all primary key indexes.\n</code></pre>"},{"location":"tokudb/tokudb_variables.html","title":"TokuDB Variables","text":"<p>Like all storage engines, TokuDB has variables to tune performance and control behavior. Fractal Tree algorithms are designed for near optimal performance and TokuDB\u2019s default settings should work well in most situations, eliminating the need for complex and time consuming tuning in most cases.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb-server-variables","title":"TokuDB Server Variables","text":"Name Cmd-Line Option File Var Scope Dynamic tokudb_alter_print_error Yes Yes Session, Global Yes tokudb_analyze_delete_fraction Yes Yes Session, Global Yes tokudb_analyze_in_background Yes Yes Session, Global Yes tokudb_analyze_mode Yes Yes Session, Global Yes tokudb_analyze_throttle Yes Yes Session, Global Yes tokudb_analyze_time Yes Yes Session, Global Yes tokudb_auto_analyze Yes Yes Session, Global Yes tokudb_backup_allowed_prefix No Yes Global No tokudb_backup_dir No Yes Session No tokudb_backup_exclude Yes Yes Session, Global Yes tokudb_backup_last_error Yes Yes Session, Global Yes tokudb_backup_last_error_string Yes Yes Session, Global Yes tokudb_backup_plugin_version No No Global No tokudb_backup_throttle Yes Yes Session, Global Yes tokudb_backup_version No No Global No tokudb_block_size Yes Yes Session, Global Yes tokudb_bulk_fetch Yes Yes Session, Global Yes tokudb_cachetable_pool_threads Yes Yes Global No tokudb_cardinality_scale_percent Yes Yes Global Yes tokudb_check_jemalloc Yes Yes Global No tokudb_checkpoint_lock Yes Yes Global No tokudb_checkpoint_on_flush_logs Yes Yes Global Yes tokudb_checkpoint_pool_threads Yes Yes Global Yes tokudb_checkpointing_period Yes Yes Global Yes tokudb_cleaner_iterations Yes Yes Global Yes tokudb_cleaner_period Yes Yes Global Yes tokudb_client_pool_threads Yes Yes Global No tokudb_commit_sync Yes Yes Session, Global Yes tokudb_compress_buffers_before_eviction Yes Yes Global No tokudb_create_index_online Yes Yes Session, Global Yes tokudb_data_dir Yes Yes Global No tokudb_debug Yes Yes Global Yes tokudb_dir_per_db Yes Yes Global Yes tokudb_directio Yes Yes Global No tokudb_disable_hot_alter Yes Yes Session, Global Yes tokudb_disable_prefetching Yes Yes Session, Global Yes tokudb_disable_slow_alter Yes Yes Session, Global Yes tokudb_empty_scan Yes Yes Session, Global Yes tokudb_enable_fast_update Yes Yes Session, Global Yes tokudb_enable_fast_upsert Yes Yes Session, Global Yes tokudb_enable_partial_eviction Yes Yes Global No tokudb_fanout Yes Yes Session, Global Yes tokudb_fs_reserve_percent Yes Yes Global No tokudb_fsync_log_period Yes Yes Global Yes tokudb_hide_default_row_format Yes Yes Session, Global Yes tokudb_killed_time Yes Yes Session, Global Yes tokudb_last_lock_timeout Yes Yes Session, Global Yes tokudb_load_save_space Yes Yes Session, Global Yes tokudb_loader_memory_size Yes Yes Session, Global Yes tokudb_lock_timeout Yes Yes Session, Global Yes tokudb_lock_timeout_debug Yes Yes Session, Global Yes tokudb_log_dir Yes Yes Global No tokudb_max_lock_memory Yes Yes Global No tokudb_optimize_index_fraction Yes Yes Session, Global Yes tokudb_optimize_index_name Yes Yes Session, Global Yes tokudb_optimize_throttle Yes Yes Session, Global Yes tokudb_pk_insert_mode Yes Yes Session, Global Yes tokudb_prelock_empty Yes Yes Session, Global Yes tokudb_read_block_size Yes Yes Session, Global Yes tokudb_read_buf_size Yes Yes Session, Global Yes tokudb_read_status_frequency Yes Yes Global Yes tokudb_row_format Yes Yes Session, Global Yes tokudb_rpl_check_readonly Yes Yes Session, Global Yes tokudb_rpl_lookup_rows Yes Yes Session, Global Yes tokudb_rpl_lookup_rows_delay Yes Yes Session, Global Yes tokudb_rpl_unique_checks Yes Yes Session, Global Yes tokudb_rpl_unique_checks_delay Yes Yes Session, Global Yes tokudb_strip_frm_data Yes Yes Global No tokudb_support_xa Yes Yes Session, Global Yes tokudb_tmp_dir Yes Yes Global No tokudb_version No No Global No tokudb_write_status_frequency Yes Yes Global Yes"},{"location":"tokudb/tokudb_variables.html#tokudb_alter_print_error","title":"<code>tokudb_alter_print_error</code>","text":"Option Description Command-line Yes Config file Yes Scope Global/Session Dynamic Yes Data type Boolean Default OFF <p>When set to <code>ON</code> errors will be printed to the client during the <code>ALTER TABLE</code> operations on TokuDB tables.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_analyze_delete_fraction","title":"<code>tokudb_analyze_delete_fraction</code>","text":"Option Description Command-line Yes Config file Yes Scope Global/Session Dynamic Yes Data type Numeric Default 1.000000 Range 0.0 - 1.000000 <p>This variables controls whether or not deleted rows in the fractal tree are reported to the client and to the MySQL error log during an <code>ANALYZE TABLE</code> operation on a TokuDB table. When set to <code>1</code>, nothing is reported. When set to <code>0.1</code> and at least 10% of the rows scanned by <code>ANALYZE</code> were deleted rows that are not yet garbage collected, a report is returned to the client and the MySQL error log.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_backup_allowed_prefix","title":"<code>tokudb_backup_allowed_prefix</code>","text":"Option Description Command-line No Config file Yes Scope Global Dynamic No Data type String Default NULL <p>This system-level variable restricts the location of the destination directory where the backups can be located. Attempts to backup to a location outside of the directory this variable points to or its children will result in an error.</p> <p>The default is NULL, backups have no restricted locations. This read only variable can be set in the <code>my.cnf</code> configuration file and displayed with the <code>SHOW VARIABLES</code> command when Percona TokuBackup plugin is loaded.</p> <pre><code>mysql&gt; SHOW VARIABLES LIKE 'tokudb_backup_allowed_prefix';\n</code></pre> <p>The output could be:</p> <pre><code>+------------------------------+-----------+\n| Variable_name                | Value     |\n+------------------------------+-----------+\n| tokudb_backup_allowed_prefix | /dumpdir  |\n+------------------------------+-----------+\n</code></pre>"},{"location":"tokudb/tokudb_variables.html#tokudb_backup_dir","title":"<code>tokudb_backup_dir</code>","text":"Option Description Command-line No Config file No Scope Session Dynamic Yes Data type String Default NULL <p>When enabled, this session level variable serves two purposes, to point to the destination directory where the backups will be dumped and to kick off the backup as soon as it is set. For more information see Percona TokuBackup.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_backup_exclude","title":"<code>tokudb_backup_exclude</code>","text":"Option Description Command-line No Config file No Scope Session Dynamic Yes Data type String Default (mysqld_safe.pid)+ <p>Use this variable to set a regular expression that defines source files excluded from backup. For example, to exclude all <code>lost+found</code> directories, use the following command:</p> <pre><code>mysqlset tokudb_backup_exclude='/lost\\\\+found(#|/)';\n</code></pre> <p>For more information see Percona TokuBackup.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_backup_last_error","title":"<code>tokudb_backup_last_error</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 0 <p>This session variable will contain the error number from the last backup. <code>0</code> indicates success. For more information see Percona TokuBackup.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_backup_last_error_string","title":"<code>tokudb_backup_last_error_string</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type String Default NULL <p>This session variable will contain the error string from the last backup. For more information see Percona TokuBackup.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_backup_plugin_version","title":"<code>tokudb_backup_plugin_version</code>","text":"Option Description Command-line No Config file No Scope Global Dynamic No Data type String <p>This read-only server variable documents the version of the TokuBackup plugin. For more information see Percona TokuBackup.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_backup_throttle","title":"<code>tokudb_backup_throttle</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 18446744073709551615 <p>This variable specifies the maximum number of bytes per second the copier of a hot backup process will consume. Lowering its value will cause the hot backup operation to take more time but consume less I/O on the server. The default value is <code>18446744073709551615</code> which means no throttling. For more information see Percona TokuBackup.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_backup_version","title":"<code>tokudb_backup_version</code>","text":"Option Description Command-line No Config file No Scope Global Dynamic No Data type String <p>This read-only server variable documents the version of the hot backup library. For more information see Percona TokuBackup.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_block_size","title":"<code>tokudb_block_size</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 4194304 Range 4096 - 4294967295 <p>This variable controls the maximum size of node in memory before messages must be flushed or node must be split.</p> <p>Changing the value of tokudb_block_size only affects subsequently created tables and indexes. The value of this variable cannot be changed for an existing table/index without a dump and reload.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_bulk_fetch","title":"<code>tokudb_bulk_fetch</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default ON <p>This variable determines if our bulk fetch algorithm is used for <code>SELECT</code> statements. <code>SELECT</code> statements include pure <code>SELECT ...</code> statements, as well as <code>INSERT INTO table-name ... SELECT ...</code>, <code>CREATE TABLE table-name ... SELECT ...</code>, <code>REPLACE INTO table-name ... SELECT ...</code>, <code>INSERT IGNORE INTO table-name ... SELECT ...</code>, and <code>INSERT INTO table-name ... SELECT ... ON DUPLICATE KEY UPDATE</code>.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_cache_size","title":"<code>tokudb_cache_size</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type Numeric <p>This variable configures the size in bytes of the TokuDB cache table. The default cache table size is \u00bd of physical memory. Percona highly recommends using the default setting if using buffered I/O, if using direct I/O then consider setting this parameter to 80% of available memory.</p> <p>Consider decreasing tokudb_cache_size if excessive swapping is causing performance problems. Swapping may occur when running multiple MySQL server instances or if other running applications use large amounts of physical memory.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_cachetable_pool_threads","title":"<code>tokudb_cachetable_pool_threads</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 0 Range 0 - 1024 <p>This variable defines the number of threads for the cachetable worker thread pool. This pool is used to perform node prefetches, and to serialize, compress, and write nodes during cachetable eviction. The default value of 0 calculates the pool size to be num_cpu_threads * 2.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_check_jemalloc","title":"<code>tokudb_check_jemalloc</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type Boolean Default ON <p>This variable enables/disables startup checking if jemalloc is linked and correct version and that transparent huge pages are disabled. Used for testing only.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_checkpoint_lock","title":"<code>tokudb_checkpoint_lock</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default OFF <p>Disables checkpointing when true. Session variable but acts like a global, any session disabling checkpointing disables it globally. If a session sets this lock and disconnects or terminates for any reason, the lock will not be released. Special purpose only, do not use this in your application.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_checkpoint_on_flush_logs","title":"<code>tokudb_checkpoint_on_flush_logs</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Boolean Default OFF <p>When enabled forces a checkpoint if we get a flush logs command from the server.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_checkpoint_pool_threads","title":"<code>tokudb_checkpoint_pool_threads</code>","text":"Option Description Command-line Yes Config file Yes Scope Dynamic No Data type Numeric Default 0 Range 0 - 1024 <p>This defines the number of threads for the checkpoint worker thread pool. This pool is used to serialize, compress and write nodes cloned during checkpoint. Default of <code>0</code> uses old algorithm to set pool size to <code>num_cpu_threads/4</code>.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_checkpointing_period","title":"<code>tokudb_checkpointing_period</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 60 Range 0 - 4294967295 <p>This variable specifies the time in seconds between the beginning of one checkpoint and the beginning of the next. The default time between TokuDB checkpoints is 60 seconds. We recommend leaving this variable unchanged.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_cleaner_iterations","title":"<code>tokudb_cleaner_iterations</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 5 Range 0 - 18446744073709551615 <p>This variable specifies how many internal nodes get processed in each tokudb_cleaner_period period. The default value is <code>5</code>. Setting this variable to <code>0</code> turns off cleaner threads.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_cleaner_period","title":"<code>tokudb_cleaner_period</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 1 Range 0 - 18446744073709551615 <p>This variable specifies how often in seconds the cleaner thread runs. The default value is <code>1</code>. Setting this variable to <code>0</code> turns off cleaner threads.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_client_pool_threads","title":"<code>tokudb_client_pool_threads</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type Numeric Default 0 Range 0 - 1024 <p>This variable defines the number of threads for the client operations thread pool. This pool is used to perform node maintenance on over/undersized nodes such as message flushing down the tree, node splits, and node merges. Default of <code>0</code> uses old algorithm to set pool size to <code>1 \\* num_cpu_threads</code>.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_commit_sync","title":"<code>tokudb_commit_sync</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default ON <p>Session variable tokudb_commit_sync controls whether or not the transaction log is flushed when a transaction commits. The default behavior is that the transaction log is flushed by the commit. Flushing the transaction log requires a disk write and may adversely affect the performance of your application.</p> <p>To disable synchronous flushing of the transaction log, disable the tokudb_commit_sync session variable as follows:</p> <pre><code>SET tokudb_commit_sync=OFF;\n</code></pre> <p>Disabling this variable may make the system run faster. However, transactions committed since the last checkpoint are not guaranteed to survive a crash.</p> <p>Warning</p> <p>By disabling this variable and/or setting the tokudb_fsync_log_period to non-zero value you have effectively downgraded the durability of the storage engine. If you were to have a crash in this same window, you would lose data. The same issue would also appear if you were using some kind of volume snapshot for backups.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_compress_buffers_before_eviction","title":"<code>tokudb_compress_buffers_before_eviction</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type Boolean Default ON <p>When this variable is enabled it allows the evictor to compress unused internal node partitions in order to reduce memory requirements as a first step of partial eviction before fully evicting the partition and eventually the entire node.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_create_index_online","title":"<code>tokudb_create_index_online</code>","text":"<p>This variable controls whether indexes created with the <code>CREATE INDEX</code> command are hot (if enabled), or offline (if disabled). Hot index creation means that the table is available for inserts and queries while the index is being created. Offline index creation means that the table is not available for inserts and queries while the index is being created.</p> <p>Note</p> <p>Hot index creation is slower than offline index creation.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_data_dir","title":"<code>tokudb_data_dir</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type String Default NULL <p>This variable configures the directory name where the TokuDB tables are stored. The default value is <code>NULL</code> which uses the location of the MySQL data directory. For more information check TokuDB files and file types and TokuDB file management.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_debug","title":"<code>tokudb_debug</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 0 Range 0 - 18446744073709551615 <p>This variable enables mysqld debug printing to <code>STDERR</code> for TokuDB. Produces tremendous amounts of output that is nearly useless to anyone but a TokuDB developer, not recommended for any production use at all. It is a mask value <code>ULONG</code>:</p> <pre><code>#define TOKUDB_DEBUG_INIT                   (1&lt;&lt;0)\n#define TOKUDB_DEBUG_OPEN                   (1&lt;&lt;1)\n#define TOKUDB_DEBUG_ENTER                  (1&lt;&lt;2)\n#define TOKUDB_DEBUG_RETURN                 (1&lt;&lt;3)\n#define TOKUDB_DEBUG_ERROR                  (1&lt;&lt;4)\n#define TOKUDB_DEBUG_TXN                    (1&lt;&lt;5)\n#define TOKUDB_DEBUG_AUTO_INCREMENT         (1&lt;&lt;6)\n#define TOKUDB_DEBUG_INDEX_KEY              (1&lt;&lt;7)\n#define TOKUDB_DEBUG_LOCK                   (1&lt;&lt;8)\n#define TOKUDB_DEBUG_CHECK_KEY              (1&lt;&lt;9)\n#define TOKUDB_DEBUG_HIDE_DDL_LOCK_ERRORS   (1&lt;&lt;10)\n#define TOKUDB_DEBUG_ALTER_TABLE            (1&lt;&lt;11)\n#define TOKUDB_DEBUG_UPSERT                 (1&lt;&lt;12)\n#define TOKUDB_DEBUG_CHECK                  (1&lt;&lt;13)\n#define TOKUDB_DEBUG_ANALYZE                (1&lt;&lt;14)\n#define TOKUDB_DEBUG_XA                     (1&lt;&lt;15)\n#define TOKUDB_DEBUG_SHARE                  (1&lt;&lt;16)\n</code></pre>"},{"location":"tokudb/tokudb_variables.html#tokudb_dir_per_db","title":"<code>tokudb_dir_per_db</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Boolean Default ON <p>The variable has been implemented in Percona Server for MySQL 5.7.15-9. When this variable is set to <code>ON</code> all new tables and indices will be placed within their corresponding database directory within the tokudb_data_dir or system source/glossary.rst`datadir`. Existing table files will not be automatically relocated to their corresponding database directory. If you rename a table, while this variable is enabled, the mapping in the Percona FT directory file will be updated and the files will be renamed on disk to reflect the new table name. For more information check TokuDB files and file types and TokuDB file management.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_directio","title":"<code>tokudb_directio</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type Boolean Default OFF <p>When enabled, TokuDB employs Direct I/O rather than Buffered I/O for writes. When using Direct I/O, consider increasing tokudb_cache_size from its default of \u00bd physical memory.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_disable_hot_alter","title":"<code>tokudb_disable_hot_alter</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default OFF <p>This variable is used specifically for testing or to disable hot alter in case there are bugs. Not for use in production.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_disable_prefetching","title":"<code>tokudb_disable_prefetching</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default OFF <p>TokuDB attempts to aggressively prefetch additional blocks of rows, which is helpful for most range queries but may create unnecessary I/O for range queries with <code>LIMIT</code> clauses. Prefetching is <code>ON</code> by default, with a value of <code>0</code>, it can be disabled by setting this variable to <code>1</code>.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_disable_slow_alter","title":"<code>tokudb_disable_slow_alter</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default OFF <p>This variable is used specifically for testing or to disable hot alter in case there are bugs. Not for use in production. It controls whether slow alter tables are allowed. For example, the following command is slow because <code>HCADER</code> does not allow a mixture of column additions, deletions, or expansions:</p> <pre><code>ALTER TABLE table\nADD COLUMN column_a INT,\nDROP COLUMN column_b;\n</code></pre> <p>By default, tokudb_disable_slow_alter is disabled, and the engine reports back to MySQL that this is unsupported resulting in the following output:</p> <pre><code>ERROR 1112 (42000): Table 'test_slow' uses an extension that doesn't exist in this MySQL version\n</code></pre>"},{"location":"tokudb/tokudb_variables.html#tokudb_empty_scan","title":"<code>tokudb_empty_scan</code>","text":"<p>Defines direction to be used to perform table scan to check for empty tables for bulk loader.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_enable_fast_update","title":"<code>tokudb_enable_fast_update</code>","text":"Option Description Command-line Yes Config file Yes Scope Global/Session Dynamic Yes Data type Boolean Default OFF <p>Toggles the fast updates feature ON/OFF for the <code>UPDATE</code> statement. Fast update involves queries optimization to avoid random reads during their execution.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_enable_fast_upsert","title":"<code>tokudb_enable_fast_upsert</code>","text":"Option Description Command-line Yes Config file Yes Scope Global/Session Dynamic Yes Data type Boolean Default OFF <p>Toggles the fast updates feature ON/OFF for the <code>INSERT</code> statement. Fast update involves queries optimization to avoid random reads during their execution.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_enable_partial_eviction","title":"<code>tokudb_enable_partial_eviction</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type Boolean Default ON <p>This variable is used to control if partial eviction of nodes is enabled or disabled.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_fanout","title":"<code>tokudb_fanout</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 16 Range 2-16384 <p>This variable controls the Fractal Tree fanout. The fanout defines the number of pivate keys or child nodes for each internal tree node. Changing the value of tokudb_fanout only affects subsequently created tables and indexes. The value of this variable cannot be changed for an existing table/index without a dump and reload.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_fs_reserve_percent","title":"<code>tokudb_fs_reserve_percent</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type Numeric Default 5 Range 0-100 <p>This variable controls the percentage of the file system that must be available for inserts to be allowed. By default, this is set to <code>5</code>. We recommend that this reserve be at least half the size of your physical memory. See Full Disks for more information.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_fsync_log_period","title":"<code>tokudb_fsync_log_period</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 0 Range 0-4294967295 <p>This variable controls the frequency, in milliseconds, for <code>fsync()</code> operations. If set to <code>0</code> then the <code>fsync()</code> behavior is only controlled by the tokudb_commit_sync, which can be <code>ON</code> or <code>OFF</code>.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_hide_default_row_format","title":"<code>tokudb_hide_default_row_format</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default ON <p>This variable is used to hide the <code>ROW_FORMAT</code> in <code>SHOW CREATE TABLE</code>. If <code>zlib</code> compression is used, row format will show as <code>DEFAULT</code>.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_killed_time","title":"<code>tokudb_killed_time</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 4000 Range 0-18446744073709551615 <p>This variable is used to specify frequency in milliseconds for lock wait to check to see if the lock was killed.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_last_lock_timeout","title":"<code>tokudb_last_lock_timeout</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type String Default NULL <p>This variable contains a JSON document that describes the last lock conflict seen by the current MySQL client. It gets set when a blocked lock request times out or a lock deadlock is detected.</p> <p>The tokudb_lock_timeout_debug session variable must have bit <code>0</code> set for this behavior, otherwise this session variable will be empty.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_load_save_space","title":"<code>tokudb_load_save_space</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default ON <p>This session variable changes the behavior of the bulk loader. When it is disabled the bulk loader stores intermediate data using uncompressed files (which consumes additional CPU), whereas <code>ON</code> compresses the intermediate files.</p> <p>Note</p> <p>The location of the temporary disk space used by the bulk loader may be specified with the tokudb_tmp_dir server variable.</p> <p>If a <code>LOAD DATA INFILE</code> statement fails with the error message <code>ERROR 1030 (HY000): Got error 1</code> from storage engine, then there may not be enough disk space for the optimized loader, so disable tokudb_prelock_empty and try again. More information is available in Known Issues.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_loader_memory_size","title":"<code>tokudb_loader_memory_size</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 100000000 Range 0-18446744073709551615 <p>This variable limits the amount of memory (in bytes) that the TokuDB bulk loader will use for each loader instance. Increasing this value may provide a performance benefit when loading extremely large tables with several secondary indexes.</p> <p>Note</p> <p>Memory allocated to a loader is taken from the TokuDB cache, defined in tokudb_cache_size, and may impact the running workload\u2019s performance as existing cached data must be ejected for the loader to begin.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_lock_timeout","title":"<code>tokudb_lock_timeout</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 4000 Range 0-18446744073709551615 <p>This variable controls the amount of time that a transaction will wait for a lock held by another transaction to be released. If the conflicting transaction does not release the lock within the lock timeout, the transaction that was waiting for the lock will get a lock timeout error. The units are milliseconds. A value of <code>0</code> disables lock waits. The default value is 4000 (four seconds).</p> <p>If your application gets a <code>lock wait timeout</code> error (-30994), then you may find that increasing the tokudb_lock_timeout may help. If your application gets a <code>deadlock found</code> error (-30995), then you need to abort the current transaction and retry it.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_lock_timeout_debug","title":"<code>tokudb_lock_timeout_debug</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 1 Range 0-3 <p>The following values are available:</p> <ul> <li> <p><code>0</code>: No lock timeouts or lock deadlocks are reported.</p> </li> <li> <p><code>1</code>: A JSON document that describes the lock conflict is stored in the tokudb_last_lock_timeout session variable</p> </li> <li> <p><code>2</code>: A JSON document that describes the lock conflict is printed to the MySQL error log.</p> </li> </ul> <p>In addition to the JSON document describing the lock conflict, the following lines are printed to the MySQL error log:</p> <pre><code>* A line containing the blocked thread id and blocked SQL\n\n* A line containing the blocking thread id and the blocking SQL.\n\n* `3`: A JSON document that describes the lock conflict is stored in the\n</code></pre> <p>tokudb_last_lock_timeout session variable and is printed to the MySQL error log.</p> <p>In addition to the JSON document describing the lock conflict, the following lines are printed to the MySQL error log:</p> <pre><code>* A line containing the blocked thread id and blocked SQL\n\n\n* A line containing the blocking thread id and the blocking SQL.\n</code></pre>"},{"location":"tokudb/tokudb_variables.html#tokudb_log_dir","title":"<code>tokudb_log_dir</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type String Default NULL <p>This variable specifies the directory where the TokuDB log files are stored. The default value is <code>NULL</code> which uses the location of the MySQL data directory. Configuring a separate log directory is somewhat involved. Please contact Percona support for more details. For more information check TokuDB files and file types and TokuDB file management.</p> <p>Warning</p> <p>After changing TokuDB log directory path, the old TokuDB recovery log file should be moved to new directory prior to start of MySQL server and log file\u2019s owner must be the <code>mysql</code> user. Otherwise server will fail to initialize the TokuDB store engine restart.</p> <p><code>tokudb_max_lock_memory</code></p> Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type Numeric Default 65560320 Range 0-18446744073709551615 <p>This variable specifies the maximum amount of memory for the PerconaFT lock table.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_optimize_index_fraction","title":"<code>tokudb_optimize_index_fraction</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 1.000000 Range 0.000000 - 1.000000 <p>For patterns where the left side of the tree has many deletions (a common pattern with increasing id or date values), it may be useful to delete a percentage of the tree. In this case, it\u2019s possible to optimize a subset of a fractal tree starting at the left side. The tokudb_optimize_index_fraction session variable controls the size of the sub tree. Valid values are in the range [0.0,1.0] with default 1.0 (optimize the whole tree).</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_optimize_index_name","title":"<code>tokudb_optimize_index_name</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type String Default NULL <p>This variable can be used to optimize a single index in a table, it can be set to select the index by name.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_optimize_throttle","title":"<code>tokudb_optimize_throttle</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 0 Range 0-18446744073709551615 <p>By default, table optimization will run with all available resources. To limit the amount of resources, it is possible to limit the speed of table optimization. This determines an upper bound on how many fractal tree leaf nodes per second are optimized. The default <code>0</code> imposes no limit.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_pk_insert_mode","title":"<code>tokudb_pk_insert_mode</code>","text":"<p>Removed in Percona Server for MySQL 5.7.12-5.</p> Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 1 Range 0-3 <p>Note</p> <p>The tokudb_pk_insert_mode session variable has been removed in Percona Server for MySQL Percona Server for MySQL 5.7.12-5 and the behavior is now that of the former tokudb_pk_insert_mode set to <code>1</code>. The optimization will be used where safe and not used where not safe.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_prelock_empty","title":"<code>tokudb_prelock_empty</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default ON <p>By default TokuDB preemptively grabs an entire table lock on empty tables. If one transaction is doing the loading, such as when the user is doing a table load into an empty table, this default provides a considerable speedup.</p> <p>However, if multiple transactions try to do concurrent operations on an empty table, all but one transaction will be locked out. Disabling tokudb_prelock_empty optimizes for this multi-transaction case by turning off preemptive pre-locking.</p> <p>Note</p> <p>If this variable is set to <code>OFF</code>, fast bulk loading is turned off as well.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_read_block_size","title":"<code>tokudb_read_block_size</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 65536 (64KB) Range 4096 - 4294967295 <p>Fractal tree leaves are subdivided into read blocks, in order to speed up point queries. This variable controls the target uncompressed size of the read blocks. The units are bytes and the default is 65,536 (64 KB). A smaller value favors read performance for point and small range scans over large range scans and higher compression. The minimum value of this variable is 4096.</p> <p>Changing the value of tokudb_read_block_size only affects subsequently created tables. The value of this variable cannot be changed for an existing table without a dump and reload.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_read_buf_size","title":"<code>tokudb_read_buf_size</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 131072 (128KB) Range 0 - 1048576 <p>This variable controls the size of the buffer used to store values that are bulk fetched as part of a large range query. Its unit is bytes and its default value is 131,072 (128 KB).</p> <p>A value of <code>0</code> turns off bulk fetching. Each client keeps a thread of this size, so it should be lowered if situations where there are a large number of clients simultaneously querying a table.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_read_status_frequency","title":"<code>tokudb_read_status_frequency</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 10000 Range 0 - 4294967295 <p>This variable controls in how many reads the progress is measured to display <code>SHOW PROCESSLIST</code>. Reads are defined as <code>SELECT</code> queries.</p> <p>For slow queries, it can be helpful to set this variable and tokudb_write_status_frequency to <code>1</code>, and then run <code>SHOW PROCESSLIST</code> several times to understand what progress is being made.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_row_format","title":"<code>tokudb_row_format</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type ENUM Default TOKUDB_ZLIB Range TOKUDB_DEFAULT, TOKUDB_FAST, TOKUDB_SMALL, TOKUDB_ZLIB, TOKUDB_QUICKLZ, TOKUDB_LZMA, TOKUDB_SNAPPY, TOKUDB_UNCOMPRESSED <p>This controls the default compression algorithm used to compress data when no row format is specified in the <code>CREATE TABLE</code> command. For more information on compression algorithms see Compression Details.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_rpl_check_readonly","title":"<code>tokudb_rpl_check_readonly</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default ON <p>The TokuDB replication code will run row events from the binary log with Read Free Replication when the replica is in read-only mode. This variable is used to disable the replica read only check in the TokuDB replication code.</p> <p>This allows Read-Free-Replication to run when the replica is NOT read-only. By default, tokudb_rpl_check_readonly is enabled (check that replica is read-only). Do NOT change this value unless you completely understand the implications!</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_rpl_lookup_rows","title":"<code>tokudb_rpl_lookup_rows</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default ON <p>When disabled, TokuDB replication replicas skip row lookups for <code>delete row</code> log events and <code>update row</code> log events, which eliminates all associated read I/O for these operations.</p> <p>Warning</p> <p>TokuDB Read Free Replication will not propagate <code>UPDATE</code> and <code>DELETE</code> events reliably if TokuDB table is missing the primary key which will eventually lead to data inconsistency on the replica.</p> <p>Note</p> <p>Optimization is only enabled when read_only is set to <code>1</code> and binlog_format is <code>ROW</code>.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_rpl_lookup_rows_delay","title":"<code>tokudb_rpl_lookup_rows_delay</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 0 Range 0 - 18446744073709551615 <p>This variable allows for simulation of long disk reads by sleeping for the given number of microseconds prior to the row lookup query, it should only be set to a non-zero value for testing.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_rpl_unique_checks","title":"<code>tokudb_rpl_unique_checks</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default ON <p>When disabled, TokuDB replication replicas skip uniqueness checks on inserts and updates, which eliminates all associated read I/O for these operations.</p> <p>Note</p> <p>Optimization is only enabled when read_only is set to <code>1</code> and binlog_format is <code>ROW</code>.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_rpl_unique_checks_delay","title":"<code>tokudb_rpl_unique_checks_delay</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Numeric Default 0 Range 0 - 18446744073709551615 <p>This variable allows for simulation of long disk reads by sleeping for the given number of microseconds prior to the row lookup query, it should only be set to a non-zero value for testing.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_strip_frm_data","title":"<code>tokudb_strip_frm_data</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Boolean Default OFF <p>When this variable is set to <code>ON</code> during the startup server will check all the status files and remove the embedded <code>.frm</code> metadata. This variable can be used to assist in TokuDB data recovery. </p> <p>Warning</p> <p>Use this variable only if you know what you\u2019re doing otherwise it could lead to data loss.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_support_xa","title":"<code>tokudb_support_xa</code>","text":"Option Description Command-line Yes Config file Yes Scope Session, Global Dynamic Yes Data type Boolean Default ON <p>This variable defines whether or not the prepare phase of an XA transaction performs an <code>fsync()</code>.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_tmp_dir","title":"<code>tokudb_tmp_dir</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic No Data type String <p>This variable specifies the directory where the TokuDB bulk loader stores temporary files. The bulk loader can create large temporary files while it is loading a table, so putting these temporary files on a disk separate from the data directory can be useful.</p> <p>tokudb_load_save_space determines whether the data is compressed or not. The error message <code>ERROR 1030 (HY000): Got error 1 from storage engine</code> could indicate that the disk has run out of space.</p> <p>For example, it can make sense to use a high-performance disk for the data directory and a very inexpensive disk for the temporary directory. The default location for temporary files is the MySQL data directory.</p> <p>For more information check TokuDB files and file types and TokuDB file management.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_version","title":"<code>tokudb_version</code>","text":"Option Description Command-line No Config file No Scope Global Dynamic No Data type String <p>This read-only variable documents the version of the TokuDB storage engine.</p>"},{"location":"tokudb/tokudb_variables.html#tokudb_write_status_frequency","title":"<code>tokudb_write_status_frequency</code>","text":"Option Description Command-line Yes Config file Yes Scope Global Dynamic Yes Data type Numeric Default 1000 Range 0 - 4294967295 <p>This variable controls in how many writes the progress is measured to display <code>SHOW PROCESSLIST</code>. Writes are defined as <code>INSERT</code>, <code>UPDATE</code> and <code>DELETE</code> queries.</p> <p>For slow queries, it can be helpful to set this variable and tokudb_read_status_frequency to 1, and then run <code>SHOW PROCESSLIST</code> several times to understand what progress is being made.</p>"},{"location":"tokudb/using_tokudb.html","title":"Using TokuDB","text":"<p>Warning</p> <p>Do not move or modify any TokuDB files. You will break the database, and need to recover the database from a backup.</p>"},{"location":"tokudb/using_tokudb.html#fast-insertions-and-richer-indexes","title":"Fast Insertions and Richer Indexes","text":"<p>TokuDB\u2019s fast indexing enables fast queries through the use of rich indexes, such as covering and clustering indexes. It\u2019s worth investing some time to optimize index definitions to get the best performance from MySQL and TokuDB. Here are some resources to get you started:</p> <ul> <li> <p>\u201cUnderstanding Indexing\u201d by Zardosht Kasheff (video)</p> </li> <li> <p>Rule of Thumb for Choosing Column Order in Indexes</p> </li> <li> <p>Covering Indexes: Orders-of-Magnitude Improvements</p> </li> <li> <p>Introducing Multiple Clustering Indexes</p> </li> <li> <p>Clustering Indexes vs. Covering Indexes</p> </li> <li> <p>How Clustering Indexes Sometimes Helps UPDATE and DELETE Performance</p> </li> <li> <p>High Performance MySQL, 3<sup>rd</sup> Edition by Baron Schwartz, Peter Zaitsev, Vadim Tkachenko, Copyright 2012, O\u2019Reilly Media. See Chapter 5, Indexing for High Performance.</p> </li> </ul>"},{"location":"tokudb/using_tokudb.html#clustering-secondary-indexes","title":"Clustering Secondary Indexes","text":"<p>One of the keys to exploiting TokuDB\u2019s strength in indexing is to make use of clustering secondary indexes.</p> <p>TokuDB allows a secondary key to be defined as a clustering key. This means that all of the columns in the table are clustered with the secondary key. Percona Server for MySQL parser and query optimizer support Multiple Clustering Keys when TokuDB engine is used. This means that the query optimizer will avoid primary clustered index reads and replace them by secondary clustered index reads in certain scenarios.</p> <p>The parser has been extended to support following syntax:</p> <pre><code>CREATE TABLE ... ( ..., CLUSTERING KEY identifier (column list), ...\nCREATE TABLE ... ( ..., UNIQUE CLUSTERING KEY identifier (column list), ...\nCREATE TABLE ... ( ..., CLUSTERING UNIQUE KEY identifier (column list), ...\nCREATE TABLE ... ( ..., CONSTRAINT identifier UNIQUE CLUSTERING KEY identifier (column list), ...\nCREATE TABLE ... ( ..., CONSTRAINT identifier CLUSTERING UNIQUE KEY identifier (column list), ...\n\nCREATE TABLE ... (... column type CLUSTERING [UNIQUE] [KEY], ...)\nCREATE TABLE ... (... column type [UNIQUE] CLUSTERING [KEY], ...)\n\nALTER TABLE ..., ADD CLUSTERING INDEX identifier (column list), ...\nALTER TABLE ..., ADD UNIQUE CLUSTERING INDEX identifier (column list), ...\nALTER TABLE ..., ADD CLUSTERING UNIQUE INDEX identifier (column list), ...\nALTER TABLE ..., ADD CONSTRAINT identifier UNIQUE CLUSTERING INDEX identifier (column list), ...\nALTER TABLE ..., ADD CONSTRAINT identifier CLUSTERING UNIQUE INDEX identifier (column list), ...\n\nCREATE CLUSTERING INDEX identifier ON ...\n</code></pre> <p>To define a secondary index as clustering, simply add the word <code>CLUSTERING</code> before the key definition. For example:</p> <pre><code>CREATE TABLE foo (\ncolumn_a INT,\ncolumn_b INT,\ncolumn_c INT,\nPRIMARY KEY index_a (column_a),\nCLUSTERING KEY index_b (column_b)) ENGINE = TokuDB;\n</code></pre> <p>In the previous example, the primary table is indexed on column_a. Additionally, there is a secondary clustering index (named index_b) sorted on column_b. Unlike non-clustered indexes, clustering indexes include all the columns of a table and can be used as covering indexes. For example, the following query will run very fast using the clustering index_b:</p> <pre><code>SELECT column_c\nFROM foo\nWHERE column_b BETWEEN 10 AND 100;\n</code></pre> <p>This index is sorted on column_b, making the <code>WHERE</code> clause fast, and includes column_c, which avoids lookups in the primary table to satisfy the query.</p> <p>TokuDB makes clustering indexes feasible because of its excellent compression and very high indexing rates. For more information about using clustering indexes, see Introducing Multiple Clustering Indexes.</p>"},{"location":"tokudb/using_tokudb.html#hot-index-creation","title":"Hot Index Creation","text":"<p>TokuDB enables you to add indexes to an existing table and still perform inserts and queries on that table while the index is being created.</p> <p>The <code>ONLINE</code> keyword is not used. Instead, the value of the tokudb_create_index_online client session variable is examined.</p> <p>Hot index creation is invoked using the <code>CREATE INDEX</code> command after setting tokudb_create_index_online to <code>on</code> as follows:</p> <pre><code>mysql&gt; SET tokudb_create_index_online=on;\n</code></pre> <p>The output should resemble the following:</p> <p><pre><code>Query OK, 0 rows affected (0.00 sec)\n</code></pre> <pre><code>mysql&gt; CREATE INDEX index ON foo (field_name);\n</code></pre></p> <p>Alternatively, using the <code>ALTER TABLE</code> command for creating an index will create the index offline (with the table unavailable for inserts or queries), regardless of the value of tokudb_create_index_online. The only way to hot create an index is to use the <code>CREATE INDEX</code> command.</p> <p>Hot creating an index will be slower than creating the index offline, and progress depends how busy the mysqld server is with other tasks. Progress of the index creation can be seen by using the <code>SHOW PROCESSLIST</code> command (in another client). Once the index creation completes, the new index will be used in future query plans.</p> <p>If more than one hot <code>CREATE INDEX</code> is issued for a particular table, the indexes will be created serially. An index creation that is waiting for another to complete will be shown as Locked in <code>SHOW PROCESSLIST</code>. We recommend that each <code>CREATE INDEX</code> be allowed to complete before the next one is started.</p>"},{"location":"tokudb/using_tokudb.html#hot-column-add-delete-expand-and-rename-hcader","title":"Hot Column Add, Delete, Expand, and Rename (HCADER)","text":"<p>TokuDB enables you to add or delete columns in an existing table, expand <code>char</code>, <code>varchar</code>, <code>varbinary</code>, and <code>integer</code> type columns in an existing table, or rename an existing column in a table with little blocking of other updates and queries. HCADER typically blocks other queries with a table lock for no more than a few seconds. After that initial short-term table locking, the system modifies each row (when adding, deleting, or expanding columns) later, when the row is next brought into main memory from disk. For column rename, all the work is done during the seconds of downtime. On-disk rows need not be modified.</p> <p>To get good performance from HCADER, observe the following guidelines:</p> <ul> <li>The work of altering the table for column addition, deletion, or expansion is performed as subsequent operations touch parts of the Fractal Tree, both in the primary index and secondary indexes.</li> </ul> <p>You can force the column addition, deletion, or expansion work to be performed all at once using the standard syntax of <code>OPTIMIZE TABLE X</code>, when a column has been added to, deleted from, or expanded in table X. It is important to note that as of TokuDB version 7.1.0, <code>OPTIMIZE TABLE</code> is also hot, so that a table supports updates and queries without blocking while an <code>OPTIMIZE TABLE</code> is being performed. Also, a hot <code>OPTIMIZE TABLE</code> does not rebuild the indexes, since TokuDB indexes do not age. Rather, they flush all background work, such as that induced by a hot column addition, deletion, or expansion.</p> <ul> <li> <p>Each hot column addition, deletion, or expansion operation must be performed individually (with its own SQL statement). If you want to add, delete, or expand multiple columns use multiple statements.</p> </li> <li> <p>Avoid adding, deleting, or expanding a column at the same time as adding or dropping an index.</p> </li> <li> <p>The time that the table lock is held can vary. The table-locking time for HCADER is dominated by the time it takes to flush dirty pages, because MySQL closes the table after altering it. If a checkpoint has happened recently, this operation is fast (on the order of seconds). However, if the table has many dirty pages, then the flushing stage can take on the order of minutes.</p> </li> <li> <p>Avoid dropping a column that is part of an index. If a column to be dropped is part of an index, then dropping that column is slow. To drop a column that is part of an index, first drop the indexes that reference the column in one alter table statement, and then drop the column in another statement.</p> </li> <li> <p>Hot column expansion operations are only supported to <code>char</code>, <code>varchar</code>, <code>varbinary</code>, and <code>integer</code> data types. Hot column expansion is not supported if the given column is part of the primary key or any secondary keys.</p> </li> <li> <p>Rename only one column per statement. Renaming more than one column will revert to the standard MySQL blocking behavior. The proper syntax is as follows:</p> </li> </ul> <pre><code>ALTER TABLE table\nCHANGE column_old column_new\nDATA_TYPE REQUIRED_NESS DEFAULT\n</code></pre> <p>Here\u2019s an example of how that might look:</p> <pre><code>ALTER TABLE table\nCHANGE column_old column_new\nINT(10) NOT NULL;\n</code></pre> <p>Notice that all of the column attributes must be specified. <code>ALTER TABLE table CHANGE column_old column_new;</code> induces a slow, blocking column rename.</p> <ul> <li> <p>Hot column rename does not support the following data types: <code>TIME</code>, <code>ENUM</code>, <code>BLOB</code>, <code>TINYBLOB</code>, <code>MEDIUMBLOB</code>, <code>LONGBLOB</code>. Renaming columns of these types will revert to the standard MySQL blocking behavior.</p> </li> <li> <p>Temporary tables cannot take advantage of HCADER. Temporary tables are typically small anyway, so altering them using the standard method is usually fast.</p> </li> </ul>"},{"location":"tokudb/using_tokudb.html#compression-details","title":"Compression Details","text":"<p>TokuDB offers different levels of compression, which trade off between the amount of CPU used and the compression achieved. Standard compression uses less CPU but generally compresses at a lower level, high compression uses more CPU and generally compresses at a higher level. We have seen compression up to 25x on customer data.</p> <p>Compression in TokuDB occurs on background threads, which means that high compression need not slow down your database. Indeed, in some settings, we\u2019ve seen higher overall database performance with high compression.</p> <p>Note</p> <p>We recommend that users use standard compression on machines with six or fewer cores, and high compression on machines with more than six cores.</p> <p>The ultimate choice depends on the particulars of how a database is used, and we recommend that users use the default settings unless they have profiled their system with high compression in place.</p> <p>Compression is set on a per-table basis and is controlled by setting row format during a <code>CREATE TABLE</code> or <code>ALTER TABLE</code>. For example:</p> <pre><code>CREATE TABLE table (\ncolumn_a INT NOT NULL PRIMARY KEY,\ncolumn_b INT NOT NULL) ENGINE=TokuDB\nROW_FORMAT=row_format;\n</code></pre> <p>If no row format is specified in a <code>CREATE TABLE</code>, the table is compressed using whichever row format is specified in the session variable tokudb_row_format. If no row format is set nor is tokudb_row_format, the zlib compressor is used.</p> <p>row_format and tokudb_row_format variables accept the following values:</p> <ul> <li> <p><code>TOKUDB_DEFAULT</code>: This sets the compression to the default behavior. As of TokuDB 7.1.0, the default behavior is to compress using the zlib library. In the future this behavior may change.</p> </li> <li> <p><code>TOKUDB_FAST</code>: This sets the compression to use the quicklz library.</p> </li> <li> <p><code>TOKUDB_SMALL</code>: This sets the compression to use the lzma library.</p> </li> </ul> <p>In addition, you can choose a compression library directly, which will override previous values. The following libraries are available:</p> <ul> <li> <p><code>TOKUDB_ZLIB</code>: Compress using the zlib library, which provides mid-range compression and CPU utilization.</p> </li> <li> <p><code>TOKUDB_QUICKLZ</code>: Compress using the quicklz library, which provides light compression and low CPU utilization.</p> </li> <li> <p><code>TOKUDB_LZMA</code>: Compress using the lzma library, which provides the highest compression and high CPU utilization.</p> </li> <li> <p><code>TOKUDB_SNAPPY</code> - This compression is using snappy library and aims for very high speeds and reasonable compression.</p> </li> <li> <p><code>TOKUDB_UNCOMPRESSED</code>: This setting turns off compression and is useful for tables with data that cannot be compressed.</p> </li> </ul>"},{"location":"tokudb/using_tokudb.html#changing-compression-of-a-table","title":"Changing Compression of a Table","text":"<p>Modify the compression used on a particular table with the following command:</p> <pre><code>ALTER TABLE table\nROW_FORMAT=row_format;\n</code></pre> <p>Note</p> <p>Changing the compression of a table only affects newly written data (dirtied blocks). After changing a table\u2019s compression you can run <code>OPTIMZE TABLE</code> to rewrite all blocks of the table and its indexes.</p>"},{"location":"tokudb/using_tokudb.html#read-free-replication","title":"Read Free Replication","text":"<p>TokuDB replicas can be configured to perform significantly less read IO in order to apply changes from the source. By utilizing the power of Fractal Tree indexes:</p> <ul> <li> <p>insert/update/delete operations can be configured to eliminate read-modify-write behavior and simply inject messages into the appropriate Fractal Tree indexes</p> </li> <li> <p>update/delete operations can be configured to eliminate the IO required for uniqueness checking</p> </li> </ul> <p>To enable Read Free Replication, the servers must be configured as follows:</p> <ul> <li> <p>On the replication source:</p> <ul> <li>Enable row based replication: set <code>BINLOG_FORMAT=ROW</code></li> </ul> </li> <li> <p>On the replication replica(s):</p> <ul> <li> <p>The replica must be in read-only mode: set <code>read_only=1</code></p> </li> <li> <p>Disable unique checks: set <code>tokudb_rpl_unique_checks=0</code></p> </li> <li> <p>Disable lookups (read-modify-write): set <code>tokudb_rpl_lookup_rows=0</code></p> </li> </ul> </li> </ul> <p>Note</p> <p>You can modify one or both behaviors on the replica(s).</p> <p>Note</p> <p>As long as the source is using row based replication, this optimization is available on a TokuDB replica. This means that it\u2019s available even if the source is using InnoDB or MyISAM tables, or running non-TokuDB binaries.</p> <p>Warning</p> <p>TokuDB Read Free Replication will not propagate <code>UPDATE</code> and <code>DELETE</code> events reliably if TokuDB table is missing the primary key which will eventually lead to data inconsistency on the replica.</p>"},{"location":"tokudb/using_tokudb.html#transactions-and-acid-compliant-recovery","title":"Transactions and ACID-compliant Recovery","text":"<p>By default, TokuDB checkpoints all open tables regularly and logs all changes between checkpoints, so that after a power failure or system crash, TokuDB will restore all tables into their fully ACID-compliant state. That is, all committed transactions will be reflected in the tables, and any transaction not committed at the time of failure will be rolled back.</p> <p>The default checkpoint period is every 60 seconds, and this specifies the time from the beginning of one checkpoint to the beginning of the next. If a checkpoint requires more than the defined checkpoint period to complete, the next checkpoint begins immediately. It is also related to the frequency with which log files are trimmed, as described below. The user can induce a checkpoint at any time by issuing the <code>FLUSH LOGS</code> command. When a database is shut down normally it is also checkpointed and all open transactions are aborted. The logs are trimmed at startup.</p>"},{"location":"tokudb/using_tokudb.html#managing-log-size","title":"Managing Log Size","text":"<p>TokuDB keeps log files back to the most recent checkpoint. Whenever a log file reaches 100 MB, a new log file is started. Whenever there is a checkpoint, all log files older than the checkpoint are discarded. If the checkpoint period is set to be a very large number, logs will get trimmed less frequently. This value is set to 60 seconds by default.</p> <p>TokuDB also keeps rollback logs for each open transaction. The size of each log is proportional to the amount of work done by its transaction and is stored compressed on disk. Rollback logs are trimmed when the associated transaction completes.</p>"},{"location":"tokudb/using_tokudb.html#recovery","title":"Recovery","text":"<p>Recovery is fully automatic with TokuDB. TokuDB uses both the log files and rollback logs to recover from a crash. The time to recover from a crash is proportional to the combined size of the log files and uncompressed size of rollback logs. Thus, if there were no long-standing transactions open at the time of the most recent checkpoint, recovery will take less than a minute.</p>"},{"location":"tokudb/using_tokudb.html#disabling-the-write-cache","title":"Disabling the Write Cache","text":"<p>When using any transaction-safe database, it is essential that you understand the write-caching characteristics of your hardware. TokuDB provides transaction safe (ACID compliant) data storage for MySQL. However, if the underlying operating system or hardware does not actually write data to disk when it says it did, the system can corrupt your database when the machine crashes. For example, TokuDB can not guarantee proper recovery if it is mounted on an NFS volume. It is always safe to disable the write cache, but you may be giving up some performance.</p> <p>For most configurations you must disable the write cache on your disk drives. On ATA/SATA drives, the following command should disable the write cache:</p> <pre><code>$ hdparm -W0 /dev/hda\n</code></pre> <p>There are some cases when you can keep the write cache, for example:</p> <ul> <li> <p>Write caching can remain enabled when using XFS, but only if XFS reports that disk write barriers work. If you see one of the following messages in /var/log/messages, then you must disable the write cache:</p> <ul> <li> <p><code>Disabling barriers, not supported with external log device</code></p> </li> <li> <p><code>Disabling barriers, not supported by the underlying device</code></p> </li> <li> <p><code>Disabling barriers, trial barrier write failed</code></p> </li> </ul> </li> </ul> <p>XFS write barriers appear to succeed for single disks (with no LVM), or for very recent kernels (such as that provided by Fedora 12). For more information, see the XFS FAQ.</p> <p>In the following cases, you must disable the write cache:</p> <ul> <li> <p>If you use the ext3 filesystem</p> </li> <li> <p>If you use LVM (although recent Linux kernels, such as Fedora 12, have fixed this problem)</p> </li> <li> <p>If you use Linux\u2019s software RAID</p> </li> <li> <p>If you use a RAID controller with battery-backed-up memory. This may seem counter-intuitive. For more information, see the XFS FAQ</p> </li> </ul> <p>In summary, you should disable the write cache, unless you have a very specific reason not to do so.</p>"},{"location":"tokudb/using_tokudb.html#progress-tracking","title":"Progress Tracking","text":"<p>TokuDB has a system for tracking progress of long running statements, thereby removing the need to define triggers to track statement execution, as follows:</p> <ul> <li> <p>Bulk Load: When loading large tables using <code>LOAD DATA INFILE</code> commands, doing a <code>SHOW PROCESSLIST</code> command in a separate client session shows progress. There are two progress stages. The first will state something like <code>Inserted about 1000000 rows</code>. After all rows are processed like this, the next stage tracks progress by showing what fraction of the work is done (e.g. <code>Loading of data about 45% done</code>)</p> </li> <li> <p>Adding Indexes: When adding indexes via <code>ALTER TABLE</code> or <code>CREATE INDEX</code>, the command <code>SHOW PROCESSLIST</code> shows progress. When adding indexes via <code>ALTER TABLE</code> or <code>CREATE INDEX</code>, the command <code>SHOW PROCESSLIST</code> will include an estimation of the number of rows processed. Use this information to verify progress is being made. Similar to bulk loading, the first stage shows how many rows have been processed, and the second stage shows progress with a fraction.</p> </li> <li> <p>Commits and Aborts: When committing or aborting a transaction, the command <code>SHOW PROCESSLIST</code> will include an estimate of the transactional operations processed.</p> </li> </ul>"},{"location":"tokudb/using_tokudb.html#migrating-to-tokudb","title":"Migrating to TokuDB","text":"<p>To convert an existing table to use the TokuDB engine, run <code>ALTER TABLE... ENGINE=TokuDB</code>. If you wish to load from a file, use <code>LOAD DATA INFILE</code> and not <code>mysqldump</code>. Using <code>mysqldump</code> will be much slower. To create a file that can be loaded with <code>LOAD DATA INFILE</code>, refer to the <code>INTO OUTFILE</code> option of the SELECT Syntax.</p> <p>Note</p> <p>Creating this file does not save the schema of your table, so you may want to create a copy of that as well.</p>"}]}